{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-28T15:24:06.460386Z",
     "iopub.status.busy": "2025-10-28T15:24:06.459632Z",
     "iopub.status.idle": "2025-10-28T15:24:06.661153Z",
     "shell.execute_reply": "2025-10-28T15:24:06.660212Z",
     "shell.execute_reply.started": "2025-10-28T15:24:06.460360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Credentials written to: /root/.kaggle/kaggle.json\n",
      "total 16\n",
      "drwxr-xr-x 2 root root 4096 Oct 28 15:24 .\n",
      "drwx------ 1 root root 4096 Oct 28 15:24 ..\n",
      "-rw------- 1 root root   72 Oct 28 15:24 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import json, os, pathlib, subprocess, sys\n",
    "\n",
    "# --- 1. Load your secret (full JSON from kaggle.json) ---\n",
    "secret_name = \"kaggle_json\"  # change if you used another name\n",
    "user_secrets = UserSecretsClient()\n",
    "raw = user_secrets.get_secret(secret_name)\n",
    "creds = json.loads(raw)\n",
    "\n",
    "# --- 2. Forcefully recreate ~/.kaggle/kaggle.json ---\n",
    "kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "cred_path = kaggle_dir / \"kaggle.json\"\n",
    "cred_path.write_text(json.dumps(creds))\n",
    "os.chmod(cred_path, 0o600)\n",
    "\n",
    "# --- 3. Double-check the file actually exists and is readable ---\n",
    "print(\"✅ Credentials written to:\", cred_path)\n",
    "!ls -la ~/.kaggle/\n",
    "#!cat ~/.kaggle/kaggle.json | head -1\n",
    "\n",
    "# --- 4. Reinstall Kaggle CLI cleanly ---\n",
    "#!pip install --upgrade --force-reinstall kaggle --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T15:24:09.771625Z",
     "iopub.status.busy": "2025-10-28T15:24:09.771308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Page Token = CfDJ8IaGWDgvvrBFtGGva9hUIY5LdZ3PXrJfJU_fwc4Tr2pQj5scaB-LHXTjzASO4EOq9C1eO350ZDByfcYKnqrGpyE\n",
      "name                                     size  creationDate                \n",
      "---------------------------------  ----------  --------------------------  \n",
      "test/test/captions.txt                  90426  2025-10-25 16:32:52.931000  \n",
      "test/test/test.clean.npz              5765331  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1000092795.jpg      218143  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/10002456.jpg        113525  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1000268201.jpg      199606  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1000344755.jpg      154005  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1000366164.jpg      103316  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1000919630.jpg      117183  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/10010052.jpg         44514  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1001465944.jpg      141082  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1001545525.jpg       86149  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1001573224.jpg      100421  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1001633352.jpg      120095  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1001773457.jpg      142258  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/100197432.jpg       110871  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/100207720.jpg        87981  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1002674143.jpg      159390  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1003163366.jpg      153171  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/1003428081.jpg       83429  2025-10-25 16:32:52.931000  \n",
      "train/train/Images/100444898.jpg        86660  2025-10-25 16:32:52.931000  \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions files -c aml-competition\n",
    "!kaggle competitions download -c aml-competition -p /kaggle/working --force\n",
    "!mkdir -p /kaggle/working/data\n",
    "!unzip -o /kaggle/working/aml-competition.zip -d /kaggle/working/data\n",
    "!ls -lah /kaggle/working/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:21:53.302199Z",
     "iopub.status.busy": "2025-10-28T16:21:53.301907Z",
     "iopub.status.idle": "2025-10-28T16:21:56.559404Z",
     "shell.execute_reply": "2025-10-28T16:21:56.558319Z",
     "shell.execute_reply.started": "2025-10-28T16:21:53.302178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'challenge'...\n",
      "remote: Enumerating objects: 98, done.\u001b[K\n",
      "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
      "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
      "remote: Total 98 (delta 39), reused 72 (delta 26), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (98/98), 21.03 MiB | 20.96 MiB/s, done.\n",
      "Resolving deltas: 100% (39/39), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Mamiglia/challenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:21:56.561676Z",
     "iopub.status.busy": "2025-10-28T16:21:56.561418Z",
     "iopub.status.idle": "2025-10-28T16:21:56.618876Z",
     "shell.execute_reply": "2025-10-28T16:21:56.618176Z",
     "shell.execute_reply.started": "2025-10-28T16:21:56.561655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# runner.py\n",
    "import argparse, json, random, re, time\n",
    "from pathlib import Path\n",
    "from os.path import basename\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from challenge.src.common import load_data, generate_submission\n",
    "\n",
    "# ---------- Paths ----------\n",
    "DATA_ROOT = Path(\"/kaggle/working/data\")\n",
    "TRAIN_DIR = DATA_ROOT / \"train\" / \"train\"\n",
    "TEST_DIR  = DATA_ROOT / \"test\"  / \"test\"\n",
    "TRAIN_NPZ = TRAIN_DIR / \"train.npz\"\n",
    "TEST_NPZ  = TEST_DIR  / \"test.clean.npz\"\n",
    "TRAIN_CAPTIONS = TRAIN_DIR / \"captions.txt\"\n",
    "TEST_CAPTIONS  = TEST_DIR  / \"captions.txt\"\n",
    "\n",
    "# ---------- Determinism ----------\n",
    "def seed_all(s: int):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ---------- Metadata/safety ----------\n",
    "def _assert_metadata_dims(npz_path: Path, expect_text=1024, expect_img=1536):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    md = {k: d[k][0] for k in d.files if k.startswith(\"metadata/\")}\n",
    "    tdim = md.get(\"metadata/embedding_dim_text\", None)\n",
    "    idim = md.get(\"metadata/embedding_dim_image\", None)\n",
    "    print(f\"[meta] text_dim={tdim} | image_dim={idim}\")\n",
    "    assert tdim == expect_text and idim == expect_img, (\n",
    "        f\"Encoder dims mismatch: expected text={expect_text}, image={expect_img} \"\n",
    "        f\"but got text={tdim}, image={idim}. Regenerate NPZ with the fixed encoders.\"\n",
    "    )\n",
    "\n",
    "# ---------- Caption→image matching from captions.txt ----------\n",
    "def _build_image_index(image_ids):\n",
    "    idx_exact = {}; idx_base={}; idx_stem={}\n",
    "    for i, v in enumerate(image_ids):\n",
    "        s = str(v); idx_exact[s]=i\n",
    "        b = basename(s); idx_base[b]=i\n",
    "        stem = b.rsplit(\".\",1)[0] if \".\" in b else b\n",
    "        idx_stem[stem]=i\n",
    "    return idx_exact, idx_base, idx_stem\n",
    "\n",
    "def _iter_targets_from_captions(path: Path, n_text: int, idx_exact, idx_base, idx_stem):\n",
    "    targets=[]\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            if len(targets)>=n_text: break\n",
    "            line=raw.strip()\n",
    "            if not line: continue\n",
    "            # be robust to delimiters\n",
    "            parts=re.split(r'\\||\\t|,| {2,}', line)\n",
    "            if len(parts)==1: parts=line.split(\" \",1)\n",
    "            tok=parts[0].strip().strip('\"').strip(\"'\")\n",
    "            if not tok: continue\n",
    "            def _match(tok_):\n",
    "                if tok_ in idx_exact: return idx_exact[tok_]\n",
    "                b=basename(tok_)\n",
    "                if b in idx_base: return idx_base[b]\n",
    "                stem=b.rsplit(\".\",1)[0] if \".\" in b else b\n",
    "                if stem in idx_stem: return idx_stem[stem]\n",
    "                return None\n",
    "            idx=_match(tok)\n",
    "            if idx is None:\n",
    "                tok2=tok.replace(\"Images/\",\"\").replace(\"./\",\"\")\n",
    "                idx=_match(tok2)\n",
    "            if idx is None:\n",
    "                if \".\" not in tok: continue\n",
    "                raise AssertionError(f\"Could not match image token '{tok}'\")\n",
    "            targets.append(idx)\n",
    "    assert len(targets)==n_text, f\"matched {len(targets)} vs N_text {n_text}\"\n",
    "    return np.asarray(targets, dtype=np.int64)\n",
    "\n",
    "# ---------- Data loaders ----------\n",
    "def load_train(out_dir: Path):\n",
    "    _assert_metadata_dims(TRAIN_NPZ, 1024, 1536)\n",
    "    d=np.load(TRAIN_NPZ, allow_pickle=True)\n",
    "    X=d[\"captions/embeddings\"].astype(np.float32)  # (N_text,1024)\n",
    "    I=d[\"images/embeddings\"].astype(np.float32)    # (N_img,1536)\n",
    "    cap_ids=d.get(\"captions/ids\", np.arange(len(X)).astype(str))\n",
    "    img_names=d.get(\"images/names\", np.arange(len(I)).astype(str))\n",
    "\n",
    "    ex,ba,st=_build_image_index(img_names)\n",
    "    assert TRAIN_CAPTIONS.exists(), f\"Missing {TRAIN_CAPTIONS}\"\n",
    "    targets=_iter_targets_from_captions(TRAIN_CAPTIONS, len(X), ex,ba,st)\n",
    "\n",
    "    Y=I[targets]                       # (N_text, 1536) GT image vec per caption\n",
    "    img_ids_row = img_names[targets]   # image name per caption row (for splitting)\n",
    "    meta={\"n_text\":int(len(X)),\"n_images\":int(len(I))}\n",
    "    (out_dir/\"train_detect.json\").write_text(json.dumps(meta, indent=2))\n",
    "    return X,Y,cap_ids,img_ids_row,I,img_names\n",
    "\n",
    "def load_test_npz():\n",
    "    d_test=np.load(TEST_NPZ, allow_pickle=True)\n",
    "    Q=d_test[\"captions/embeddings\"].astype(np.float32)\n",
    "    q_ids=d_test.get(\"captions/ids\", np.arange(len(Q)).astype(str))\n",
    "    if \"images/embeddings\" in d_test.files:\n",
    "        G=d_test[\"images/embeddings\"].astype(np.float32)\n",
    "        g_ids=d_test.get(\"images/names\", np.arange(len(G)).astype(str))\n",
    "    else:\n",
    "        d_tr=np.load(TRAIN_NPZ, allow_pickle=True)\n",
    "        G=d_tr[\"images/embeddings\"].astype(np.float32)\n",
    "        g_ids=d_tr.get(\"images/names\", np.arange(len(G)).astype(str))\n",
    "    return Q,G,q_ids,g_ids\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class PairDS(Dataset):\n",
    "    def __init__(self, X, Y): self.X=torch.from_numpy(X); self.Y=torch.from_numpy(Y)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
    "\n",
    "# ---------- Pooling (no-op per spec) ----------\n",
    "def apply_pooling(x: torch.Tensor, mode: str, n_patches=None):\n",
    "    return x\n",
    "\n",
    "# ---------- Base Models ----------\n",
    "class LinearProj(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(din,dout)\n",
    "        nn.init.xavier_normal_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
    "    def forward(self,x): return self.fc(x)\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self, din, dout, hidden=512, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(din,hidden), nn.ReLU(), nn.Dropout(pdrop),\n",
    "            nn.Linear(hidden,dout)\n",
    "        )\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    # Spec: 1024→1024→512→1536, dropout=0.1 on hidden layers\n",
    "    def __init__(self, din, dout, h1=1024, h2=512, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(din,h1), nn.ReLU(), nn.Dropout(pdrop),\n",
    "            nn.Linear(h1,h2), nn.ReLU(), nn.Dropout(pdrop),\n",
    "            nn.Linear(h2,dout)\n",
    "        )\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "# ---------- Geometry-Preserving Linear (Whiten → Procrustes → Re-color) ----------\n",
    "def _cov_eigh(zc, eps):\n",
    "    # zc: (N,D), zero-mean\n",
    "    N = zc.shape[0]\n",
    "    C = (zc.T @ zc) / max(1, N)\n",
    "    # symmetric PSD\n",
    "    S, U = np.linalg.eigh(C)\n",
    "    S = np.clip(S, 0.0, None)\n",
    "    inv_sqrt = 1.0 / np.sqrt(S + eps)\n",
    "    sqrt = np.sqrt(S + eps)\n",
    "    C_mhalf = (U * inv_sqrt) @ U.T     # C^{-1/2}\n",
    "    C_phalf = (U * sqrt) @ U.T         # C^{ 1/2}\n",
    "    return C_mhalf.astype(np.float32), C_phalf.astype(np.float32)\n",
    "\n",
    "def procrustes_closed_form(X, Y, eps=1e-5):\n",
    "    \"\"\"\n",
    "    X: (N, 1024) text, Y: (N, 1536) targets (per-caption image vectors)\n",
    "    Returns A (1536x1024), b (1536,) such that y_hat = A x + b\n",
    "    \"\"\"\n",
    "    # center\n",
    "    mu_x = X.mean(0, dtype=np.float64)\n",
    "    mu_y = Y.mean(0, dtype=np.float64)\n",
    "    Xc = X - mu_x\n",
    "    Yc = Y - mu_y\n",
    "\n",
    "    # whiten both\n",
    "    Cx_mh, _ = _cov_eigh(Xc, eps)      # 1024x1024\n",
    "    Cy_mh, Cy_ph = _cov_eigh(Yc, eps)  # 1536x1536 (only Cy_ph used)\n",
    "    Xw = Xc @ Cx_mh.T                  # (N,1024)\n",
    "    Yw = Yc @ Cy_mh.T                  # (N,1536)\n",
    "\n",
    "    # orthogonal Procrustes: maximize Tr(R^T Xw^T Yw)\n",
    "    M = Xw.T @ Yw                      # (1024,1536)\n",
    "    # SVD on M; for rectangular, do SVD and build R = U V^T in common subspace\n",
    "    U, _, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "    R = U @ Vt                         # (1024,1536) @ (1536,1536) -> (1024,1536) OK since full_matrices=False\n",
    "    # we need R as (1536x1024) mapping whitened X to whitened Y; above is (1024x1536)\n",
    "    R = R.T                            # (1536,1024)\n",
    "\n",
    "    # re-color to Y space\n",
    "    A = (Cy_ph @ R @ Cx_mh).astype(np.float32)     # (1536,1536)*(1536,1024)*(1024,1024) = (1536,1024)\n",
    "    b = (mu_y - (A @ mu_x)).astype(np.float32)     # (1536,)\n",
    "    return A, b\n",
    "\n",
    "class GeomLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with weights initialized from closed-form geometry mapping.\n",
    "    Optionally fine-tunes with tiny LR.\n",
    "    \"\"\"\n",
    "    def __init__(self, A: np.ndarray, b: np.ndarray):\n",
    "        super().__init__()\n",
    "        D_in = A.shape[1]; D_out = A.shape[0]\n",
    "        self.fc = nn.Linear(D_in, D_out, bias=True)\n",
    "        with torch.no_grad():\n",
    "            self.fc.weight.copy_(torch.from_numpy(A))\n",
    "            self.fc.bias.copy_(torch.from_numpy(b))\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------- Loss pieces (for optional fine-tune) ----------\n",
    "def moment_align(pred, tgt):\n",
    "    mu_p, mu_t = pred.mean(0), tgt.mean(0)\n",
    "    sd_p, sd_t = pred.std(0, unbiased=False), tgt.std(0, unbiased=False)\n",
    "    return F.mse_loss(mu_p, mu_t) + F.mse_loss(sd_p, sd_t)\n",
    "\n",
    "def info_nce(pred, tgt):\n",
    "    p = F.normalize(pred, dim=-1)\n",
    "    t = F.normalize(tgt, dim=-1)\n",
    "    logits = p @ t.t()\n",
    "    labels = torch.arange(pred.size(0), device=pred.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "# ---------- Split by image id & mapping ----------\n",
    "def build_image_id_split(img_ids_row, all_img_names, full_img, X, Y, val_ratio, seed, out_dir: Path):\n",
    "    uniq_img_names = np.array(sorted(set(map(str, all_img_names))))\n",
    "    rng = np.random.default_rng(seed); rng.shuffle(uniq_img_names)\n",
    "    n_val = max(1, int(len(uniq_img_names) * val_ratio))\n",
    "    val_images = set(uniq_img_names[:n_val])\n",
    "    tr_images  = set(uniq_img_names[n_val:])\n",
    "    assert len(val_images & tr_images) == 0, \"Leakage in image id split!\"\n",
    "\n",
    "    # Caption masks\n",
    "    cap_is_val = np.array([str(iid) in val_images for iid in img_ids_row], dtype=bool)\n",
    "    cap_is_tr  = ~cap_is_val\n",
    "\n",
    "    # Build VAL gallery (unique images in VAL) as sorted names → local indices\n",
    "    val_img_names_sorted = np.array(sorted(val_images))\n",
    "    name2local = {name:i for i,name in enumerate(val_img_names_sorted)}\n",
    "    name2global = {str(n):i for i,n in enumerate(all_img_names)}\n",
    "    val_img_indices = np.array([name2global[n] for n in val_img_names_sorted], dtype=np.int64)\n",
    "    val_gallery = full_img[val_img_indices]  # (M,1536)\n",
    "\n",
    "    # For each VAL caption row, get local gallery index (no remap later)\n",
    "    cap2gal_local = np.array([name2local[str(n)] for n in img_ids_row[cap_is_val]], dtype=np.int64)\n",
    "\n",
    "    # Persist mapping for debugging/acceptance\n",
    "    (out_dir/\"val_indices.json\").write_text(json.dumps({\n",
    "        \"val_img_indices\": val_img_indices.tolist(),\n",
    "        \"val_caption_to_gallery_index\": cap2gal_local.tolist(),\n",
    "        \"n_val_captions\": int(cap_is_val.sum()),\n",
    "        \"n_val_unique_images\": int(len(val_images))\n",
    "    }, indent=2))\n",
    "\n",
    "    return cap_is_tr, cap_is_val, val_gallery, cap2gal_local, val_img_indices\n",
    "\n",
    "# ---------- Metrics / evaluator (full gallery, cosine on L2) ----------\n",
    "@torch.no_grad()\n",
    "def validate_retrieval(model, Xv, val_gallery, cap2gal_local, pooling, n_patches, bs=1024):\n",
    "    device=next(model.parameters()).device\n",
    "    Gi = torch.from_numpy(val_gallery).to(device)\n",
    "    Gi = F.normalize(Gi, dim=-1)\n",
    "\n",
    "    ranks=[]\n",
    "    for i in range(0, len(Xv), bs):\n",
    "        xb=torch.from_numpy(Xv[i:i+bs]).to(device)\n",
    "        xb=apply_pooling(xb, pooling, n_patches)   # no-op\n",
    "        pred=model(xb)\n",
    "        pred=F.normalize(pred, dim=-1)\n",
    "        sims=pred @ Gi.t()                         # (b, M)\n",
    "        for j in range(sims.size(0)):\n",
    "            true_idx = int(cap2gal_local[i+j])     # local gallery index\n",
    "            order = torch.argsort(sims[j], descending=True)\n",
    "            rank = (order==true_idx).nonzero(as_tuple=False).item() + 1\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    mrr = float(np.mean(1.0 / ranks))\n",
    "    r1  = float(np.mean(ranks<=1))\n",
    "    r5  = float(np.mean(ranks<=5))\n",
    "    r10 = float(np.mean(ranks<=10))\n",
    "    return {\n",
    "        \"MRR\": mrr,\n",
    "        \"R1\": r1,\n",
    "        \"R5\": r5,\n",
    "        \"R10\": r10,\n",
    "        \"rank_median\": int(np.median(ranks)),\n",
    "        \"rank_p75\": int(np.percentile(ranks, 75))\n",
    "    }\n",
    "\n",
    "def count_params_mb(model):\n",
    "    params=sum(p.numel() for p in model.parameters())\n",
    "    mb = params * 4 / (1024**2)\n",
    "    return params, mb\n",
    "\n",
    "def time_ms_per_query(model, din, pooling, n_patches):\n",
    "    device=next(model.parameters()).device\n",
    "    x=torch.randn(2048, din, device=device)\n",
    "    x=apply_pooling(x, pooling, n_patches)\n",
    "    if device.type==\"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0=time.time()\n",
    "    with torch.no_grad(): _=model(x)\n",
    "    if device.type==\"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    ms_gpu=(time.time()-t0)*1000/len(x)\n",
    "    # CPU\n",
    "    mcpu=model.to(\"cpu\"); xcpu=x.to(\"cpu\")\n",
    "    t1=time.time()\n",
    "    with torch.no_grad(): _=mcpu(xcpu)\n",
    "    ms_cpu=(time.time()-t1)*1000/len(xcpu)\n",
    "    model.to(device)\n",
    "    return ms_gpu, ms_cpu\n",
    "\n",
    "# ---------- Train (for optional fine-tune) ----------\n",
    "def train_one(model, loader, opt, alpha, beta, gamma, moment_w, pooling, n_patches, device):\n",
    "    model.train(); total=0.0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        xb=apply_pooling(xb, pooling, n_patches)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        pred=model(xb)\n",
    "        # α·(1−cos) + β·MSE + λ·moment_align + γ·InfoNCE\n",
    "        cos = 1 - F.cosine_similarity(pred, yb, dim=-1).mean()\n",
    "        mse = F.mse_loss(pred, yb)\n",
    "        a_loss = moment_w * moment_align(pred, yb) if moment_w>0 else pred.new_tensor(0.0)\n",
    "        ce = info_nce(pred, yb) if gamma>0 else pred.new_tensor(0.0)\n",
    "        loss = alpha*cos + beta*mse + gamma*ce + a_loss\n",
    "        loss.backward(); opt.step()\n",
    "        total += loss.item()*xb.size(0)\n",
    "    return total/len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:22:00.810448Z",
     "iopub.status.busy": "2025-10-28T16:22:00.809811Z",
     "iopub.status.idle": "2025-10-28T16:22:08.061202Z",
     "shell.execute_reply": "2025-10-28T16:22:08.060158Z",
     "shell.execute_reply.started": "2025-10-28T16:22:00.810420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[meta] text_dim=1024 | image_dim=1536\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'out_check/train_detect.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/96513099.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# --- Load base data ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./out_check\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcap_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_ids_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALL_I\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mALL_img_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[data] captions={len(X):,}, images={len(ALL_I):,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[dims] text={X.shape}, image={Y.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37/2583156041.py\u001b[0m in \u001b[0;36mload_train\u001b[0;34m(out_dir)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mimg_ids_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m# image name per caption row (for splitting)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"n_text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n_images\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"train_detect.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcap_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_ids_row\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mwrite_text\u001b[0;34m(self, data, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                             data.__class__.__name__)\n\u001b[1;32m   1077\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'out_check/train_detect.json'"
     ]
    }
   ],
   "source": [
    "# check_data_splits_and_sampling.py\n",
    "import numpy as np, torch, json, random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "\n",
    "seed_all(42)\n",
    "\n",
    "# --- Load base data ---\n",
    "out_dir = Path(\"./out_check\")\n",
    "X, Y, cap_ids, img_ids_row, ALL_I, ALL_img_names = load_train(out_dir)\n",
    "print(f\"[data] captions={len(X):,}, images={len(ALL_I):,}\")\n",
    "print(f\"[dims] text={X.shape}, image={Y.shape}\")\n",
    "\n",
    "# --- Split by image-id (same logic as runner) ---\n",
    "cap_is_tr, cap_is_val, val_gallery, cap2gal_local, _ = build_image_id_split(\n",
    "    img_ids_row, ALL_img_names, ALL_I, X, Y, val_ratio=0.1, seed=42, out_dir=out_dir\n",
    ")\n",
    "print(f\"[split] train captions: {cap_is_tr.sum()} | val captions: {cap_is_val.sum()}\")\n",
    "print(f\"[split] train unique imgs: {len(set(img_ids_row[cap_is_tr]))} | \"\n",
    "      f\"val unique imgs: {len(set(img_ids_row[cap_is_val]))}\")\n",
    "\n",
    "# --- Frequency-aware sampling (F0.2) ---\n",
    "alpha = 0.5\n",
    "ids_train = img_ids_row[cap_is_tr]\n",
    "cnt = Counter(map(str, ids_train))\n",
    "print(\"\\n[top 10 most frequent images before weighting]\")\n",
    "for k, v in Counter(cnt).most_common(10):\n",
    "    print(f\"  {k:<30} {v}\")\n",
    "\n",
    "print(\"\\n✅ Done. No data were modified on disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captions: 125k, Images: 25k ⇒ exactly 5 captions/image (125k / 25k = 5).\n",
    "\n",
    "Train: 112,500 captions over 22,500 images; Val: 12,500 captions over 2,500 images. Clean 90/10 split by unique image ids → good for avoiding leakage. \n",
    "\n",
    "Each listed image shows count = 5. Since all images have 5 captions, there’s no frequency skew in this dataset. Good news: balanced multi-caption coverage.\n",
    "\n",
    "sampling is effectively uniform over images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can safely skip the frequency-aware sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:22:12.267436Z",
     "iopub.status.busy": "2025-10-28T16:22:12.266842Z",
     "iopub.status.idle": "2025-10-28T16:22:12.301990Z",
     "shell.execute_reply": "2025-10-28T16:22:12.300917Z",
     "shell.execute_reply.started": "2025-10-28T16:22:12.267406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==== BIG-JUMP ADDITIONS (paste in your runner.py after GeomLinear and before losses/train) ====\n",
    "\n",
    "# ---------- Residual Adapter on top of Geometry ----------\n",
    "class ResidualAdapter(nn.Module):\n",
    "    def __init__(self, din=1024, hidden=768, dout=1536, pdrop=0.1, init_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(din, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dout)\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        # scale down residual at start\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.mul_(init_scale)\n",
    "            self.fc2.weight.mul_(init_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop(h)\n",
    "        return self.fc2(h)\n",
    "\n",
    "class GeomWithAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    y_hat = GeomLinear(x) + Adapter(x)\n",
    "    Geom (A,b) is frozen for epoch 1; optionally unfrozen later with small LR.\n",
    "    \"\"\"\n",
    "    def __init__(self, A: np.ndarray, b: np.ndarray, din=1024, dout=1536, hidden=768, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.geom = GeomLinear(A, b)\n",
    "        for p in self.geom.parameters():\n",
    "            p.requires_grad = False  # warm start frozen\n",
    "        self.adapter = ResidualAdapter(din=din, hidden=hidden, dout=dout, pdrop=pdrop)\n",
    "\n",
    "    def unfreeze_geom(self):\n",
    "        for p in self.geom.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.geom(x) + self.adapter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:22:26.134812Z",
     "iopub.status.busy": "2025-10-28T16:22:26.134270Z",
     "iopub.status.idle": "2025-10-28T16:22:26.140773Z",
     "shell.execute_reply": "2025-10-28T16:22:26.139887Z",
     "shell.execute_reply.started": "2025-10-28T16:22:26.134791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def agreement_loss(names, preds, eps=1e-8):\n",
    "    \"\"\"\n",
    "    names: list[str] length B (image ids per caption)\n",
    "    preds: (B, D) unnormalized predictions\n",
    "    Returns mean variance across groups (after L2-norm), scalar.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        groups = {}\n",
    "        for i, n in enumerate(map(str, names)):\n",
    "            groups.setdefault(n, []).append(i)\n",
    "        valid = [idxs for idxs in groups.values() if len(idxs) >= 2]\n",
    "    if not valid:\n",
    "        return preds.new_tensor(0.0)\n",
    "    z = F.normalize(preds, dim=-1)\n",
    "    vars_ = []\n",
    "    for idxs in valid:\n",
    "        g = z[idxs]\n",
    "        v = g.var(dim=0, unbiased=False).mean()\n",
    "        vars_.append(v)\n",
    "    return torch.stack(vars_).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:22:28.607227Z",
     "iopub.status.busy": "2025-10-28T16:22:28.606470Z",
     "iopub.status.idle": "2025-10-28T16:22:28.615796Z",
     "shell.execute_reply": "2025-10-28T16:22:28.614982Z",
     "shell.execute_reply.started": "2025-10-28T16:22:28.607200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImgQueue:\n",
    "    def __init__(self, dim: int, capacity: int, device: torch.device):\n",
    "        self.capacity = int(capacity)\n",
    "        self.device = device\n",
    "        self.ptr = 0                  # write pointer into a circular buffer\n",
    "        self.full = False\n",
    "        self.bank = torch.zeros(self.capacity, dim, device=device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def enqueue(self, feats: torch.Tensor):\n",
    "        # store as negatives only; avoid holding computation graphs\n",
    "        feats = feats.detach()\n",
    "        n = feats.size(0)\n",
    "        if n >= self.capacity:\n",
    "            # keep only the most recent 'capacity' entries\n",
    "            self.bank.copy_(feats[-self.capacity:])\n",
    "            self.ptr = 0\n",
    "            self.full = True\n",
    "            return\n",
    "        end = self.ptr + n\n",
    "        if end <= self.capacity:\n",
    "            self.bank[self.ptr:end] = feats\n",
    "        else:\n",
    "            cut = self.capacity - self.ptr\n",
    "            self.bank[self.ptr:] = feats[:cut]\n",
    "            self.bank[:end - self.capacity] = feats[cut:]\n",
    "        self.ptr = (self.ptr + n) % self.capacity\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def size(self) -> int:\n",
    "        # correct empty vs full handling\n",
    "        if self.full:\n",
    "            return self.capacity\n",
    "        return self.ptr  # 0 when empty\n",
    "\n",
    "    def get(self) -> torch.Tensor:\n",
    "        # return FIFO-ordered contents (oldest -> newest)\n",
    "        if self.size() == 0:\n",
    "            return self.bank[:0]  # empty tensor with correct shape\n",
    "        if self.full:\n",
    "            # [ptr:cap] then [0:ptr] so that tail is the most recent\n",
    "            return torch.cat([self.bank[self.ptr:], self.bank[:self.ptr]], dim=0)\n",
    "        # not full: contents are [0:ptr)\n",
    "        return self.bank[:self.ptr]\n",
    "\n",
    "    def recent(self, max_items: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return only the freshest 'max_items' features (chronological).\n",
    "        \"\"\"\n",
    "        q = self.get()\n",
    "        if q is None or q.numel() == 0:\n",
    "            return q\n",
    "        if max_items is None or q.size(0) <= int(max_items):\n",
    "            return q\n",
    "        return q[-int(max_items):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T21:21:30.128410Z",
     "iopub.status.busy": "2025-10-27T21:21:30.128053Z",
     "iopub.status.idle": "2025-10-27T21:21:30.150870Z",
     "shell.execute_reply": "2025-10-27T21:21:30.150255Z",
     "shell.execute_reply.started": "2025-10-27T21:21:30.128380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------- viz_utils.py (drop-in) -----------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import List, Sequence, Optional, Tuple\n",
    "\n",
    "@torch.inference_mode()\n",
    "def visualize_retrieval(\n",
    "    pred_emb: torch.Tensor,              # (D,) predicted caption->image vector (already in image space)\n",
    "    gt_index: int,                       # ground truth image index (into image_files / image_embeddings)\n",
    "    image_files: Sequence[str],\n",
    "    caption_text: str,\n",
    "    image_embeddings: torch.Tensor,      # (N, D) gallery in same space\n",
    "    k: int = 5,\n",
    "    dataset_path: str = \"/kaggle/working/data/train/train\",\n",
    ") -> Tuple[bool, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Show ground-truth image + top-k retrieved images. Highlights correct match if present.\n",
    "    Returns: (gt_in_topk, gt_rank or None)\n",
    "    \"\"\"\n",
    "    # Ensure CPU + float\n",
    "    pred = F.normalize(pred_emb.view(1, -1), dim=-1).cpu()\n",
    "    gallery = F.normalize(image_embeddings, dim=-1).cpu()\n",
    "\n",
    "    # cosine similarities\n",
    "    sims = (pred @ gallery.T).squeeze(0).numpy()  # (N,)\n",
    "    retrieved = np.argsort(-sims)[:k]\n",
    "    gt_in_topk = int(gt_index) in retrieved\n",
    "    gt_rank = int(np.where(retrieved == gt_index)[0][0] + 1) if gt_in_topk else None\n",
    "\n",
    "    # figure\n",
    "    fig, axes = plt.subplots(1, k + 1, figsize=(22, 4))\n",
    "\n",
    "    def _load_show(ax, img_name, title, color=None, weight=None):\n",
    "        ax.axis(\"off\")\n",
    "        p = Path(dataset_path) / \"Images\" / img_name\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "        except Exception:\n",
    "            ax.text(0.5, 0.5, f\"Image not found:\\n{img_name}\", ha=\"center\", va=\"center\")\n",
    "        if color or weight:\n",
    "            ax.set_title(title, fontsize=10, color=color or \"black\", fontweight=weight or \"normal\")\n",
    "        else:\n",
    "            ax.set_title(title, fontsize=10)\n",
    "\n",
    "    # GT on the left\n",
    "    gt_name = image_files[int(gt_index)]\n",
    "    _load_show(axes[0], gt_name, f\"Ground Truth\\n{gt_name[:28]}...\", color=\"green\", weight=\"bold\")\n",
    "\n",
    "    # Top-k\n",
    "    for i, idx in enumerate(retrieved):\n",
    "        name = image_files[int(idx)]\n",
    "        is_gt = (int(idx) == int(gt_index))\n",
    "        title = f\"Rank {i+1}\\nCos: {sims[idx]:.3f}\" + (\"\\n✓ CORRECT\" if is_gt else \"\")\n",
    "        _load_show(axes[i+1], name, title, color=(\"green\" if is_gt else None), weight=(\"bold\" if is_gt else None))\n",
    "\n",
    "    plt.suptitle(f\"Query caption: “{caption_text}”\\nStatus: {'✓ Found at rank '+str(gt_rank) if gt_in_topk else f'✗ Not in top-{k}'}\",\n",
    "                 fontsize=13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return bool(gt_in_topk), (int(gt_rank) if gt_rank is not None else None)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def visualize_samples(\n",
    "    model: torch.nn.Module,\n",
    "    text_embeddings: torch.Tensor,       # (M, din) caption embeddings (VAL)\n",
    "    image_embeddings: torch.Tensor,      # (N, D) gallery (VAL)\n",
    "    image_index_of_caption: Sequence[int],# length M: for each caption row, the GT image index\n",
    "    image_files: Sequence[str],          # length N image filenames\n",
    "    caption_texts: Optional[Sequence[str]] = None,  # optional strings to show (len M). If None, uses \"caption i\"\n",
    "    n_examples: int = 6,\n",
    "    k: int = 5,\n",
    "    device: Optional[torch.device] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Picks n_examples random caption rows, runs them through `model`, and calls visualize_retrieval().\n",
    "    Also prints the current gate value g (if your model has .gate_raw).\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    device = device or model_device\n",
    "\n",
    "    # Report gating scalar (if present)\n",
    "    g = None\n",
    "    if hasattr(model, \"gate_raw\"):\n",
    "        g = torch.sigmoid(model.gate_raw.detach().to(\"cpu\")).item()\n",
    "        print(f\"[gate] adapter gate g = {g:.4f} (0=off, 1=full)\")\n",
    "\n",
    "    M = text_embeddings.shape[0]\n",
    "    idxs = torch.randperm(M)[:n_examples].tolist()\n",
    "\n",
    "    for i, row in enumerate(idxs, 1):\n",
    "        x = text_embeddings[row:row+1].to(device)   # (1, din)\n",
    "        pred = model(x).squeeze(0)                  # (D,)\n",
    "        gt_idx = int(image_index_of_caption[row])\n",
    "        cap_txt = caption_texts[row] if (caption_texts is not None) else f\"caption {row}\"\n",
    "        print(f\"\\n[{i}/{n_examples}] cap_row={row} → GT image idx={gt_idx}\")\n",
    "        visualize_retrieval(\n",
    "            pred_emb=pred,\n",
    "            gt_index=gt_idx,\n",
    "            image_files=image_files,\n",
    "            caption_text=cap_txt,\n",
    "            image_embeddings=image_embeddings,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def compute_gt_ranks(\n",
    "    model: torch.nn.Module,\n",
    "    text_embeddings: torch.Tensor,         # (M, din)\n",
    "    image_embeddings: torch.Tensor,        # (N, D)\n",
    "    image_index_of_caption: Sequence[int], # (M,)\n",
    "    batch_size: int = 256,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an array of size (M,) with 1-based ranks of the ground-truth image for each caption.\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    device = device or model_device\n",
    "\n",
    "    # Pre-norm gallery on device for speed\n",
    "    gallery = F.normalize(image_embeddings.to(device), dim=-1)      # (N, D)\n",
    "\n",
    "    ranks = []\n",
    "    M = text_embeddings.shape[0]\n",
    "    for i in range(0, M, batch_size):\n",
    "        xb = text_embeddings[i:i+batch_size].to(device)\n",
    "        pred = model(xb)                                            # (B, D)\n",
    "        pred = F.normalize(pred, dim=-1)\n",
    "        sims = pred @ gallery.T                                     # (B, N)\n",
    "        order = torch.argsort(sims, dim=1, descending=True)         # (B, N)\n",
    "        gt = torch.as_tensor(image_index_of_caption[i:i+xb.size(0)], device=device).view(-1, 1)\n",
    "        # find rank (1-based)\n",
    "        # We locate GT in each row\n",
    "        eq = (order == gt).nonzero(as_tuple=False)\n",
    "        # eq has rows: [row_idx, col_idx], where col_idx is the rank-1\n",
    "        pos = torch.zeros(xb.size(0), dtype=torch.long, device=device)\n",
    "        pos[eq[:,0]] = eq[:,1] + 1\n",
    "        ranks.append(pos.detach().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(ranks, axis=0)\n",
    "\n",
    "\n",
    "def plot_rank_hist(ranks: np.ndarray, k_marks: Tuple[int, ...] = (1,5,10,50)):\n",
    "    \"\"\"\n",
    "    Simple histogram of ranks with vertical lines for common top-k cutoffs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    # do NOT set colors explicitly (keeps your style guide clean)\n",
    "    plt.hist(ranks, bins=1000)\n",
    "    for k in k_marks:\n",
    "        plt.axvline(k, linestyle=\"--\")\n",
    "    top1 = (ranks <= 1).mean()*100\n",
    "    top5 = (ranks <= 5).mean()*100\n",
    "    top10 = (ranks <= 10).mean()*100\n",
    "    plt.title(f\"GT rank distribution | R@1={top1:.1f}%  R@5={top5:.1f}%  R@10={top10:.1f}%\")\n",
    "    plt.xlabel(\"Ground-truth rank (1=best)\")\n",
    "    plt.ylabel(\"# captions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings for new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:05.804309Z",
     "iopub.status.busy": "2025-10-28T16:24:05.803678Z",
     "iopub.status.idle": "2025-10-28T16:24:05.817922Z",
     "shell.execute_reply": "2025-10-28T16:24:05.817018Z",
     "shell.execute_reply.started": "2025-10-28T16:24:05.804276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== P×K Sampler (F1.1) ===============\n",
    "class PKBatchSampler(torch.utils.data.Sampler[list[int]]):\n",
    "    \"\"\"\n",
    "    Samples batches with P unique images, each with K captions.\n",
    "    Use in DataLoader(batch_sampler=PKBatchSampler(...)).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ids: list[str], P: int, K: int, drop_last: bool = True, seed: int = 42):\n",
    "        # group dataset indices by image id\n",
    "        self.by_img = {}\n",
    "        for idx, iid in enumerate(map(str, img_ids)):\n",
    "            self.by_img.setdefault(iid, []).append(idx)\n",
    "        # shuffle within each image's index list\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        for lst in self.by_img.values():\n",
    "            perm = torch.randperm(len(lst), generator=g).tolist()\n",
    "            lst[:] = [lst[i] for i in perm]\n",
    "\n",
    "        self.P, self.K, self.drop_last = int(P), int(K), bool(drop_last)\n",
    "        self.iids = list(self.by_img.keys())\n",
    "        self.cur = {iid: 0 for iid in self.iids}\n",
    "        self.pool = [iid for iid in self.iids if len(self.by_img[iid]) > 0]\n",
    "\n",
    "    def __iter__(self):\n",
    "        import random\n",
    "        pool = self.pool[:]\n",
    "        random.shuffle(pool)\n",
    "        batch = []\n",
    "        while len(pool) >= self.P:\n",
    "            pick = pool[:self.P]\n",
    "            pool = pool[self.P:]\n",
    "            for iid in pick:\n",
    "                start = self.cur[iid]\n",
    "                end = start + self.K\n",
    "                lst = self.by_img[iid]\n",
    "                if start >= len(lst):\n",
    "                    continue\n",
    "                take = lst[start:min(end, len(lst))]\n",
    "                self.cur[iid] = min(end, len(lst))\n",
    "                batch.extend(take)\n",
    "            if len(batch) == self.P * self.K:\n",
    "                yield batch\n",
    "            else:\n",
    "                if not self.drop_last and len(batch) > 0:\n",
    "                    yield batch\n",
    "                break\n",
    "            batch = []\n",
    "\n",
    "    def __len__(self):\n",
    "        total = sum(len(v) // self.K for v in self.by_img.values())\n",
    "        return max(0, total // self.P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:08.254563Z",
     "iopub.status.busy": "2025-10-28T16:24:08.253985Z",
     "iopub.status.idle": "2025-10-28T16:24:08.266015Z",
     "shell.execute_reply": "2025-10-28T16:24:08.265378Z",
     "shell.execute_reply.started": "2025-10-28T16:24:08.254541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Cross-batch Positives Buffer (F1.2) ===============\n",
    "from collections import deque\n",
    "\n",
    "class XBPBuffer:\n",
    "    \"\"\"\n",
    "    Keeps recent predicted caption embeddings as extra positives per image.\n",
    "    Read-only in loss; write after optimizer step.\n",
    "    \"\"\"\n",
    "    def __init__(self, per_image_cap: int = 4, global_cap: int = 32000, device: torch.device = torch.device(\"cpu\")):\n",
    "        self.per_img = {}            # iid -> deque of normalized tensors\n",
    "        self.order = deque()         # (iid, stamp) to enforce global cap\n",
    "        self.per_image_cap = int(per_image_cap)\n",
    "        self.global_cap = int(global_cap)\n",
    "        self.device = device\n",
    "        self._count = 0\n",
    "        self._stamp = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add(self, img_ids: list[str], pred_feats: torch.Tensor):\n",
    "        feats = F.normalize(pred_feats.detach(), dim=-1)\n",
    "        for iid, f in zip(map(str, img_ids), feats):\n",
    "            dq = self.per_img.setdefault(iid, deque(maxlen=self.per_image_cap))\n",
    "            dq.append(f.to(self.device))\n",
    "            self.order.append((iid, self._stamp))\n",
    "            self._stamp += 1\n",
    "            self._count += 1\n",
    "            # trim global\n",
    "            while self._count > self.global_cap:\n",
    "                old_iid, _ = self.order.popleft()\n",
    "                if self.per_img.get(old_iid):\n",
    "                    try:\n",
    "                        self.per_img[old_iid].popleft()\n",
    "                        self._count -= 1\n",
    "                        if len(self.per_img[old_iid]) == 0:\n",
    "                            self.per_img.pop(old_iid, None)\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "\n",
    "    def build_bank_and_mask(self, batch_img_names: list[str], device: torch.device):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          xbp_bank: (M, D) stacked positives from XBP across batch (M can be 0)\n",
    "          xbp_pos_cols_per_row: list[list[int]] with column indices into xbp_bank for each row\n",
    "        \"\"\"\n",
    "        rows = [str(x) for x in batch_img_names]\n",
    "        per_row_feats = []\n",
    "        row_counts = []\n",
    "        for iid in rows:\n",
    "            dq = self.per_img.get(iid, None)\n",
    "            if dq is None or len(dq) == 0:\n",
    "                row_counts.append(0)\n",
    "            else:\n",
    "                row_counts.append(len(dq))\n",
    "                per_row_feats.extend(list(dq))\n",
    "        if len(per_row_feats) == 0:\n",
    "            return torch.empty(0, 0, device=device), [[] for _ in rows]\n",
    "        xbp_bank = torch.stack(per_row_feats, dim=0).to(device)\n",
    "        xbp_pos_cols = []\n",
    "        offset = 0\n",
    "        for cnt in row_counts:\n",
    "            if cnt == 0:\n",
    "                xbp_pos_cols.append([])\n",
    "            else:\n",
    "                xbp_pos_cols.append(list(range(offset, offset + cnt)))\n",
    "                offset += cnt\n",
    "        return xbp_bank, xbp_pos_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:11.397087Z",
     "iopub.status.busy": "2025-10-28T16:24:11.396288Z",
     "iopub.status.idle": "2025-10-28T16:24:11.402215Z",
     "shell.execute_reply": "2025-10-28T16:24:11.401409Z",
     "shell.execute_reply.started": "2025-10-28T16:24:11.397060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Hard-subset Mining (F2.1) ===============\n",
    "@torch.no_grad()\n",
    "def mine_hard(q_recent: torch.Tensor, pred: torch.Tensor, H: int, exclude_feats: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    q_recent: (Q,D) normalized features to mine from (negatives pool)\n",
    "    pred:     (B,D) normalized anchor predictions\n",
    "    Returns:\n",
    "      idx: (B,H) indices into q_recent for each anchor\n",
    "      flat_idx: (B*H,) flattened indices\n",
    "      mined: (B*H,D) the mined subset\n",
    "    \"\"\"\n",
    "    if q_recent is None or q_recent.numel() == 0 or H <= 0:\n",
    "        return None, None, None\n",
    "    B = pred.size(0)\n",
    "    Q = q_recent.size(0)\n",
    "    sims = pred @ q_recent.t()  # (B,Q)\n",
    "    H = min(int(H), Q)\n",
    "    _, idx = torch.topk(sims, k=H, dim=1, largest=True, sorted=False)  # (B,H)\n",
    "    flat_idx = idx.reshape(-1)\n",
    "    mined = q_recent.index_select(0, flat_idx)      # (B*H,D)\n",
    "    return idx, flat_idx, mined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:13.597492Z",
     "iopub.status.busy": "2025-10-28T16:24:13.596910Z",
     "iopub.status.idle": "2025-10-28T16:24:13.602195Z",
     "shell.execute_reply": "2025-10-28T16:24:13.601309Z",
     "shell.execute_reply.started": "2025-10-28T16:24:13.597467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Debiased NT-Xent denominator (F2.2) ===============\n",
    "def _debiased_logZ(logits: torch.Tensor, pos_mask: torch.Tensor, class_prior: float = 0.01):\n",
    "    \"\"\"\n",
    "    Chuang et al., 2020 (Debiased Contrastive Learning) — practical variant:\n",
    "      Z* = sum_k exp(l_ik) - π * sum_{j in P_i} exp(l_ij)\n",
    "    \"\"\"\n",
    "    exp_all = torch.exp(logits)          # (B,K)\n",
    "    exp_pos = exp_all * pos_mask\n",
    "    Z = exp_all.sum(dim=1)               # (B,)\n",
    "    corr = class_prior * exp_pos.sum(dim=1)\n",
    "    Z_star = torch.clamp(Z - corr, min=1e-8)\n",
    "    return torch.log(Z_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:16.280264Z",
     "iopub.status.busy": "2025-10-28T16:24:16.279756Z",
     "iopub.status.idle": "2025-10-28T16:24:16.289923Z",
     "shell.execute_reply": "2025-10-28T16:24:16.289310Z",
     "shell.execute_reply.started": "2025-10-28T16:24:16.280222Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Multi-positive InfoNCE with XBP, Hard Mining, DCL (F1.1+F1.2+F2.1+F2.2) ===============\n",
    "def info_nce_multi(\n",
    "    pred,                                    # (B,D) unnormed\n",
    "    batch_img_targets,                       # (B,D)\n",
    "    batch_img_names,                         # list length B\n",
    "    queue_feats=None,                        # (Q,D) negatives pool\n",
    "    tau: float = 0.07,\n",
    "    xbp_bank: torch.Tensor = None,           # (M,D) extra positives\n",
    "    xbp_pos_cols_per_row: list[list[int]] = None,\n",
    "    hard_subset_H: int = 0,\n",
    "    use_dcl: bool = False,\n",
    "    dcl_prior: float = 0.01,\n",
    "):\n",
    "    p = F.normalize(pred, dim=-1)\n",
    "    t = F.normalize(batch_img_targets, dim=-1)\n",
    "\n",
    "    # base bank block 0: in-batch targets (positives exist here)\n",
    "    bank_blocks = [t]\n",
    "    B = pred.size(0)\n",
    "\n",
    "    # optional hard mining over queue\n",
    "    if queue_feats is not None and queue_feats.numel() > 0:\n",
    "        qn = F.normalize(queue_feats, dim=-1)\n",
    "        if hard_subset_H and hard_subset_H > 0:\n",
    "            _, _, mined_block = mine_hard(qn, p, int(hard_subset_H))\n",
    "            bank_blocks.append(mined_block)\n",
    "        else:\n",
    "            bank_blocks.append(qn)\n",
    "\n",
    "    # optional XBP block\n",
    "    xbp_offsets = None\n",
    "    if xbp_bank is not None and xbp_bank.numel() > 0:\n",
    "        xbp_offsets = sum(b.size(0) for b in bank_blocks)  # start col for XBP block\n",
    "        bank_blocks.append(xbp_bank)\n",
    "\n",
    "    bank = torch.cat(bank_blocks, dim=0) if len(bank_blocks) > 1 else bank_blocks[0]  # (K,D)\n",
    "    logits = (p @ bank.t()) / float(tau)                                              # (B,K)\n",
    "    K = bank.size(0)\n",
    "\n",
    "    # build pos mask: in-batch + XBP\n",
    "    pos_mask = torch.zeros(B, K, dtype=torch.bool, device=pred.device)\n",
    "    names = list(map(str, batch_img_names))\n",
    "    for i in range(B):\n",
    "        for j in range(B):\n",
    "            if names[i] == names[j]:\n",
    "                pos_mask[i, j] = True\n",
    "    if xbp_offsets is not None and xbp_pos_cols_per_row is not None:\n",
    "        for i, cols in enumerate(xbp_pos_cols_per_row):\n",
    "            for c in cols:\n",
    "                pos_mask[i, xbp_offsets + c] = True\n",
    "\n",
    "    assert pos_mask.any(dim=1).all(), \"Every row must have at least one positive.\"\n",
    "\n",
    "    # denominator: standard vs debiased\n",
    "    if not use_dcl:\n",
    "        logZ = torch.logsumexp(logits, dim=1)\n",
    "    else:\n",
    "        logZ = _debiased_logZ(logits, pos_mask, class_prior=float(dcl_prior))\n",
    "\n",
    "    logits_pos = logits.masked_fill(~pos_mask, float('-inf'))\n",
    "    logPos = torch.logsumexp(logits_pos, dim=1)\n",
    "    return (-(logPos - logZ)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:18.894882Z",
     "iopub.status.busy": "2025-10-28T16:24:18.894270Z",
     "iopub.status.idle": "2025-10-28T16:24:18.904889Z",
     "shell.execute_reply": "2025-10-28T16:24:18.903955Z",
     "shell.execute_reply.started": "2025-10-28T16:24:18.894847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Dual Queue: Prediction FIFO (F2.3) ===============\n",
    "class PredQueue:\n",
    "    def __init__(self, dim: int, capacity: int, device: torch.device):\n",
    "        self.capacity = int(capacity)\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        self.feats = torch.zeros(self.capacity, dim, device=device)  # normalized feats\n",
    "        self.ids = torch.empty(self.capacity, dtype=torch.long, device=device)  # hashed img ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def enqueue(self, feats: torch.Tensor, img_ids: list[str]):\n",
    "        feats = F.normalize(feats.detach(), dim=-1)\n",
    "        ids = torch.tensor([hash(str(i)) for i in img_ids], dtype=torch.long, device=self.device)\n",
    "        n = feats.size(0)\n",
    "        if n >= self.capacity:\n",
    "            self.feats.copy_(feats[-self.capacity:])\n",
    "            self.ids.copy_(ids[-self.capacity:])\n",
    "            self.ptr = 0\n",
    "            self.full = True\n",
    "            return\n",
    "        end = self.ptr + n\n",
    "        if end <= self.capacity:\n",
    "            self.feats[self.ptr:end] = feats\n",
    "            self.ids[self.ptr:end] = ids\n",
    "        else:\n",
    "            cut = self.capacity - self.ptr\n",
    "            self.feats[self.ptr:] = feats[:cut]\n",
    "            self.ids[self.ptr:] = ids[:cut]\n",
    "            self.feats[:end - self.capacity] = feats[cut:]\n",
    "            self.ids[:end - self.capacity] = ids[cut:]\n",
    "        self.ptr = (self.ptr + n) % self.capacity\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def size(self) -> int:\n",
    "        if self.full:\n",
    "            return self.capacity\n",
    "        return self.ptr\n",
    "\n",
    "    def recent(self, max_items: int):\n",
    "        n = self.size()\n",
    "        if n == 0:\n",
    "            return self.feats[:0], self.ids[:0]\n",
    "        k = min(int(max_items), n) if max_items is not None else n\n",
    "        if self.full:\n",
    "            idx = torch.arange(self.ptr - n, self.ptr, device=self.device) % self.capacity\n",
    "        else:\n",
    "            idx = torch.arange(0, n, device=self.device)\n",
    "        idx = idx[-k:]\n",
    "        return self.feats.index_select(0, idx), self.ids.index_select(0, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:21.276971Z",
     "iopub.status.busy": "2025-10-28T16:24:21.276679Z",
     "iopub.status.idle": "2025-10-28T16:24:21.310510Z",
     "shell.execute_reply": "2025-10-28T16:24:21.309816Z",
     "shell.execute_reply.started": "2025-10-28T16:24:21.276948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "#       TRAIN BIGJUMP\n",
    "# =========================\n",
    "def train_bigjump(\n",
    "    model,\n",
    "    Xtr, Ytr, img_ids_row_tr,\n",
    "    Xva, val_gallery, cap2gal_local,\n",
    "    batch=512, epochs=20, base_lr=2e-4, wd=1e-4,\n",
    "    # legacy-friendly knobs\n",
    "    tau=None,                 # if set, fixed τ; else curriculum below\n",
    "    alpha_cos=0.5,            # cosine aux weight\n",
    "    lambda_moment=0.02,       # moment matching weight\n",
    "    queue_size=65536,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    pooling=\"none\", n_patches=None,\n",
    "    out_dir: \"Path | str\" = None, seed=42,\n",
    "    # curriculum (when tau is None)\n",
    "    tau_start=0.10, tau_end=0.06,\n",
    "    queue_warmup_epochs=2,             # ep1–2: exclude queue from loss\n",
    "    queue_recent_schedule=(16000, 32000, 65536),\n",
    "    lambda_agree=0.05,                 # caption-agreement\n",
    "    geom_unfreeze_epoch=3,             # unfreeze A at ep3; 0 disables\n",
    "    geom_lr_scale=0.05,\n",
    "    # ========= NEW FEATURES =========\n",
    "    P: int = 256, K_pk: int = 2, use_pk: bool = False,     # F1.1 PK sampler\n",
    "    xbp_per_img: int = 4, xbp_global: int = 32000,         # F1.2 XBP buffer\n",
    "    use_dcl: bool = False, dcl_prior: float = 0.01,        # F2.2 DCL\n",
    "    mine_H: int = 64,                                      # F2.1 hard mining\n",
    "    queue_pred_capacity: int = 65536, use_pred_queue: bool = False,  # F2.3 dual queues\n",
    "    **_ignored\n",
    "):\n",
    "    \"\"\"\n",
    "    MRR-focused trainer with:\n",
    "      - Multi-positive InfoNCE (MIL-NCE style) using in-batch positives\n",
    "      - Optional P×K sampler per image (F1.1)\n",
    "      - Cross-batch positives buffer (XBP) (F1.2)\n",
    "      - Hard-subset negative mining from recent queue slice (F2.1)\n",
    "      - Debiased NT-Xent denominator correction (F2.2)\n",
    "      - Dual queues: image targets + past predictions (F2.3)\n",
    "      - τ curriculum if tau=None\n",
    "      - Queue warm-up & recency schedule\n",
    "      - Agreement loss across captions of same image\n",
    "      - Late tiny unfreeze of geometry matrix A\n",
    "    \"\"\"\n",
    "    import math, json\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # --- prep IO\n",
    "    out_dir = Path(out_dir) if out_dir is not None else Path(\"./outputs/bigjump\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- seed & device\n",
    "    seed_all(seed)\n",
    "    model.to(device)\n",
    "\n",
    "    # --- dataset\n",
    "    class TripletSet(torch.utils.data.Dataset):\n",
    "        def __init__(self, X_np, Y_np, names_np):\n",
    "            self.X = torch.from_numpy(X_np).float()\n",
    "            self.Y = torch.from_numpy(Y_np).float()\n",
    "            self.N = [str(n) for n in names_np.tolist()]\n",
    "            assert len(self.X) == len(self.Y) == len(self.N)\n",
    "        def __len__(self):  return len(self.X)\n",
    "        def __getitem__(self, i):  return self.X[i], self.Y[i], self.N[i]\n",
    "\n",
    "    ds = TripletSet(Xtr, Ytr, img_ids_row_tr)\n",
    "\n",
    "    # --- DataLoader: PK sampler or vanilla\n",
    "    if use_pk:\n",
    "        pk_sampler = PKBatchSampler(img_ids_row_tr.tolist(), P=P, K=K_pk, drop_last=True, seed=seed)\n",
    "        dl = DataLoader(ds, batch_sampler=pk_sampler, num_workers=2, pin_memory=True)\n",
    "    else:\n",
    "        dl = DataLoader(ds, batch_size=batch, shuffle=True, num_workers=2, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # --- optimizer (geometry frozen to start)\n",
    "    params_adapter = list(model.adapter.parameters()) if hasattr(model, \"adapter\") else list(model.parameters())\n",
    "    params_geom    = list(model.geom.parameters()) if hasattr(model, \"geom\") else []\n",
    "    for p in params_geom:  p.requires_grad = False\n",
    "\n",
    "    opt = torch.optim.AdamW([\n",
    "        {\"params\": params_adapter, \"lr\": base_lr, \"weight_decay\": wd},\n",
    "        {\"params\": params_geom,    \"lr\": base_lr * geom_lr_scale, \"weight_decay\": wd},  # tiny when later unfrozen\n",
    "    ])\n",
    "\n",
    "    # --- LR schedule: 1 epoch warmup → cosine to 0.1×\n",
    "    steps_per_epoch = max(1, len(dl))\n",
    "    total_steps = epochs * steps_per_epoch\n",
    "    warmup_steps = 1 * steps_per_epoch\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step + 1) / float(max(1, warmup_steps))\n",
    "        progress = (step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "    # --- τ schedule (epoch-based). If fixed tau given, keep constant.\n",
    "    def tau_at_epoch(e):\n",
    "        if tau is not None:\n",
    "            return float(tau)\n",
    "        t = (e - 1) / max(1, epochs - 1)\n",
    "        return tau_end + (tau_start - tau_end) * 0.5 * (1 + math.cos(math.pi * t))\n",
    "\n",
    "    # --- AMP + queues/buffers\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n",
    "    queue = ImgQueue(dim=Ytr.shape[1], capacity=int(queue_size), device=device)                # image-target queue\n",
    "    xbp = XBPBuffer(per_image_cap=xbp_per_img, global_cap=xbp_global, device=device)          # XBP positives\n",
    "    pred_queue = PredQueue(dim=Ytr.shape[1], capacity=int(queue_pred_capacity), device=device) if use_pred_queue else None\n",
    "\n",
    "    best_stats, best_mrr, best_ep = None, -1.0, 0\n",
    "    global_step = 0\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        # Late tiny unfreeze of geometry (A only)\n",
    "        if geom_unfreeze_epoch and ep == int(geom_unfreeze_epoch) and len(params_geom) > 0:\n",
    "            for n, p in model.geom.named_parameters():\n",
    "                p.requires_grad = False\n",
    "            for n, p in model.geom.named_parameters():\n",
    "                if 'A' in n:  # only the linear map, not the bias\n",
    "                    p.requires_grad = True\n",
    "            opt = torch.optim.AdamW([\n",
    "                {\"params\": list(model.adapter.parameters()) if hasattr(model, \"adapter\") else list(model.parameters()),\n",
    "                 \"lr\": base_lr, \"weight_decay\": wd},\n",
    "                {\"params\": [p for p in model.geom.parameters() if p.requires_grad],\n",
    "                 \"lr\": base_lr * geom_lr_scale, \"weight_decay\": wd},\n",
    "            ])\n",
    "            start_offset = global_step\n",
    "            def lr_lambda_rebuilt(step):\n",
    "                step_total = start_offset + step\n",
    "                if step_total < warmup_steps:\n",
    "                    return float(step_total + 1) / float(max(1, warmup_steps))\n",
    "                progress = (step_total - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "                return 0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda_rebuilt)\n",
    "\n",
    "        tau_curr = tau_at_epoch(ep)\n",
    "\n",
    "        # queue policy\n",
    "        use_queue = ep > int(queue_warmup_epochs)\n",
    "        if not use_queue:\n",
    "            recent_limit = 0\n",
    "        else:\n",
    "            if ep == queue_warmup_epochs + 1:\n",
    "                recent_limit = queue_recent_schedule[0]\n",
    "            elif ep == queue_warmup_epochs + 2:\n",
    "                recent_limit = queue_recent_schedule[1]\n",
    "            else:\n",
    "                recent_limit = queue_recent_schedule[-1]\n",
    "\n",
    "        running = 0.0\n",
    "\n",
    "        for step, (xb, yb, names) in enumerate(dl):\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            qb = None\n",
    "            if use_queue and recent_limit and queue.size() > 0:\n",
    "                qb = queue.recent(int(recent_limit))\n",
    "\n",
    "            qb_pred = (None, None)\n",
    "            if pred_queue is not None and use_queue and recent_limit and pred_queue.size() > 0:\n",
    "                qb_pred = pred_queue.recent(int(recent_limit))\n",
    "\n",
    "            # Build cross-batch positives (read-only)\n",
    "            xbp_bank, xbp_pos_cols = xbp.build_bank_and_mask(names, device=device)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.autocast(device_type=(\"cuda\" if device.type == \"cuda\" else \"cpu\"), enabled=True):\n",
    "                pred = model(xb)\n",
    "\n",
    "                # negatives pool: image queue feats (+ optional pred-queue feats)\n",
    "                q_feats = qb if qb is not None and qb.numel() > 0 else None\n",
    "                if qb_pred[0] is not None and qb_pred[0].numel() > 0:\n",
    "                    q_feats = qb_pred[0] if q_feats is None else torch.cat([q_feats, qb_pred[0]], dim=0)\n",
    "\n",
    "                # primary: multi-positive InfoNCE with XBP + Hard Mining + optional DCL\n",
    "                loss_nce = info_nce_multi(\n",
    "                    pred, yb, names,\n",
    "                    queue_feats=q_feats,\n",
    "                    tau=tau_curr,\n",
    "                    xbp_bank=xbp_bank, xbp_pos_cols_per_row=xbp_pos_cols,\n",
    "                    hard_subset_H=int(mine_H) if (mine_H and q_feats is not None) else 0,\n",
    "                    use_dcl=bool(use_dcl), dcl_prior=float(dcl_prior),\n",
    "                )\n",
    "\n",
    "                # aux: cosine + moment + caption agreement\n",
    "                loss_cos = (1.0 - F.cosine_similarity(pred, yb, dim=-1)).mean()\n",
    "                mu_loss  = moment_align(pred, yb) if lambda_moment > 0 else pred.new_tensor(0.0)\n",
    "                loss_agree = agreement_loss(names, pred) if lambda_agree > 0 else pred.new_tensor(0.0)\n",
    "\n",
    "                loss = loss_nce + alpha_cos * loss_cos + lambda_moment * mu_loss + lambda_agree * loss_agree\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            sched.step()\n",
    "\n",
    "            running += loss.item() * xb.size(0)\n",
    "            global_step += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # update queues/buffers (write-only)\n",
    "                queue.enqueue(F.normalize(yb, dim=-1))      # image features (negatives)\n",
    "                xbp.add(names, pred)                        # cross-batch positives\n",
    "                if pred_queue is not None:                  # prediction negatives\n",
    "                    pred_queue.enqueue(pred, names)\n",
    "\n",
    "        # ---- eval\n",
    "        stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "        ep_loss = running / len(ds)\n",
    "        print(\n",
    "            f\"[bigjump {ep:02d}] loss={ep_loss:.6f} | val_MRR={stats['MRR']:.4f} \"\n",
    "            f\"| R@1={stats['R1']:.3f} R@5={stats['R5']:.3f} R@10={stats['R10']:.3f} \"\n",
    "            f\"| median={stats['rank_median']} p75={stats['rank_p75']} \"\n",
    "            f\"| queue={queue.size()} | tau={tau_curr:.3f} | recentQ={recent_limit if use_queue else 0}\"\n",
    "        )\n",
    "\n",
    "        if stats[\"MRR\"] > best_mrr:\n",
    "            best_mrr, best_ep, best_stats = stats[\"MRR\"], ep, stats\n",
    "            torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"val\": stats}, out_dir / \"best.pt\")\n",
    "\n",
    "        # safety: if unfreeze hurt, lock geometry again\n",
    "        if geom_unfreeze_epoch and ep >= geom_unfreeze_epoch and stats[\"MRR\"] + 0.005 < best_mrr and len(params_geom) > 0:\n",
    "            for n, p in model.geom.named_parameters():\n",
    "                p.requires_grad = False\n",
    "            opt = torch.optim.AdamW([\n",
    "                {\"params\": list(model.adapter.parameters()) if hasattr(model, \"adapter\") else list(model.parameters()),\n",
    "                 \"lr\": base_lr, \"weight_decay\": wd},\n",
    "            ])\n",
    "            start_offset = global_step\n",
    "            def lr_lambda_refreeze(step):\n",
    "                step_total = start_offset + step\n",
    "                if step_total < warmup_steps:\n",
    "                    return float(step_total + 1) / float(max(1, warmup_steps))\n",
    "                progress = (step_total - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "                return 0.1 + 0.9 * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "            sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda_refreeze)\n",
    "\n",
    "    if best_stats is None:\n",
    "        best_stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "        torch.save({\"model\": model.state_dict(), \"epoch\": 0, \"val\": best_stats}, out_dir / \"best.pt\")\n",
    "\n",
    "    # dump best metrics\n",
    "    (out_dir / \"val_metrics.json\").write_text(json.dumps(dict(best_epoch=best_ep, **best_stats), indent=2))\n",
    "    return best_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:33.847198Z",
     "iopub.status.busy": "2025-10-28T16:24:33.846920Z",
     "iopub.status.idle": "2025-10-28T16:24:33.874852Z",
     "shell.execute_reply": "2025-10-28T16:24:33.873982Z",
     "shell.execute_reply.started": "2025-10-28T16:24:33.847177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "#          MAIN\n",
    "# =========================\n",
    "def main(args):\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # ---- core flags\n",
    "    out_dir, seed = args.out_dir, args.seed\n",
    "    epochs, lr, wd = args.epochs, args.lr, args.wd\n",
    "    pooling, n_patches = args.pooling, None  # kept for API parity\n",
    "\n",
    "    # loss knobs (legacy-compatible)\n",
    "    alpha, beta, gamma = args.alpha, args.beta, args.gamma\n",
    "    moment_w, arch, do_train = args.moment, args.arch, (not args.eval_only)\n",
    "    val_ratio = args.val_ratio\n",
    "\n",
    "    # geometry init & optional FT for pure geom\n",
    "    geom_eps = args.geom_eps\n",
    "    geom_ft_epochs, geom_ft_lr, geom_ft_wd = args.geom_finetune_epochs, args.geom_finetune_lr, args.geom_finetune_wd\n",
    "\n",
    "    # negatives / temperature (legacy + new)\n",
    "    tau_fixed = args.tau  # None → use curriculum\n",
    "    queue_size = args.queue\n",
    "\n",
    "    # curricula & extras\n",
    "    tau_start, tau_end = args.tau_start, args.tau_end\n",
    "    queue_warmup_epochs = args.queue_warmup_epochs\n",
    "    queue_recent_schedule = tuple(args.queue_recent_k)  # (16k, 32k, 65k) by default\n",
    "    lambda_agree = args.lambda_agree\n",
    "    geom_unfreeze_epoch = args.geom_unfreeze_epoch\n",
    "    geom_lr_scale = args.geom_lr_scale\n",
    "\n",
    "    # NEW: PK/XBP/DCL/mining/dual-queue flags\n",
    "    use_pk, P, K_pk = args.use_pk, args.P, args.K_pk\n",
    "    xbp_per_img, xbp_global = args.xbp_per_img, args.xbp_global\n",
    "    use_dcl, dcl_prior = args.use_dcl, args.dcl_prior\n",
    "    mine_H = args.mine_H\n",
    "    use_pred_queue, queue_pred_capacity = args.queue_pred, args.queue_pred_capacity\n",
    "\n",
    "    # ---- setup\n",
    "    seed_all(seed)\n",
    "    OUT = Path(f\"/kaggle/working/outputs/{out_dir}\")\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- data\n",
    "    X, Y, cap_ids, img_ids_row, full_img, all_img_ids = load_train(OUT)\n",
    "\n",
    "    # ---- split by image id (NO LEAKAGE)\n",
    "    cap_is_tr, cap_is_val, val_gallery, cap2gal_local, val_img_indices = build_image_id_split(\n",
    "        img_ids_row, all_img_ids, full_img, X, Y, val_ratio, seed, OUT\n",
    "    )\n",
    "    val_set = set(all_img_ids[val_img_indices])\n",
    "    train_set = set(all_img_ids) - val_set\n",
    "    assert val_set.isdisjoint(train_set), \"Leakage detected between TRAIN and VAL image sets!\"\n",
    "\n",
    "    # propagate masks to captions\n",
    "    Xtr, Ytr = X[cap_is_tr], Y[cap_is_tr]\n",
    "    Xva      = X[cap_is_val]\n",
    "\n",
    "    din, dout = X.shape[1], Y.shape[1]\n",
    "    assert din == 1024 and dout == 1536, f\"Dimension mismatch: text={din}, image={dout} (expected 1024→1536)\"\n",
    "\n",
    "    # ---- model (geom init for geom/geom+adapter/bigjump)\n",
    "    geom_init = None\n",
    "    if arch.lower() in (\"geom\", \"geom_adapter\", \"geom+adapter\", \"bigjump\"):\n",
    "        geom_init = (Xtr.astype(np.float32), Ytr.astype(np.float32), float(geom_eps))\n",
    "    model = make_model(arch, din, dout, geom_init).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _probe = model(torch.zeros(2, din, device=device))\n",
    "    assert _probe.shape[-1] == 1536, f\"Translator output dim { _probe.shape[-1] } != 1536\"\n",
    "\n",
    "    params, mb = count_params_mb(model)\n",
    "    print(f\"[model] {arch} | params={params:,} (~{mb:.2f} MB) | pooling={pooling} \"\n",
    "          f\"| α={alpha} β={beta} λ_moment={moment_w}\")\n",
    "\n",
    "    best_stats, best_ep = None, 0\n",
    "\n",
    "    # =========================\n",
    "    #        TRAINING\n",
    "    # =========================\n",
    "    if do_train and arch.lower() in (\"geom_adapter\", \"geom+adapter\", \"bigjump\"):\n",
    "        # BIG JUMP training\n",
    "        best_stats = train_bigjump(\n",
    "            model,\n",
    "            Xtr, Ytr, img_ids_row[cap_is_tr],\n",
    "            Xva, val_gallery, cap2gal_local,\n",
    "            batch=args.batch, epochs=epochs, base_lr=lr, wd=wd,\n",
    "            # legacy-compatible knobs\n",
    "            tau=tau_fixed, alpha_cos=alpha, lambda_moment=moment_w, queue_size=queue_size,\n",
    "            # new schedule knobs\n",
    "            tau_start=tau_start, tau_end=tau_end,\n",
    "            queue_warmup_epochs=queue_warmup_epochs,\n",
    "            queue_recent_schedule=queue_recent_schedule,\n",
    "            lambda_agree=lambda_agree,\n",
    "            # late tiny unfreeze of geometry (A only)\n",
    "            geom_unfreeze_epoch=geom_unfreeze_epoch,\n",
    "            geom_lr_scale=geom_lr_scale,\n",
    "            # misc\n",
    "            device=device, pooling=pooling, n_patches=None,\n",
    "            out_dir=str(OUT), seed=seed,\n",
    "            # NEW knobs (F1.1–F2.3)\n",
    "            use_pk=use_pk, P=P, K_pk=K_pk,\n",
    "            xbp_per_img=xbp_per_img, xbp_global=xbp_global,\n",
    "            use_dcl=use_dcl, dcl_prior=dcl_prior,\n",
    "            mine_H=mine_H,\n",
    "            use_pred_queue=use_pred_queue, queue_pred_capacity=queue_pred_capacity,\n",
    "        )\n",
    "\n",
    "    elif do_train and arch.lower() == \"geom\" and geom_ft_epochs > 0:\n",
    "        dl = DataLoader(PairDS(Xtr, Ytr), batch_size=args.batch, shuffle=True,\n",
    "                        num_workers=2, pin_memory=True, drop_last=False)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=geom_ft_lr, weight_decay=geom_ft_wd)\n",
    "        best = -1.0\n",
    "        for ep in range(1, geom_ft_epochs + 1):\n",
    "            tr = train_one(model, dl, opt, alpha, beta, gamma, moment_w, pooling, n_patches, device)\n",
    "            stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "            print(f\"[geom-ft {ep:02d}] train_loss={tr:.6f} | val_MRR={stats['MRR']:.4f} \"\n",
    "                  f\"| R@1={stats['R1']:.3f} R@5={stats['R5']:.3f} R@10={stats['R10']:.3f} \"\n",
    "                  f\"| median={stats['rank_median']} p75={stats['rank_p75']}\")\n",
    "            if stats[\"MRR\"] > best:\n",
    "                best, best_ep, best_stats = stats[\"MRR\"], ep, stats\n",
    "                torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"val\": stats}, OUT / \"best.pt\")\n",
    "        if best_stats is None:\n",
    "            best_stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "            torch.save({\"model\": model.state_dict(), \"epoch\": 0, \"val\": best_stats}, OUT / \"best.pt\")\n",
    "        (OUT / \"val_metrics.json\").write_text(json.dumps(dict(best_epoch=best_ep, **best_stats), indent=2))\n",
    "\n",
    "    elif do_train:\n",
    "        dl = DataLoader(PairDS(Xtr, Ytr), batch_size=args.batch, shuffle=True,\n",
    "                        num_workers=2, pin_memory=True, drop_last=False)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        best = -1.0\n",
    "        for ep in range(1, epochs + 1):\n",
    "            tr = train_one(model, dl, opt, alpha, beta, gamma, moment_w, pooling, n_patches, device)\n",
    "            stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "            print(f\"[{ep:02d}] train_loss={tr:.6f} | val_MRR={stats['MRR']:.4f} \"\n",
    "                  f\"| R@1={stats['R1']:.3f} R@5={stats['R5']:.3f} R@10={stats['R10']:.3f} \"\n",
    "                  f\"| median={stats['rank_median']} p75={stats['rank_p75']}\")\n",
    "            if stats[\"MRR\"] > best:\n",
    "                best, best_ep, best_stats = stats[\"MRR\"], ep, stats\n",
    "                torch.save({\"model\": model.state_dict(), \"epoch\": ep, \"val\": stats}, OUT / \"best.pt\")\n",
    "        if best_stats is None:\n",
    "            best_stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "            torch.save({\"model\": model.state_dict(), \"epoch\": 0, \"val\": best_stats}, OUT / \"best.pt\")\n",
    "        (OUT / \"val_metrics.json\").write_text(json.dumps(dict(best_epoch=best_ep, **best_stats), indent=2))\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            ckpt = torch.load(OUT / \"best.pt\", map_location=\"cpu\")\n",
    "            print(f\"[resume] loaded epoch={ckpt.get('epoch','?')} MRR={ckpt.get('val',{}).get('MRR','?')}\")\n",
    "            model.load_state_dict(ckpt[\"model\"])\n",
    "            best_stats = ckpt.get(\"val\", None)\n",
    "        except FileNotFoundError:\n",
    "            best_stats = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "\n",
    "    # ---- efficiency logging\n",
    "    ms_gpu, ms_cpu = time_ms_per_query(model, din, pooling, n_patches)\n",
    "    eff = {\"params\": params, \"mb_fp32\": mb, \"ms_per_query_gpu\": ms_gpu, \"ms_per_query_cpu\": ms_cpu}\n",
    "    (OUT / \"efficiency.json\").write_text(json.dumps(eff, indent=2))\n",
    "\n",
    "    # ---- submission (L2-normalized outputs for cosine retrieval)\n",
    "    test_data = load_data(TEST_NPZ)\n",
    "    Q   = test_data[\"captions/embeddings\"].astype(np.float32)\n",
    "    ids = test_data.get(\"captions/ids\", np.arange(len(Q)).astype(str))\n",
    "    model.eval()\n",
    "    BS, outs = 1024, []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(Q), BS):\n",
    "            q = torch.from_numpy(Q[i:i+BS]).to(device)\n",
    "            q = apply_pooling(q, pooling, n_patches)\n",
    "            z = model(q)\n",
    "            z = F.normalize(z, dim=-1)\n",
    "            outs.append(z.detach().cpu().numpy())\n",
    "    pred_embds = np.concatenate(outs, axis=0)\n",
    "    sub = OUT / \"submission.csv\"\n",
    "    generate_submission(ids, pred_embds, str(sub))\n",
    "    print(f\"[ok] submission written → {sub}\")\n",
    "\n",
    "    # ---- final sanity print\n",
    "    sanity = {\n",
    "        \"dims\": {\"text\": int(din), \"image\": int(dout)},\n",
    "        \"split\": {\n",
    "            \"train_captions\": int(cap_is_tr.sum()),\n",
    "            \"val_captions\": int(cap_is_val.sum()),\n",
    "            \"val_unique_images\": int(val_gallery.shape[0]),\n",
    "            \"leakage\": False\n",
    "        },\n",
    "        \"val_metrics\": best_stats if best_stats is not None else validate_retrieval(\n",
    "            model, Xva, val_gallery, cap2gal_local, pooling, n_patches\n",
    "        ),\n",
    "        \"efficiency\": eff,\n",
    "        \"curriculum\": {\n",
    "            \"tau\": (tau_fixed if tau_fixed is not None else None),\n",
    "            \"tau_start\": float(tau_start), \"tau_end\": float(tau_end),\n",
    "            \"queue_warmup_epochs\": int(queue_warmup_epochs),\n",
    "            \"queue_recent_schedule\": list(map(int, queue_recent_schedule)),\n",
    "            \"lambda_agree\": float(lambda_agree),\n",
    "            \"geom_unfreeze_epoch\": int(geom_unfreeze_epoch),\n",
    "            \"geom_lr_scale\": float(geom_lr_scale)\n",
    "        },\n",
    "        # NEW knobs for reproducibility\n",
    "        \"pk_sampler\": {\"use_pk\": bool(use_pk), \"P\": int(P), \"K\": int(K_pk)},\n",
    "        \"xbp\": {\"per_img\": int(xbp_per_img), \"global\": int(xbp_global)},\n",
    "        \"hard_mining\": {\"H\": int(mine_H)},\n",
    "        \"dcl\": {\"use_dcl\": bool(use_dcl), \"prior\": float(dcl_prior)},\n",
    "        \"dual_queue\": {\"use_pred_queue\": bool(use_pred_queue), \"capacity\": int(queue_pred_capacity)},\n",
    "    }\n",
    "    print(json.dumps(sanity, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:36.944752Z",
     "iopub.status.busy": "2025-10-28T16:24:36.944140Z",
     "iopub.status.idle": "2025-10-28T16:24:36.951184Z",
     "shell.execute_reply": "2025-10-28T16:24:36.950314Z",
     "shell.execute_reply.started": "2025-10-28T16:24:36.944730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_model(arch: str, din: int, dout: int, geom_init_data=None):\n",
    "    a = arch.lower()\n",
    "    if a == \"linear\":\n",
    "        return LinearProj(din, dout)\n",
    "    if a == \"mlp1\":\n",
    "        return MLP1(din, dout, hidden=512, pdrop=0.1)\n",
    "    if a == \"mlp2\":\n",
    "        return MLP2(din, dout, h1=1024, h2=512, pdrop=0.1)\n",
    "    if a == \"geom\":\n",
    "        assert geom_init_data is not None, \"geom_init_data required for arch='geom'.\"\n",
    "        Xtr_np, Ytr_np, eps = geom_init_data\n",
    "        A, b = procrustes_closed_form(Xtr_np, Ytr_np, eps=float(eps))\n",
    "        return GeomLinear(A, b)\n",
    "    if a in (\"geom_adapter\", \"geom+adapter\", \"bigjump\"):\n",
    "        assert geom_init_data is not None, \"geom_init_data required for arch='geom_adapter'/'geom+adapter'/'bigjump'.\"\n",
    "        Xtr_np, Ytr_np, eps = geom_init_data\n",
    "        A, b = procrustes_closed_form(Xtr_np, Ytr_np, eps=float(eps))\n",
    "        # Adapter-capacity tuned for BigJump path\n",
    "        return GeomWithAdapter(A, b, din=din, dout=dout, hidden=1024, pdrop=0.1)\n",
    "    if a == \"auto\":\n",
    "        return MLP2(din, dout)\n",
    "    raise ValueError(f\"Unknown arch {arch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:24:41.998748Z",
     "iopub.status.busy": "2025-10-28T16:24:41.998124Z",
     "iopub.status.idle": "2025-10-28T16:33:32.170468Z",
     "shell.execute_reply": "2025-10-28T16:33:32.169519Z",
     "shell.execute_reply.started": "2025-10-28T16:24:41.998724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[meta] text_dim=1024 | image_dim=1536\n",
      "[model] bigjump | params=4,198,400 (~16.02 MB) | pooling=CLS | α=0.5 β=1 λ_moment=0.02\n",
      "[bigjump 01] loss=4.103592 | val_MRR=0.3521 | R@1=0.232 R@5=0.483 R@10=0.606 | median=6 p75=25 | queue=65536 | tau=0.100 | recentQ=0\n",
      "[bigjump 02] loss=3.226839 | val_MRR=0.3571 | R@1=0.234 R@5=0.488 R@10=0.621 | median=6 p75=23 | queue=65536 | tau=0.100 | recentQ=0\n",
      "[bigjump 03] loss=4.965233 | val_MRR=0.3379 | R@1=0.215 R@5=0.469 R@10=0.604 | median=6 p75=24 | queue=65536 | tau=0.099 | recentQ=16000\n",
      "[bigjump 04] loss=4.850332 | val_MRR=0.3456 | R@1=0.220 R@5=0.480 R@10=0.614 | median=6 p75=23 | queue=65536 | tau=0.098 | recentQ=32000\n",
      "[bigjump 05] loss=4.842537 | val_MRR=0.3532 | R@1=0.226 R@5=0.492 R@10=0.624 | median=6 p75=21 | queue=65536 | tau=0.097 | recentQ=65536\n",
      "[bigjump 06] loss=4.845235 | val_MRR=0.3608 | R@1=0.232 R@5=0.502 R@10=0.634 | median=5 p75=21 | queue=65536 | tau=0.096 | recentQ=65536\n",
      "[bigjump 07] loss=4.826713 | val_MRR=0.3628 | R@1=0.234 R@5=0.504 R@10=0.636 | median=5 p75=20 | queue=65536 | tau=0.094 | recentQ=65536\n",
      "[bigjump 08] loss=4.810815 | val_MRR=0.3681 | R@1=0.238 R@5=0.511 R@10=0.643 | median=5 p75=20 | queue=65536 | tau=0.092 | recentQ=65536\n",
      "[bigjump 09] loss=4.792167 | val_MRR=0.3736 | R@1=0.244 R@5=0.518 R@10=0.647 | median=5 p75=19 | queue=65536 | tau=0.090 | recentQ=65536\n",
      "[bigjump 10] loss=4.753000 | val_MRR=0.3784 | R@1=0.248 R@5=0.524 R@10=0.654 | median=5 p75=18 | queue=65536 | tau=0.088 | recentQ=65536\n",
      "[bigjump 11] loss=4.738036 | val_MRR=0.3826 | R@1=0.253 R@5=0.528 R@10=0.657 | median=5 p75=18 | queue=65536 | tau=0.085 | recentQ=65536\n",
      "[bigjump 12] loss=4.708380 | val_MRR=0.3873 | R@1=0.257 R@5=0.532 R@10=0.661 | median=5 p75=18 | queue=65536 | tau=0.083 | recentQ=65536\n",
      "[bigjump 13] loss=4.679388 | val_MRR=0.3918 | R@1=0.261 R@5=0.539 R@10=0.667 | median=5 p75=17 | queue=65536 | tau=0.080 | recentQ=65536\n",
      "[bigjump 14] loss=4.644935 | val_MRR=0.3968 | R@1=0.266 R@5=0.544 R@10=0.670 | median=4 p75=17 | queue=65536 | tau=0.077 | recentQ=65536\n",
      "[bigjump 15] loss=4.612253 | val_MRR=0.4008 | R@1=0.269 R@5=0.549 R@10=0.676 | median=4 p75=16 | queue=65536 | tau=0.075 | recentQ=65536\n",
      "[bigjump 16] loss=4.590411 | val_MRR=0.4051 | R@1=0.273 R@5=0.554 R@10=0.680 | median=4 p75=16 | queue=65536 | tau=0.072 | recentQ=65536\n",
      "[bigjump 17] loss=4.555262 | val_MRR=0.4095 | R@1=0.277 R@5=0.558 R@10=0.684 | median=4 p75=16 | queue=65536 | tau=0.070 | recentQ=65536\n",
      "[bigjump 18] loss=4.541553 | val_MRR=0.4123 | R@1=0.280 R@5=0.562 R@10=0.686 | median=4 p75=15 | queue=65536 | tau=0.068 | recentQ=65536\n",
      "[bigjump 19] loss=4.505564 | val_MRR=0.4149 | R@1=0.282 R@5=0.568 R@10=0.690 | median=4 p75=15 | queue=65536 | tau=0.066 | recentQ=65536\n",
      "[bigjump 20] loss=4.500967 | val_MRR=0.4174 | R@1=0.284 R@5=0.570 R@10=0.692 | median=4 p75=15 | queue=65536 | tau=0.064 | recentQ=65536\n",
      "[bigjump 21] loss=4.492386 | val_MRR=0.4196 | R@1=0.287 R@5=0.572 R@10=0.694 | median=4 p75=15 | queue=65536 | tau=0.063 | recentQ=65536\n",
      "[bigjump 22] loss=4.489647 | val_MRR=0.4206 | R@1=0.287 R@5=0.574 R@10=0.696 | median=4 p75=15 | queue=65536 | tau=0.062 | recentQ=65536\n",
      "[bigjump 23] loss=4.499916 | val_MRR=0.4217 | R@1=0.287 R@5=0.577 R@10=0.697 | median=4 p75=15 | queue=65536 | tau=0.061 | recentQ=65536\n",
      "[bigjump 24] loss=4.502958 | val_MRR=0.4235 | R@1=0.290 R@5=0.577 R@10=0.698 | median=4 p75=14 | queue=65536 | tau=0.060 | recentQ=65536\n",
      "[bigjump 25] loss=4.516894 | val_MRR=0.4236 | R@1=0.290 R@5=0.578 R@10=0.697 | median=4 p75=15 | queue=65536 | tau=0.060 | recentQ=65536\n",
      "Generating submission file...\n",
      "✓ Saved submission to /kaggle/working/outputs/bigjump_2/submission.csv\n",
      "[ok] submission written → /kaggle/working/outputs/bigjump_2/submission.csv\n",
      "{\n",
      "  \"dims\": {\n",
      "    \"text\": 1024,\n",
      "    \"image\": 1536\n",
      "  },\n",
      "  \"split\": {\n",
      "    \"train_captions\": 112500,\n",
      "    \"val_captions\": 12500,\n",
      "    \"val_unique_images\": 2500,\n",
      "    \"leakage\": false\n",
      "  },\n",
      "  \"val_metrics\": {\n",
      "    \"MRR\": 0.4236314477278399,\n",
      "    \"R1\": 0.28984,\n",
      "    \"R5\": 0.57816,\n",
      "    \"R10\": 0.69728,\n",
      "    \"rank_median\": 4,\n",
      "    \"rank_p75\": 15\n",
      "  },\n",
      "  \"efficiency\": {\n",
      "    \"params\": 4198400,\n",
      "    \"mb_fp32\": 16.015625,\n",
      "    \"ms_per_query_gpu\": 0.001616310328245163,\n",
      "    \"ms_per_query_cpu\": 0.053171650506556034\n",
      "  },\n",
      "  \"curriculum\": {\n",
      "    \"tau\": null,\n",
      "    \"tau_start\": 0.1,\n",
      "    \"tau_end\": 0.06,\n",
      "    \"queue_warmup_epochs\": 2,\n",
      "    \"queue_recent_schedule\": [\n",
      "      16000,\n",
      "      32000,\n",
      "      65536\n",
      "    ],\n",
      "    \"lambda_agree\": 0.05,\n",
      "    \"geom_unfreeze_epoch\": 3,\n",
      "    \"geom_lr_scale\": 0.05\n",
      "  },\n",
      "  \"pk_sampler\": {\n",
      "    \"use_pk\": false,\n",
      "    \"P\": 256,\n",
      "    \"K\": 2\n",
      "  },\n",
      "  \"xbp\": {\n",
      "    \"per_img\": 4,\n",
      "    \"global\": 32000\n",
      "  },\n",
      "  \"hard_mining\": {\n",
      "    \"H\": 64\n",
      "  },\n",
      "  \"dcl\": {\n",
      "    \"use_dcl\": false,\n",
      "    \"prior\": 0.01\n",
      "  },\n",
      "  \"dual_queue\": {\n",
      "    \"use_pred_queue\": false,\n",
      "    \"capacity\": 65536\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#        ARGPARSE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    p = argparse.ArgumentParser(description=\"RoBERTa→DINOv2 translator (MRR-focused, BigJump++)\")\n",
    "\n",
    "    # IO & run mode\n",
    "    p.add_argument(\"--out_dir\", type=str, default=\"bigjump_2\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--eval_only\", action=\"store_true\", help=\"Skip training and only evaluate / emit submission.\")\n",
    "\n",
    "    # data & split\n",
    "    p.add_argument(\"--val_ratio\", type=float, default=0.10, help=\"Fraction of unique images for validation split.\")\n",
    "\n",
    "    # architecture\n",
    "    p.add_argument(\"--arch\", type=str, default=\"bigjump\",\n",
    "                   choices=[\"linear\", \"mlp1\", \"mlp2\", \"geom\", \"geom_adapter\", \"geom+adapter\", \"bigjump\"])\n",
    "\n",
    "    # optimization (generic)\n",
    "    p.add_argument(\"--epochs\", type=int, default=25)\n",
    "    p.add_argument(\"--batch\", type=int, default=512)\n",
    "    p.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--wd\", type=float, default=1e-4)\n",
    "\n",
    "    # losses (legacy-compatible; bigjump uses alpha + moment + agreement)\n",
    "    p.add_argument(\"--alpha\", type=float, default=0.5, help=\"Weight for cosine auxiliary loss.\")\n",
    "    p.add_argument(\"--beta\",  type=float, default=1, help=\"Weight for MSE (legacy trainers).\")\n",
    "    p.add_argument(\"--gamma\", type=float, default=0.0, help=\"Weight for CE (legacy trainers).\")\n",
    "    p.add_argument(\"--moment\", type=float, default=0.02, help=\"λ for moment matching.\")\n",
    "\n",
    "    # geometry init & (optional) legacy fine-tune\n",
    "    p.add_argument(\"--geom_eps\", type=float, default=1e-5, help=\"EPS for covariance whitening in Procrustes init.\")\n",
    "    p.add_argument(\"--geom_finetune_epochs\", type=int, default=0, help=\">0 enables tiny FT when arch=='geom'.\")\n",
    "    p.add_argument(\"--geom_finetune_lr\", type=float, default=2e-5)\n",
    "    p.add_argument(\"--geom_finetune_wd\", type=float, default=1e-5)\n",
    "\n",
    "    # negatives / temperature (legacy and new)\n",
    "    p.add_argument(\"--tau\", type=float, default=None,\n",
    "                   help=\"Fixed InfoNCE temperature. Leave None to use curriculum tau_start→tau_end.\")\n",
    "    p.add_argument(\"--tau_start\", type=float, default=0.10, help=\"Start τ for curriculum (used when --tau is None).\")\n",
    "    p.add_argument(\"--tau_end\",   type=float, default=0.06, help=\"End τ for curriculum (used when --tau is None).\")\n",
    "    p.add_argument(\"--queue\", type=int, default=65536, help=\"Image FIFO queue size for negatives.\")\n",
    "    p.add_argument(\"--queue_warmup_epochs\", type=int, default=2,\n",
    "                   help=\"First N epochs exclude queue negatives from loss (still enqueue).\")\n",
    "    p.add_argument(\"--queue_recent_k\", type=int, nargs=3, default=[16000, 32000, 65536],\n",
    "                   help=\"Recent-queue sizes used after warmup: ep+1, ep+2, ep+>=3.\")\n",
    "\n",
    "    # new loss term\n",
    "    p.add_argument(\"--lambda_agree\", type=float, default=0.05,\n",
    "                   help=\"Weight for caption-agreement loss (collapse same-image captions).\")\n",
    "\n",
    "    # late tiny unfreeze of geometry\n",
    "    p.add_argument(\"--geom_unfreeze_epoch\", type=int, default=3,\n",
    "                   help=\"Epoch to unfreeze geometry matrix A (0 disables).\")\n",
    "    p.add_argument(\"--geom_lr_scale\", type=float, default=0.05,\n",
    "                   help=\"LR scale for geometry params when unfrozen (relative to base lr).\")\n",
    "\n",
    "    # pooling (no-op, kept for API symmetry)\n",
    "    p.add_argument(\"--pooling\", type=str, default=\"CLS\", choices=[\"CLS\"], help=\"No-op; kept for parity.\")\n",
    "\n",
    "    # ===== NEW CLI knobs =====\n",
    "    # P×K sampler (F1.1)\n",
    "    p.add_argument(\"--use_pk\", action=\"store_true\", help=\"Enable P×K sampler.\")\n",
    "    p.add_argument(\"--P\", type=int, default=256, help=\"Number of unique images per batch.\")\n",
    "    p.add_argument(\"--K_pk\", type=int, default=2, help=\"Captions per image in a PK batch.\")\n",
    "\n",
    "    # Cross-batch positives (F1.2)\n",
    "    p.add_argument(\"--xbp_per_img\", type=int, default=4, help=\"XBP buffer size per image.\")\n",
    "    p.add_argument(\"--xbp_global\", type=int, default=32000, help=\"Global cap for XBP buffer.\")\n",
    "\n",
    "    # Hard mining (F2.1)\n",
    "    p.add_argument(\"--mine_H\", type=int, default=64, help=\"Top-H hard negatives per anchor from recent queue slice.\")\n",
    "\n",
    "    # Debiased NT-Xent (F2.2)\n",
    "    p.add_argument(\"--use_dcl\", action=\"store_true\", help=\"Enable debiased denominator correction.\")\n",
    "    p.add_argument(\"--dcl_prior\", type=float, default=0.01, help=\"Debiased contrastive prior π.\")\n",
    "\n",
    "    # Dual queues: past predictions (F2.3)\n",
    "    p.add_argument(\"--queue_pred\", action=\"store_true\", help=\"Enable a second queue of past predictions.\")\n",
    "    p.add_argument(\"--queue_pred_capacity\", type=int, default=65536, help=\"Capacity for PredQueue.\")\n",
    "\n",
    "    args, _ = p.parse_known_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14220991,
     "sourceId": 117959,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

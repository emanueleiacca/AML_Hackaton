{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-29T19:36:44.847344Z",
     "iopub.status.busy": "2025-10-29T19:36:44.846823Z",
     "iopub.status.idle": "2025-10-29T19:36:45.042473Z",
     "shell.execute_reply": "2025-10-29T19:36:45.041800Z",
     "shell.execute_reply.started": "2025-10-29T19:36:44.847323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Credentials written to: /root/.kaggle/kaggle.json\n",
      "total 16\n",
      "drwxr-xr-x 2 root root 4096 Oct 29 19:36 .\n",
      "drwx------ 1 root root 4096 Oct 29 19:36 ..\n",
      "-rw------- 1 root root   72 Oct 29 19:36 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import json, os, pathlib, subprocess, sys\n",
    "\n",
    "# --- 1. Load your secret (full JSON from kaggle.json) ---\n",
    "secret_name = \"kaggle_json\"  # change if you used another name\n",
    "user_secrets = UserSecretsClient()\n",
    "raw = user_secrets.get_secret(secret_name)\n",
    "creds = json.loads(raw)\n",
    "\n",
    "# --- 2. Forcefully recreate ~/.kaggle/kaggle.json ---\n",
    "kaggle_dir = pathlib.Path.home() / \".kaggle\"\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "cred_path = kaggle_dir / \"kaggle.json\"\n",
    "cred_path.write_text(json.dumps(creds))\n",
    "os.chmod(cred_path, 0o600)\n",
    "\n",
    "# --- 3. Double-check the file actually exists and is readable ---\n",
    "print(\"✅ Credentials written to:\", cred_path)\n",
    "!ls -la ~/.kaggle/\n",
    "#!cat ~/.kaggle/kaggle.json | head -1\n",
    "\n",
    "# --- 4. Reinstall Kaggle CLI cleanly ---\n",
    "#!pip install --upgrade --force-reinstall kaggle --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:42:50.595475Z",
     "iopub.status.busy": "2025-10-29T19:42:50.594899Z",
     "iopub.status.idle": "2025-10-29T19:43:59.032875Z",
     "shell.execute_reply": "2025-10-29T19:43:59.031742Z",
     "shell.execute_reply.started": "2025-10-29T19:42:50.595425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!kaggle competitions files -c aml-competition\n",
    "!kaggle competitions download -c aml-competition -p /kaggle/working --force\n",
    "!mkdir -p /kaggle/working/data\n",
    "!unzip -o /kaggle/working/aml-competition.zip -d /kaggle/working/data\n",
    "!ls -lah /kaggle/working/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:37:58.573402Z",
     "iopub.status.busy": "2025-10-29T19:37:58.573086Z",
     "iopub.status.idle": "2025-10-29T19:37:58.693289Z",
     "shell.execute_reply": "2025-10-29T19:37:58.692355Z",
     "shell.execute_reply.started": "2025-10-29T19:37:58.573369Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'challenge' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Mamiglia/challenge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:44:11.279324Z",
     "iopub.status.busy": "2025-10-29T19:44:11.278737Z",
     "iopub.status.idle": "2025-10-29T19:44:13.415509Z",
     "shell.execute_reply": "2025-10-29T19:44:13.414691Z",
     "shell.execute_reply.started": "2025-10-29T19:44:11.279285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# runner.py\n",
    "import argparse, json, random, re, time\n",
    "from pathlib import Path\n",
    "from os.path import basename\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from challenge.src.common import load_data, generate_submission\n",
    "\n",
    "# ---------- Paths ----------\n",
    "DATA_ROOT = Path(\"/kaggle/working/data\")\n",
    "TRAIN_DIR = DATA_ROOT / \"train\" / \"train\"\n",
    "TEST_DIR  = DATA_ROOT / \"test\"  / \"test\"\n",
    "TRAIN_NPZ = TRAIN_DIR / \"train.npz\"\n",
    "TEST_NPZ  = TEST_DIR  / \"test.clean.npz\"\n",
    "TRAIN_CAPTIONS = TRAIN_DIR / \"captions.txt\"\n",
    "TEST_CAPTIONS  = TEST_DIR  / \"captions.txt\"\n",
    "\n",
    "# ---------- Determinism ----------\n",
    "def seed_all(s: int):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic=True; torch.backends.cudnn.benchmark=False\n",
    "\n",
    "# ---------- Metadata/safety ----------\n",
    "def _assert_metadata_dims(npz_path: Path, expect_text=1024, expect_img=1536):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    md = {k: d[k][0] for k in d.files if k.startswith(\"metadata/\")}\n",
    "    tdim = md.get(\"metadata/embedding_dim_text\", None)\n",
    "    idim = md.get(\"metadata/embedding_dim_image\", None)\n",
    "    print(f\"[meta] text_dim={tdim} | image_dim={idim}\")\n",
    "    assert tdim == expect_text and idim == expect_img, (\n",
    "        f\"Encoder dims mismatch: expected text={expect_text}, image={expect_img} \"\n",
    "        f\"but got text={tdim}, image={idim}. Regenerate NPZ with the fixed encoders.\"\n",
    "    )\n",
    "\n",
    "# ---------- Caption→image matching from captions.txt ----------\n",
    "def _build_image_index(image_ids):\n",
    "    idx_exact = {}; idx_base={}; idx_stem={}\n",
    "    for i, v in enumerate(image_ids):\n",
    "        s = str(v); idx_exact[s]=i\n",
    "        b = basename(s); idx_base[b]=i\n",
    "        stem = b.rsplit(\".\",1)[0] if \".\" in b else b\n",
    "        idx_stem[stem]=i\n",
    "    return idx_exact, idx_base, idx_stem\n",
    "\n",
    "def _iter_targets_from_captions(path: Path, n_text: int, idx_exact, idx_base, idx_stem):\n",
    "    targets=[]\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for raw in f:\n",
    "            if len(targets)>=n_text: break\n",
    "            line=raw.strip()\n",
    "            if not line: continue\n",
    "            # be robust to delimiters\n",
    "            parts=re.split(r'\\||\\t|,| {2,}', line)\n",
    "            if len(parts)==1: parts=line.split(\" \",1)\n",
    "            tok=parts[0].strip().strip('\"').strip(\"'\")\n",
    "            if not tok: continue\n",
    "            def _match(tok_):\n",
    "                if tok_ in idx_exact: return idx_exact[tok_]\n",
    "                b=basename(tok_)\n",
    "                if b in idx_base: return idx_base[b]\n",
    "                stem=b.rsplit(\".\",1)[0] if \".\" in b else b\n",
    "                if stem in idx_stem: return idx_stem[stem]\n",
    "                return None\n",
    "            idx=_match(tok)\n",
    "            if idx is None:\n",
    "                tok2=tok.replace(\"Images/\",\"\").replace(\"./\",\"\")\n",
    "                idx=_match(tok2)\n",
    "            if idx is None:\n",
    "                if \".\" not in tok: continue\n",
    "                raise AssertionError(f\"Could not match image token '{tok}'\")\n",
    "            targets.append(idx)\n",
    "    assert len(targets)==n_text, f\"matched {len(targets)} vs N_text {n_text}\"\n",
    "    return np.asarray(targets, dtype=np.int64)\n",
    "\n",
    "# ---------- Data loaders ----------\n",
    "def load_train(out_dir: Path):\n",
    "    _assert_metadata_dims(TRAIN_NPZ, 1024, 1536)\n",
    "    d=np.load(TRAIN_NPZ, allow_pickle=True)\n",
    "    X=d[\"captions/embeddings\"].astype(np.float32)  # (N_text,1024)\n",
    "    I=d[\"images/embeddings\"].astype(np.float32)    # (N_img,1536)\n",
    "    cap_ids=d.get(\"captions/ids\", np.arange(len(X)).astype(str))\n",
    "    img_names=d.get(\"images/names\", np.arange(len(I)).astype(str))\n",
    "\n",
    "    ex,ba,st=_build_image_index(img_names)\n",
    "    assert TRAIN_CAPTIONS.exists(), f\"Missing {TRAIN_CAPTIONS}\"\n",
    "    targets=_iter_targets_from_captions(TRAIN_CAPTIONS, len(X), ex,ba,st)\n",
    "\n",
    "    Y=I[targets]                       # (N_text, 1536) GT image vec per caption\n",
    "    img_ids_row = img_names[targets]   # image name per caption row (for splitting)\n",
    "    meta={\"n_text\":int(len(X)),\"n_images\":int(len(I))}\n",
    "    (out_dir/\"train_detect.json\").write_text(json.dumps(meta, indent=2))\n",
    "    return X,Y,cap_ids,img_ids_row,I,img_names\n",
    "\n",
    "def load_test_npz():\n",
    "    d_test=np.load(TEST_NPZ, allow_pickle=True)\n",
    "    Q=d_test[\"captions/embeddings\"].astype(np.float32)\n",
    "    q_ids=d_test.get(\"captions/ids\", np.arange(len(Q)).astype(str))\n",
    "    if \"images/embeddings\" in d_test.files:\n",
    "        G=d_test[\"images/embeddings\"].astype(np.float32)\n",
    "        g_ids=d_test.get(\"images/names\", np.arange(len(G)).astype(str))\n",
    "    else:\n",
    "        d_tr=np.load(TRAIN_NPZ, allow_pickle=True)\n",
    "        G=d_tr[\"images/embeddings\"].astype(np.float32)\n",
    "        g_ids=d_tr.get(\"images/names\", np.arange(len(G)).astype(str))\n",
    "    return Q,G,q_ids,g_ids\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class PairDS(Dataset):\n",
    "    def __init__(self, X, Y): self.X=torch.from_numpy(X); self.Y=torch.from_numpy(Y)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
    "\n",
    "# ---------- Pooling (no-op per spec) ----------\n",
    "def apply_pooling(x: torch.Tensor, mode: str, n_patches=None):\n",
    "    return x\n",
    "\n",
    "# ---------- Base Models ----------\n",
    "class LinearProj(nn.Module):\n",
    "    def __init__(self, din, dout):\n",
    "        super().__init__()\n",
    "        self.fc=nn.Linear(din,dout)\n",
    "        nn.init.xavier_normal_(self.fc.weight); nn.init.zeros_(self.fc.bias)\n",
    "    def forward(self,x): return self.fc(x)\n",
    "\n",
    "class MLP1(nn.Module):\n",
    "    def __init__(self, din, dout, hidden=512, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(din,hidden), nn.ReLU(), nn.Dropout(pdrop),\n",
    "            nn.Linear(hidden,dout)\n",
    "        )\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class MLP2(nn.Module):\n",
    "    # Spec: 1024→1024→512→1536, dropout=0.1 on hidden layers\n",
    "    def __init__(self, din, dout, h1=1024, h2=512, pdrop=0.1):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(din,h1), nn.ReLU(), nn.Dropout(pdrop),\n",
    "            nn.Linear(h1,h2), nn.ReLU(), nn.Dropout(pdrop),\n",
    "            nn.Linear(h2,dout)\n",
    "        )\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight); nn.init.zeros_(m.bias)\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "# ---------- Geometry-Preserving Linear (Whiten → Procrustes → Re-color) ----------\n",
    "def _cov_eigh(zc, eps):\n",
    "    # zc: (N,D), zero-mean\n",
    "    N = zc.shape[0]\n",
    "    C = (zc.T @ zc) / max(1, N)\n",
    "    # symmetric PSD\n",
    "    S, U = np.linalg.eigh(C)\n",
    "    S = np.clip(S, 0.0, None)\n",
    "    inv_sqrt = 1.0 / np.sqrt(S + eps)\n",
    "    sqrt = np.sqrt(S + eps)\n",
    "    C_mhalf = (U * inv_sqrt) @ U.T     # C^{-1/2}\n",
    "    C_phalf = (U * sqrt) @ U.T         # C^{ 1/2}\n",
    "    return C_mhalf.astype(np.float32), C_phalf.astype(np.float32)\n",
    "\n",
    "def procrustes_closed_form(X, Y, eps=1e-5):\n",
    "    \"\"\"\n",
    "    X: (N, 1024) text, Y: (N, 1536) targets (per-caption image vectors)\n",
    "    Returns A (1536x1024), b (1536,) such that y_hat = A x + b\n",
    "    \"\"\"\n",
    "    # center\n",
    "    mu_x = X.mean(0, dtype=np.float64)\n",
    "    mu_y = Y.mean(0, dtype=np.float64)\n",
    "    Xc = X - mu_x\n",
    "    Yc = Y - mu_y\n",
    "\n",
    "    # whiten both\n",
    "    Cx_mh, _ = _cov_eigh(Xc, eps)      # 1024x1024\n",
    "    Cy_mh, Cy_ph = _cov_eigh(Yc, eps)  # 1536x1536 (only Cy_ph used)\n",
    "    Xw = Xc @ Cx_mh.T                  # (N,1024)\n",
    "    Yw = Yc @ Cy_mh.T                  # (N,1536)\n",
    "\n",
    "    # orthogonal Procrustes: maximize Tr(R^T Xw^T Yw)\n",
    "    M = Xw.T @ Yw                      # (1024,1536)\n",
    "    # SVD on M; for rectangular, do SVD and build R = U V^T in common subspace\n",
    "    U, _, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "    R = U @ Vt                         # (1024,1536) @ (1536,1536) -> (1024,1536) OK since full_matrices=False\n",
    "    # we need R as (1536x1024) mapping whitened X to whitened Y; above is (1024x1536)\n",
    "    R = R.T                            # (1536,1024)\n",
    "\n",
    "    # re-color to Y space\n",
    "    A = (Cy_ph @ R @ Cx_mh).astype(np.float32)     # (1536,1536)*(1536,1024)*(1024,1024) = (1536,1024)\n",
    "    b = (mu_y - (A @ mu_x)).astype(np.float32)     # (1536,)\n",
    "    return A, b\n",
    "\n",
    "class GeomLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with weights initialized from closed-form geometry mapping.\n",
    "    Optionally fine-tunes with tiny LR.\n",
    "    \"\"\"\n",
    "    def __init__(self, A: np.ndarray, b: np.ndarray):\n",
    "        super().__init__()\n",
    "        D_in = A.shape[1]; D_out = A.shape[0]\n",
    "        self.fc = nn.Linear(D_in, D_out, bias=True)\n",
    "        with torch.no_grad():\n",
    "            self.fc.weight.copy_(torch.from_numpy(A))\n",
    "            self.fc.bias.copy_(torch.from_numpy(b))\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# ---------- Loss pieces (for optional fine-tune) ----------\n",
    "def moment_align(pred, tgt):\n",
    "    mu_p, mu_t = pred.mean(0), tgt.mean(0)\n",
    "    sd_p, sd_t = pred.std(0, unbiased=False), tgt.std(0, unbiased=False)\n",
    "    return F.mse_loss(mu_p, mu_t) + F.mse_loss(sd_p, sd_t)\n",
    "\n",
    "def info_nce(pred, tgt):\n",
    "    p = F.normalize(pred, dim=-1)\n",
    "    t = F.normalize(tgt, dim=-1)\n",
    "    logits = p @ t.t()\n",
    "    labels = torch.arange(pred.size(0), device=pred.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "# ---------- Split by image id & mapping ----------\n",
    "def build_image_id_split(img_ids_row, all_img_names, full_img, X, Y, val_ratio, seed, out_dir: Path):\n",
    "    uniq_img_names = np.array(sorted(set(map(str, all_img_names))))\n",
    "    rng = np.random.default_rng(seed); rng.shuffle(uniq_img_names)\n",
    "    n_val = max(1, int(len(uniq_img_names) * val_ratio))\n",
    "    val_images = set(uniq_img_names[:n_val])\n",
    "    tr_images  = set(uniq_img_names[n_val:])\n",
    "    assert len(val_images & tr_images) == 0, \"Leakage in image id split!\"\n",
    "\n",
    "    # Caption masks\n",
    "    cap_is_val = np.array([str(iid) in val_images for iid in img_ids_row], dtype=bool)\n",
    "    cap_is_tr  = ~cap_is_val\n",
    "\n",
    "    # Build VAL gallery (unique images in VAL) as sorted names → local indices\n",
    "    val_img_names_sorted = np.array(sorted(val_images))\n",
    "    name2local = {name:i for i,name in enumerate(val_img_names_sorted)}\n",
    "    name2global = {str(n):i for i,n in enumerate(all_img_names)}\n",
    "    val_img_indices = np.array([name2global[n] for n in val_img_names_sorted], dtype=np.int64)\n",
    "    val_gallery = full_img[val_img_indices]  # (M,1536)\n",
    "\n",
    "    # For each VAL caption row, get local gallery index (no remap later)\n",
    "    cap2gal_local = np.array([name2local[str(n)] for n in img_ids_row[cap_is_val]], dtype=np.int64)\n",
    "\n",
    "    # Persist mapping for debugging/acceptance\n",
    "    (out_dir/\"val_indices.json\").write_text(json.dumps({\n",
    "        \"val_img_indices\": val_img_indices.tolist(),\n",
    "        \"val_caption_to_gallery_index\": cap2gal_local.tolist(),\n",
    "        \"n_val_captions\": int(cap_is_val.sum()),\n",
    "        \"n_val_unique_images\": int(len(val_images))\n",
    "    }, indent=2))\n",
    "\n",
    "    return cap_is_tr, cap_is_val, val_gallery, cap2gal_local, val_img_indices\n",
    "\n",
    "# ---------- Metrics / evaluator (full gallery, cosine on L2) ----------\n",
    "@torch.no_grad()\n",
    "def validate_retrieval(model, Xv, val_gallery, cap2gal_local, pooling, n_patches, bs=1024):\n",
    "    device=next(model.parameters()).device\n",
    "    Gi = torch.from_numpy(val_gallery).to(device)\n",
    "    Gi = F.normalize(Gi, dim=-1)\n",
    "\n",
    "    ranks=[]\n",
    "    for i in range(0, len(Xv), bs):\n",
    "        xb=torch.from_numpy(Xv[i:i+bs]).to(device)\n",
    "        xb=apply_pooling(xb, pooling, n_patches)   # no-op\n",
    "        pred=model(xb)\n",
    "        pred=F.normalize(pred, dim=-1)\n",
    "        sims=pred @ Gi.t()                         # (b, M)\n",
    "        for j in range(sims.size(0)):\n",
    "            true_idx = int(cap2gal_local[i+j])     # local gallery index\n",
    "            order = torch.argsort(sims[j], descending=True)\n",
    "            rank = (order==true_idx).nonzero(as_tuple=False).item() + 1\n",
    "            ranks.append(rank)\n",
    "\n",
    "    ranks = np.array(ranks)\n",
    "    mrr = float(np.mean(1.0 / ranks))\n",
    "    r1  = float(np.mean(ranks<=1))\n",
    "    r5  = float(np.mean(ranks<=5))\n",
    "    r10 = float(np.mean(ranks<=10))\n",
    "    return {\n",
    "        \"MRR\": mrr,\n",
    "        \"R1\": r1,\n",
    "        \"R5\": r5,\n",
    "        \"R10\": r10,\n",
    "        \"rank_median\": int(np.median(ranks)),\n",
    "        \"rank_p75\": int(np.percentile(ranks, 75))\n",
    "    }\n",
    "\n",
    "def count_params_mb(model):\n",
    "    params=sum(p.numel() for p in model.parameters())\n",
    "    mb = params * 4 / (1024**2)\n",
    "    return params, mb\n",
    "\n",
    "def time_ms_per_query(model, din, pooling, n_patches):\n",
    "    device=next(model.parameters()).device\n",
    "    x=torch.randn(2048, din, device=device)\n",
    "    x=apply_pooling(x, pooling, n_patches)\n",
    "    if device.type==\"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    t0=time.time()\n",
    "    with torch.no_grad(): _=model(x)\n",
    "    if device.type==\"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    ms_gpu=(time.time()-t0)*1000/len(x)\n",
    "    # CPU\n",
    "    mcpu=model.to(\"cpu\"); xcpu=x.to(\"cpu\")\n",
    "    t1=time.time()\n",
    "    with torch.no_grad(): _=mcpu(xcpu)\n",
    "    ms_cpu=(time.time()-t1)*1000/len(xcpu)\n",
    "    model.to(device)\n",
    "    return ms_gpu, ms_cpu\n",
    "\n",
    "# ---------- Train (for optional fine-tune) ----------\n",
    "def train_one(model, loader, opt, alpha, beta, gamma, moment_w, pooling, n_patches, device):\n",
    "    model.train(); total=0.0\n",
    "    for xb,yb in loader:\n",
    "        xb,yb=xb.to(device), yb.to(device)\n",
    "        xb=apply_pooling(xb, pooling, n_patches)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        pred=model(xb)\n",
    "        # α·(1−cos) + β·MSE + λ·moment_align + γ·InfoNCE\n",
    "        cos = 1 - F.cosine_similarity(pred, yb, dim=-1).mean()\n",
    "        mse = F.mse_loss(pred, yb)\n",
    "        a_loss = moment_w * moment_align(pred, yb) if moment_w>0 else pred.new_tensor(0.0)\n",
    "        ce = info_nce(pred, yb) if gamma>0 else pred.new_tensor(0.0)\n",
    "        loss = alpha*cos + beta*mse + gamma*ce + a_loss\n",
    "        loss.backward(); opt.step()\n",
    "        total += loss.item()*xb.size(0)\n",
    "    return total/len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captions: 125k, Images: 25k ⇒ exactly 5 captions/image (125k / 25k = 5).\n",
    "\n",
    "Train: 112,500 captions over 22,500 images; Val: 12,500 captions over 2,500 images. Clean 90/10 split by unique image ids → good for avoiding leakage. \n",
    "\n",
    "Each listed image shows count = 5. Since all images have 5 captions, there’s no frequency skew in this dataset. Good news: balanced multi-caption coverage.\n",
    "\n",
    "sampling is effectively uniform over images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can safely skip the frequency-aware sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:52:04.068315Z",
     "iopub.status.busy": "2025-10-29T19:52:04.068019Z",
     "iopub.status.idle": "2025-10-29T19:52:04.080894Z",
     "shell.execute_reply": "2025-10-29T19:52:04.080089Z",
     "shell.execute_reply.started": "2025-10-29T19:52:04.068287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------- Residual Adapter + Geom (with Orthogonalization) ----------\n",
    "class ResidualAdapter(nn.Module):\n",
    "    def __init__(self, din=1024, hidden=1024, dout=1536, pdrop=0.1, init_scale=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(din, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, dout)\n",
    "        self.drop = nn.Dropout(pdrop)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight.mul_(init_scale)\n",
    "            self.fc2.weight.mul_(init_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop(h)\n",
    "        return self.fc2(h)\n",
    "\n",
    "\n",
    "class GeomWithAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    y_hat = GeomLinear(x) (+ optionally trainable) + Residual(x),\n",
    "    where Residual can be *projected* to the orthogonal complement of span(A) or *penalized* to be orthogonal.\n",
    "\n",
    "    residual_ortho: \"project\" | \"penalty\" | \"none\"\n",
    "    ortho_eps: ridge for projector\n",
    "    ortho_lambda: weight for penalty (if residual_ortho == \"penalty\")\n",
    "    unfreeze_bias: whether to unfreeze geom bias together with weight\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, A: np.ndarray, b: np.ndarray,\n",
    "        din=1024, dout=1536, hidden=1024, pdrop=0.1,\n",
    "        residual_ortho: str = \"project\",\n",
    "        ortho_eps: float = 1e-4,\n",
    "        ortho_lambda: float = 0.05,\n",
    "        unfreeze_bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.geom = GeomLinear(A, b)\n",
    "        self.adapter = ResidualAdapter(din=din, hidden=hidden, dout=dout, pdrop=pdrop)\n",
    "\n",
    "        # default: freeze geom at start; training loop may unfreeze weight later\n",
    "        for p in self.geom.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # orthogonalization config\n",
    "        assert residual_ortho in (\"project\", \"penalty\", \"none\")\n",
    "        self.residual_ortho = residual_ortho\n",
    "        self.ortho_lambda = float(ortho_lambda)\n",
    "        self.ortho = ResidualOrthogonalizer(eps=float(ortho_eps))\n",
    "        self._unfreeze_bias = bool(unfreeze_bias)\n",
    "\n",
    "    def unfreeze_geom(self, weight_lr_scale=0.05):\n",
    "        # In training we set per-parameter LRs in the optimizer; here just set requires_grad flags.\n",
    "        self.geom.fc.weight.requires_grad = True\n",
    "        if self._unfreeze_bias:\n",
    "            self.geom.fc.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, x, return_ortho_penalty: bool = False):\n",
    "        \"\"\"\n",
    "        If residual_ortho == \"project\": returns (yhat, 0.0) with hard projection (autocast OFF inside).\n",
    "        If residual_ortho == \"penalty\": returns (yhat, penalty_scalar).\n",
    "        If residual_ortho == \"none\": returns (yhat, 0.0).\n",
    "        \"\"\"\n",
    "        g = self.geom(x)               # (B,1536)\n",
    "        r = self.adapter(x)            # (B,1536)\n",
    "\n",
    "        penalty = None\n",
    "        if self.residual_ortho == \"project\":\n",
    "            # Hard projection of residual into orth complement of span(A) in float32, autocast OFF\n",
    "            A = self.geom.fc.weight     # (1536,1024)\n",
    "            device_type = \"cuda\" if g.device.type == \"cuda\" else \"cpu\"\n",
    "            with torch.autocast(device_type=device_type, enabled=False):\n",
    "                r = self.ortho.project_residual(r, A)\n",
    "            y = g + r\n",
    "            penalty = r.new_tensor(0.0)\n",
    "\n",
    "        elif self.residual_ortho == \"penalty\":\n",
    "            # Soft orthogonality: mean squared cosine between normalized parts\n",
    "            g_n = F.normalize(g, dim=-1)\n",
    "            r_n = F.normalize(r, dim=-1)\n",
    "            dot = torch.sum(g_n * r_n, dim=-1)  # (B,)\n",
    "            penalty = torch.mean(dot * dot)\n",
    "            y = g + r\n",
    "\n",
    "        else:  # \"none\"\n",
    "            y = g + r\n",
    "            penalty = r.new_tensor(0.0)\n",
    "\n",
    "        if return_ortho_penalty:\n",
    "            return y, penalty\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:09:39.224693Z",
     "iopub.status.busy": "2025-10-29T20:09:39.224232Z",
     "iopub.status.idle": "2025-10-29T20:09:39.236954Z",
     "shell.execute_reply": "2025-10-29T20:09:39.236323Z",
     "shell.execute_reply.started": "2025-10-29T20:09:39.224670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------- Residual Orthogonalization Utilities (AMP- & device-safe) ----------\n",
    "class ResidualOrthogonalizer:\n",
    "    \"\"\"\n",
    "    Projects residuals onto the orthogonal complement of span(A) safely under AMP and across devices.\n",
    "\n",
    "    Math (done in float32, autocast OFF):\n",
    "      G = A^T A + eps I   (1024x1024)\n",
    "      invG = G^{-1}       (or cholesky/pinv fallback)\n",
    "      Proj_col(A)(R) = ((R @ A) @ invG) @ A^T\n",
    "      (I - P_A)R = R - Proj_col(A)(R)\n",
    "\n",
    "    We cache invG per A-hash; at use, we always move invG to A.device to avoid CPU/GPU mismatches.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-4, device: torch.device | None = None):\n",
    "        self.eps = float(eps)\n",
    "        self.device = device\n",
    "        self._cached_invG = None\n",
    "        self._cached_A_hash = None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _hash_weight(self, A: torch.Tensor) -> tuple[int, int]:\n",
    "        h = (A.shape[0], A.shape[1])\n",
    "        sample = A.reshape(-1)[:1024] if A.numel() >= 1024 else A.reshape(-1)\n",
    "        checksum = int(torch.sum((sample.float() * 1e3).round()).item())\n",
    "        return (h[0] * 10_000 + h[1], checksum)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def refresh(self, A: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Ensure invG (float32) is up to date for current A.\n",
    "        Always computes with autocast DISABLED and in float32.\n",
    "        Caches on A.device.\n",
    "        \"\"\"\n",
    "        dev = A.device\n",
    "        device_type = \"cuda\" if dev.type == \"cuda\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            A32 = A.detach().to(dev, dtype=torch.float32)\n",
    "            key = self._hash_weight(A32)\n",
    "            # If cache hit AND cached tensor already on the right device, reuse\n",
    "            if key == self._cached_A_hash and self._cached_invG is not None and self._cached_invG.device == dev:\n",
    "                return\n",
    "\n",
    "            G = A32.transpose(0, 1) @ A32\n",
    "            G = G + self.eps * torch.eye(G.shape[0], device=dev, dtype=torch.float32)\n",
    "            try:\n",
    "                L = torch.linalg.cholesky(G)\n",
    "                invG = torch.cholesky_inverse(L)\n",
    "            except RuntimeError:\n",
    "                try:\n",
    "                    invG = torch.linalg.inv(G)\n",
    "                except RuntimeError:\n",
    "                    invG = torch.linalg.pinv(G)\n",
    "\n",
    "            self._cached_invG = invG   # stored on A.device\n",
    "            self._cached_A_hash = key\n",
    "\n",
    "    def project_residual(self, R: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return (I - P_A) R. Autocast OFF inside, math in float32, cast back to R.dtype at end.\n",
    "        Ensures invG and all operands are on the SAME device as A/R.\n",
    "        \"\"\"\n",
    "        dev = R.device\n",
    "        device_type = \"cuda\" if dev.type == \"cuda\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            # Make sure the cache matches current A AND is on correct device\n",
    "            self.refresh(A)\n",
    "            A32  = A.to(dtype=torch.float32, device=dev)\n",
    "            R32  = R.to(dtype=torch.float32, device=dev)\n",
    "            invG = self._cached_invG\n",
    "            if invG.device != dev:\n",
    "                invG = invG.to(dev)\n",
    "\n",
    "            # Proj_col(A)(R) = ((R @ A) @ invG) @ A^T\n",
    "            Z = R32 @ A32\n",
    "            Z = Z @ invG\n",
    "            R_proj = Z @ A32.transpose(0, 1)\n",
    "\n",
    "            R_orth = R32 - R_proj\n",
    "            return R_orth.to(dtype=R.dtype, device=dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def agreement_loss(names, preds, eps=1e-8):\n",
    "    \"\"\"\n",
    "    names: list[str] length B (image ids per caption)\n",
    "    preds: (B, D) unnormalized predictions\n",
    "    Returns mean variance across groups (after L2-norm), scalar.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        groups = {}\n",
    "        for i, n in enumerate(map(str, names)):\n",
    "            groups.setdefault(n, []).append(i)\n",
    "        valid = [idxs for idxs in groups.values() if len(idxs) >= 2]\n",
    "    if not valid:\n",
    "        return preds.new_tensor(0.0)\n",
    "    z = F.normalize(preds, dim=-1)\n",
    "    vars_ = []\n",
    "    for idxs in valid:\n",
    "        g = z[idxs]\n",
    "        v = g.var(dim=0, unbiased=False).mean()\n",
    "        vars_.append(v)\n",
    "    return torch.stack(vars_).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:44:46.159518Z",
     "iopub.status.busy": "2025-10-29T19:44:46.159010Z",
     "iopub.status.idle": "2025-10-29T19:44:46.167951Z",
     "shell.execute_reply": "2025-10-29T19:44:46.167303Z",
     "shell.execute_reply.started": "2025-10-29T19:44:46.159495Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImgQueue:\n",
    "    def __init__(self, dim: int, capacity: int, device: torch.device):\n",
    "        self.capacity = int(capacity)\n",
    "        self.device = device\n",
    "        self.ptr = 0                  # write pointer into a circular buffer\n",
    "        self.full = False\n",
    "        self.bank = torch.zeros(self.capacity, dim, device=device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def enqueue(self, feats: torch.Tensor):\n",
    "        # store as negatives only; avoid holding computation graphs\n",
    "        feats = feats.detach()\n",
    "        n = feats.size(0)\n",
    "        if n >= self.capacity:\n",
    "            # keep only the most recent 'capacity' entries\n",
    "            self.bank.copy_(feats[-self.capacity:])\n",
    "            self.ptr = 0\n",
    "            self.full = True\n",
    "            return\n",
    "        end = self.ptr + n\n",
    "        if end <= self.capacity:\n",
    "            self.bank[self.ptr:end] = feats\n",
    "        else:\n",
    "            cut = self.capacity - self.ptr\n",
    "            self.bank[self.ptr:] = feats[:cut]\n",
    "            self.bank[:end - self.capacity] = feats[cut:]\n",
    "        self.ptr = (self.ptr + n) % self.capacity\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def size(self) -> int:\n",
    "        # correct empty vs full handling\n",
    "        if self.full:\n",
    "            return self.capacity\n",
    "        return self.ptr  # 0 when empty\n",
    "\n",
    "    def get(self) -> torch.Tensor:\n",
    "        # return FIFO-ordered contents (oldest -> newest)\n",
    "        if self.size() == 0:\n",
    "            return self.bank[:0]  # empty tensor with correct shape\n",
    "        if self.full:\n",
    "            # [ptr:cap] then [0:ptr] so that tail is the most recent\n",
    "            return torch.cat([self.bank[self.ptr:], self.bank[:self.ptr]], dim=0)\n",
    "        # not full: contents are [0:ptr)\n",
    "        return self.bank[:self.ptr]\n",
    "\n",
    "    def recent(self, max_items: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return only the freshest 'max_items' features (chronological).\n",
    "        \"\"\"\n",
    "        q = self.get()\n",
    "        if q is None or q.numel() == 0:\n",
    "            return q\n",
    "        if max_items is None or q.size(0) <= int(max_items):\n",
    "            return q\n",
    "        return q[-int(max_items):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T21:21:30.128410Z",
     "iopub.status.busy": "2025-10-27T21:21:30.128053Z",
     "iopub.status.idle": "2025-10-27T21:21:30.150870Z",
     "shell.execute_reply": "2025-10-27T21:21:30.150255Z",
     "shell.execute_reply.started": "2025-10-27T21:21:30.128380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------- viz_utils.py (drop-in) -----------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import List, Sequence, Optional, Tuple\n",
    "\n",
    "@torch.inference_mode()\n",
    "def visualize_retrieval(\n",
    "    pred_emb: torch.Tensor,              # (D,) predicted caption->image vector (already in image space)\n",
    "    gt_index: int,                       # ground truth image index (into image_files / image_embeddings)\n",
    "    image_files: Sequence[str],\n",
    "    caption_text: str,\n",
    "    image_embeddings: torch.Tensor,      # (N, D) gallery in same space\n",
    "    k: int = 5,\n",
    "    dataset_path: str = \"/kaggle/working/data/train/train\",\n",
    ") -> Tuple[bool, Optional[int]]:\n",
    "    \"\"\"\n",
    "    Show ground-truth image + top-k retrieved images. Highlights correct match if present.\n",
    "    Returns: (gt_in_topk, gt_rank or None)\n",
    "    \"\"\"\n",
    "    # Ensure CPU + float\n",
    "    pred = F.normalize(pred_emb.view(1, -1), dim=-1).cpu()\n",
    "    gallery = F.normalize(image_embeddings, dim=-1).cpu()\n",
    "\n",
    "    # cosine similarities\n",
    "    sims = (pred @ gallery.T).squeeze(0).numpy()  # (N,)\n",
    "    retrieved = np.argsort(-sims)[:k]\n",
    "    gt_in_topk = int(gt_index) in retrieved\n",
    "    gt_rank = int(np.where(retrieved == gt_index)[0][0] + 1) if gt_in_topk else None\n",
    "\n",
    "    # figure\n",
    "    fig, axes = plt.subplots(1, k + 1, figsize=(22, 4))\n",
    "\n",
    "    def _load_show(ax, img_name, title, color=None, weight=None):\n",
    "        ax.axis(\"off\")\n",
    "        p = Path(dataset_path) / \"Images\" / img_name\n",
    "        try:\n",
    "            img = Image.open(p).convert(\"RGB\")\n",
    "            ax.imshow(img)\n",
    "        except Exception:\n",
    "            ax.text(0.5, 0.5, f\"Image not found:\\n{img_name}\", ha=\"center\", va=\"center\")\n",
    "        if color or weight:\n",
    "            ax.set_title(title, fontsize=10, color=color or \"black\", fontweight=weight or \"normal\")\n",
    "        else:\n",
    "            ax.set_title(title, fontsize=10)\n",
    "\n",
    "    # GT on the left\n",
    "    gt_name = image_files[int(gt_index)]\n",
    "    _load_show(axes[0], gt_name, f\"Ground Truth\\n{gt_name[:28]}...\", color=\"green\", weight=\"bold\")\n",
    "\n",
    "    # Top-k\n",
    "    for i, idx in enumerate(retrieved):\n",
    "        name = image_files[int(idx)]\n",
    "        is_gt = (int(idx) == int(gt_index))\n",
    "        title = f\"Rank {i+1}\\nCos: {sims[idx]:.3f}\" + (\"\\n✓ CORRECT\" if is_gt else \"\")\n",
    "        _load_show(axes[i+1], name, title, color=(\"green\" if is_gt else None), weight=(\"bold\" if is_gt else None))\n",
    "\n",
    "    plt.suptitle(f\"Query caption: “{caption_text}”\\nStatus: {'✓ Found at rank '+str(gt_rank) if gt_in_topk else f'✗ Not in top-{k}'}\",\n",
    "                 fontsize=13, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return bool(gt_in_topk), (int(gt_rank) if gt_rank is not None else None)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def visualize_samples(\n",
    "    model: torch.nn.Module,\n",
    "    text_embeddings: torch.Tensor,       # (M, din) caption embeddings (VAL)\n",
    "    image_embeddings: torch.Tensor,      # (N, D) gallery (VAL)\n",
    "    image_index_of_caption: Sequence[int],# length M: for each caption row, the GT image index\n",
    "    image_files: Sequence[str],          # length N image filenames\n",
    "    caption_texts: Optional[Sequence[str]] = None,  # optional strings to show (len M). If None, uses \"caption i\"\n",
    "    n_examples: int = 6,\n",
    "    k: int = 5,\n",
    "    device: Optional[torch.device] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Picks n_examples random caption rows, runs them through `model`, and calls visualize_retrieval().\n",
    "    Also prints the current gate value g (if your model has .gate_raw).\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    device = device or model_device\n",
    "\n",
    "    # Report gating scalar (if present)\n",
    "    g = None\n",
    "    if hasattr(model, \"gate_raw\"):\n",
    "        g = torch.sigmoid(model.gate_raw.detach().to(\"cpu\")).item()\n",
    "        print(f\"[gate] adapter gate g = {g:.4f} (0=off, 1=full)\")\n",
    "\n",
    "    M = text_embeddings.shape[0]\n",
    "    idxs = torch.randperm(M)[:n_examples].tolist()\n",
    "\n",
    "    for i, row in enumerate(idxs, 1):\n",
    "        x = text_embeddings[row:row+1].to(device)   # (1, din)\n",
    "        pred = model(x).squeeze(0)                  # (D,)\n",
    "        gt_idx = int(image_index_of_caption[row])\n",
    "        cap_txt = caption_texts[row] if (caption_texts is not None) else f\"caption {row}\"\n",
    "        print(f\"\\n[{i}/{n_examples}] cap_row={row} → GT image idx={gt_idx}\")\n",
    "        visualize_retrieval(\n",
    "            pred_emb=pred,\n",
    "            gt_index=gt_idx,\n",
    "            image_files=image_files,\n",
    "            caption_text=cap_txt,\n",
    "            image_embeddings=image_embeddings,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def compute_gt_ranks(\n",
    "    model: torch.nn.Module,\n",
    "    text_embeddings: torch.Tensor,         # (M, din)\n",
    "    image_embeddings: torch.Tensor,        # (N, D)\n",
    "    image_index_of_caption: Sequence[int], # (M,)\n",
    "    batch_size: int = 256,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns an array of size (M,) with 1-based ranks of the ground-truth image for each caption.\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    device = device or model_device\n",
    "\n",
    "    # Pre-norm gallery on device for speed\n",
    "    gallery = F.normalize(image_embeddings.to(device), dim=-1)      # (N, D)\n",
    "\n",
    "    ranks = []\n",
    "    M = text_embeddings.shape[0]\n",
    "    for i in range(0, M, batch_size):\n",
    "        xb = text_embeddings[i:i+batch_size].to(device)\n",
    "        pred = model(xb)                                            # (B, D)\n",
    "        pred = F.normalize(pred, dim=-1)\n",
    "        sims = pred @ gallery.T                                     # (B, N)\n",
    "        order = torch.argsort(sims, dim=1, descending=True)         # (B, N)\n",
    "        gt = torch.as_tensor(image_index_of_caption[i:i+xb.size(0)], device=device).view(-1, 1)\n",
    "        # find rank (1-based)\n",
    "        # We locate GT in each row\n",
    "        eq = (order == gt).nonzero(as_tuple=False)\n",
    "        # eq has rows: [row_idx, col_idx], where col_idx is the rank-1\n",
    "        pos = torch.zeros(xb.size(0), dtype=torch.long, device=device)\n",
    "        pos[eq[:,0]] = eq[:,1] + 1\n",
    "        ranks.append(pos.detach().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(ranks, axis=0)\n",
    "\n",
    "\n",
    "def plot_rank_hist(ranks: np.ndarray, k_marks: Tuple[int, ...] = (1,5,10,50)):\n",
    "    \"\"\"\n",
    "    Simple histogram of ranks with vertical lines for common top-k cutoffs.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    # do NOT set colors explicitly (keeps your style guide clean)\n",
    "    plt.hist(ranks, bins=1000)\n",
    "    for k in k_marks:\n",
    "        plt.axvline(k, linestyle=\"--\")\n",
    "    top1 = (ranks <= 1).mean()*100\n",
    "    top5 = (ranks <= 5).mean()*100\n",
    "    top10 = (ranks <= 10).mean()*100\n",
    "    plt.title(f\"GT rank distribution | R@1={top1:.1f}%  R@5={top5:.1f}%  R@10={top10:.1f}%\")\n",
    "    plt.xlabel(\"Ground-truth rank (1=best)\")\n",
    "    plt.ylabel(\"# captions\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings for new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:44:55.187864Z",
     "iopub.status.busy": "2025-10-29T19:44:55.187120Z",
     "iopub.status.idle": "2025-10-29T19:44:55.197551Z",
     "shell.execute_reply": "2025-10-29T19:44:55.196774Z",
     "shell.execute_reply.started": "2025-10-29T19:44:55.187838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== P×K Sampler (F1.1) ===============\n",
    "class PKBatchSampler(torch.utils.data.Sampler[list[int]]):\n",
    "    \"\"\"\n",
    "    Samples batches with P unique images, each with K captions.\n",
    "    Use in DataLoader(batch_sampler=PKBatchSampler(...)).\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ids: list[str], P: int, K: int, drop_last: bool = True, seed: int = 42):\n",
    "        # group dataset indices by image id\n",
    "        self.by_img = {}\n",
    "        for idx, iid in enumerate(map(str, img_ids)):\n",
    "            self.by_img.setdefault(iid, []).append(idx)\n",
    "        # shuffle within each image's index list\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        for lst in self.by_img.values():\n",
    "            perm = torch.randperm(len(lst), generator=g).tolist()\n",
    "            lst[:] = [lst[i] for i in perm]\n",
    "\n",
    "        self.P, self.K, self.drop_last = int(P), int(K), bool(drop_last)\n",
    "        self.iids = list(self.by_img.keys())\n",
    "        self.cur = {iid: 0 for iid in self.iids}\n",
    "        self.pool = [iid for iid in self.iids if len(self.by_img[iid]) > 0]\n",
    "\n",
    "    def __iter__(self):\n",
    "        import random\n",
    "        pool = self.pool[:]\n",
    "        random.shuffle(pool)\n",
    "        batch = []\n",
    "        while len(pool) >= self.P:\n",
    "            pick = pool[:self.P]\n",
    "            pool = pool[self.P:]\n",
    "            for iid in pick:\n",
    "                start = self.cur[iid]\n",
    "                end = start + self.K\n",
    "                lst = self.by_img[iid]\n",
    "                if start >= len(lst):\n",
    "                    continue\n",
    "                take = lst[start:min(end, len(lst))]\n",
    "                self.cur[iid] = min(end, len(lst))\n",
    "                batch.extend(take)\n",
    "            if len(batch) == self.P * self.K:\n",
    "                yield batch\n",
    "            else:\n",
    "                if not self.drop_last and len(batch) > 0:\n",
    "                    yield batch\n",
    "                break\n",
    "            batch = []\n",
    "\n",
    "    def __len__(self):\n",
    "        total = sum(len(v) // self.K for v in self.by_img.values())\n",
    "        return max(0, total // self.P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:44:58.508641Z",
     "iopub.status.busy": "2025-10-29T19:44:58.508170Z",
     "iopub.status.idle": "2025-10-29T19:44:58.518124Z",
     "shell.execute_reply": "2025-10-29T19:44:58.517479Z",
     "shell.execute_reply.started": "2025-10-29T19:44:58.508617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Cross-batch Positives Buffer (F1.2) ===============\n",
    "from collections import deque\n",
    "\n",
    "class XBPBuffer:\n",
    "    \"\"\"\n",
    "    Keeps recent predicted caption embeddings as extra positives per image.\n",
    "    Read-only in loss; write after optimizer step.\n",
    "    \"\"\"\n",
    "    def __init__(self, per_image_cap: int = 4, global_cap: int = 32000, device: torch.device = torch.device(\"cpu\")):\n",
    "        self.per_img = {}            # iid -> deque of normalized tensors\n",
    "        self.order = deque()         # (iid, stamp) to enforce global cap\n",
    "        self.per_image_cap = int(per_image_cap)\n",
    "        self.global_cap = int(global_cap)\n",
    "        self.device = device\n",
    "        self._count = 0\n",
    "        self._stamp = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add(self, img_ids: list[str], pred_feats: torch.Tensor):\n",
    "        feats = F.normalize(pred_feats.detach(), dim=-1)\n",
    "        for iid, f in zip(map(str, img_ids), feats):\n",
    "            dq = self.per_img.setdefault(iid, deque(maxlen=self.per_image_cap))\n",
    "            dq.append(f.to(self.device))\n",
    "            self.order.append((iid, self._stamp))\n",
    "            self._stamp += 1\n",
    "            self._count += 1\n",
    "            # trim global\n",
    "            while self._count > self.global_cap:\n",
    "                old_iid, _ = self.order.popleft()\n",
    "                if self.per_img.get(old_iid):\n",
    "                    try:\n",
    "                        self.per_img[old_iid].popleft()\n",
    "                        self._count -= 1\n",
    "                        if len(self.per_img[old_iid]) == 0:\n",
    "                            self.per_img.pop(old_iid, None)\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "\n",
    "    def build_bank_and_mask(self, batch_img_names: list[str], device: torch.device):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          xbp_bank: (M, D) stacked positives from XBP across batch (M can be 0)\n",
    "          xbp_pos_cols_per_row: list[list[int]] with column indices into xbp_bank for each row\n",
    "        \"\"\"\n",
    "        rows = [str(x) for x in batch_img_names]\n",
    "        per_row_feats = []\n",
    "        row_counts = []\n",
    "        for iid in rows:\n",
    "            dq = self.per_img.get(iid, None)\n",
    "            if dq is None or len(dq) == 0:\n",
    "                row_counts.append(0)\n",
    "            else:\n",
    "                row_counts.append(len(dq))\n",
    "                per_row_feats.extend(list(dq))\n",
    "        if len(per_row_feats) == 0:\n",
    "            return torch.empty(0, 0, device=device), [[] for _ in rows]\n",
    "        xbp_bank = torch.stack(per_row_feats, dim=0).to(device)\n",
    "        xbp_pos_cols = []\n",
    "        offset = 0\n",
    "        for cnt in row_counts:\n",
    "            if cnt == 0:\n",
    "                xbp_pos_cols.append([])\n",
    "            else:\n",
    "                xbp_pos_cols.append(list(range(offset, offset + cnt)))\n",
    "                offset += cnt\n",
    "        return xbp_bank, xbp_pos_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:45:02.086060Z",
     "iopub.status.busy": "2025-10-29T19:45:02.085802Z",
     "iopub.status.idle": "2025-10-29T19:45:02.091775Z",
     "shell.execute_reply": "2025-10-29T19:45:02.091000Z",
     "shell.execute_reply.started": "2025-10-29T19:45:02.086040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Hard-subset Mining (F2.1) ===============\n",
    "@torch.no_grad()\n",
    "def mine_hard(q_recent: torch.Tensor, pred: torch.Tensor, H: int, exclude_feats: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    q_recent: (Q,D) normalized features to mine from (negatives pool)\n",
    "    pred:     (B,D) normalized anchor predictions\n",
    "    Returns:\n",
    "      idx: (B,H) indices into q_recent for each anchor\n",
    "      flat_idx: (B*H,) flattened indices\n",
    "      mined: (B*H,D) the mined subset\n",
    "    \"\"\"\n",
    "    if q_recent is None or q_recent.numel() == 0 or H <= 0:\n",
    "        return None, None, None\n",
    "    B = pred.size(0)\n",
    "    Q = q_recent.size(0)\n",
    "    sims = pred @ q_recent.t()  # (B,Q)\n",
    "    H = min(int(H), Q)\n",
    "    _, idx = torch.topk(sims, k=H, dim=1, largest=True, sorted=False)  # (B,H)\n",
    "    flat_idx = idx.reshape(-1)\n",
    "    mined = q_recent.index_select(0, flat_idx)      # (B*H,D)\n",
    "    return idx, flat_idx, mined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:45:04.565718Z",
     "iopub.status.busy": "2025-10-29T19:45:04.565397Z",
     "iopub.status.idle": "2025-10-29T19:45:04.570504Z",
     "shell.execute_reply": "2025-10-29T19:45:04.569714Z",
     "shell.execute_reply.started": "2025-10-29T19:45:04.565697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Debiased NT-Xent denominator (F2.2) ===============\n",
    "def _debiased_logZ(logits: torch.Tensor, pos_mask: torch.Tensor, class_prior: float = 0.01):\n",
    "    \"\"\"\n",
    "    Chuang et al., 2020 (Debiased Contrastive Learning) — practical variant:\n",
    "      Z* = sum_k exp(l_ik) - π * sum_{j in P_i} exp(l_ij)\n",
    "    \"\"\"\n",
    "    exp_all = torch.exp(logits)          # (B,K)\n",
    "    exp_pos = exp_all * pos_mask\n",
    "    Z = exp_all.sum(dim=1)               # (B,)\n",
    "    corr = class_prior * exp_pos.sum(dim=1)\n",
    "    Z_star = torch.clamp(Z - corr, min=1e-8)\n",
    "    return torch.log(Z_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:45:07.781558Z",
     "iopub.status.busy": "2025-10-29T19:45:07.781243Z",
     "iopub.status.idle": "2025-10-29T19:45:07.792112Z",
     "shell.execute_reply": "2025-10-29T19:45:07.791368Z",
     "shell.execute_reply.started": "2025-10-29T19:45:07.781535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Multi-positive InfoNCE with XBP, Hard Mining, DCL (F1.1+F1.2+F2.1+F2.2) ===============\n",
    "def info_nce_multi(\n",
    "    pred,                                    # (B,D) unnormed\n",
    "    batch_img_targets,                       # (B,D)\n",
    "    batch_img_names,                         # list length B\n",
    "    queue_feats=None,                        # (Q,D) negatives pool\n",
    "    tau: float = 0.07,\n",
    "    xbp_bank: torch.Tensor = None,           # (M,D) extra positives\n",
    "    xbp_pos_cols_per_row: list[list[int]] = None,\n",
    "    hard_subset_H: int = 0,\n",
    "    use_dcl: bool = False,\n",
    "    dcl_prior: float = 0.01,\n",
    "):\n",
    "    p = F.normalize(pred, dim=-1)\n",
    "    t = F.normalize(batch_img_targets, dim=-1)\n",
    "\n",
    "    # base bank block 0: in-batch targets (positives exist here)\n",
    "    bank_blocks = [t]\n",
    "    B = pred.size(0)\n",
    "\n",
    "    # optional hard mining over queue\n",
    "    if queue_feats is not None and queue_feats.numel() > 0:\n",
    "        qn = F.normalize(queue_feats, dim=-1)\n",
    "        if hard_subset_H and hard_subset_H > 0:\n",
    "            _, _, mined_block = mine_hard(qn, p, int(hard_subset_H))\n",
    "            bank_blocks.append(mined_block)\n",
    "        else:\n",
    "            bank_blocks.append(qn)\n",
    "\n",
    "    # optional XBP block\n",
    "    xbp_offsets = None\n",
    "    if xbp_bank is not None and xbp_bank.numel() > 0:\n",
    "        xbp_offsets = sum(b.size(0) for b in bank_blocks)  # start col for XBP block\n",
    "        bank_blocks.append(xbp_bank)\n",
    "\n",
    "    bank = torch.cat(bank_blocks, dim=0) if len(bank_blocks) > 1 else bank_blocks[0]  # (K,D)\n",
    "    logits = (p @ bank.t()) / float(tau)                                              # (B,K)\n",
    "    K = bank.size(0)\n",
    "\n",
    "    # build pos mask: in-batch + XBP\n",
    "    pos_mask = torch.zeros(B, K, dtype=torch.bool, device=pred.device)\n",
    "    names = list(map(str, batch_img_names))\n",
    "    for i in range(B):\n",
    "        for j in range(B):\n",
    "            if names[i] == names[j]:\n",
    "                pos_mask[i, j] = True\n",
    "    if xbp_offsets is not None and xbp_pos_cols_per_row is not None:\n",
    "        for i, cols in enumerate(xbp_pos_cols_per_row):\n",
    "            for c in cols:\n",
    "                pos_mask[i, xbp_offsets + c] = True\n",
    "\n",
    "    assert pos_mask.any(dim=1).all(), \"Every row must have at least one positive.\"\n",
    "\n",
    "    # denominator: standard vs debiased\n",
    "    if not use_dcl:\n",
    "        logZ = torch.logsumexp(logits, dim=1)\n",
    "    else:\n",
    "        logZ = _debiased_logZ(logits, pos_mask, class_prior=float(dcl_prior))\n",
    "\n",
    "    logits_pos = logits.masked_fill(~pos_mask, float('-inf'))\n",
    "    logPos = torch.logsumexp(logits_pos, dim=1)\n",
    "    return (-(logPos - logZ)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:45:09.371148Z",
     "iopub.status.busy": "2025-10-29T19:45:09.370423Z",
     "iopub.status.idle": "2025-10-29T19:45:09.380218Z",
     "shell.execute_reply": "2025-10-29T19:45:09.379396Z",
     "shell.execute_reply.started": "2025-10-29T19:45:09.371122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =============== Dual Queue: Prediction FIFO (F2.3) ===============\n",
    "class PredQueue:\n",
    "    def __init__(self, dim: int, capacity: int, device: torch.device):\n",
    "        self.capacity = int(capacity)\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        self.feats = torch.zeros(self.capacity, dim, device=device)  # normalized feats\n",
    "        self.ids = torch.empty(self.capacity, dtype=torch.long, device=device)  # hashed img ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def enqueue(self, feats: torch.Tensor, img_ids: list[str]):\n",
    "        feats = F.normalize(feats.detach(), dim=-1)\n",
    "        ids = torch.tensor([hash(str(i)) for i in img_ids], dtype=torch.long, device=self.device)\n",
    "        n = feats.size(0)\n",
    "        if n >= self.capacity:\n",
    "            self.feats.copy_(feats[-self.capacity:])\n",
    "            self.ids.copy_(ids[-self.capacity:])\n",
    "            self.ptr = 0\n",
    "            self.full = True\n",
    "            return\n",
    "        end = self.ptr + n\n",
    "        if end <= self.capacity:\n",
    "            self.feats[self.ptr:end] = feats\n",
    "            self.ids[self.ptr:end] = ids\n",
    "        else:\n",
    "            cut = self.capacity - self.ptr\n",
    "            self.feats[self.ptr:] = feats[:cut]\n",
    "            self.ids[self.ptr:] = ids[:cut]\n",
    "            self.feats[:end - self.capacity] = feats[cut:]\n",
    "            self.ids[:end - self.capacity] = ids[cut:]\n",
    "        self.ptr = (self.ptr + n) % self.capacity\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def size(self) -> int:\n",
    "        if self.full:\n",
    "            return self.capacity\n",
    "        return self.ptr\n",
    "\n",
    "    def recent(self, max_items: int):\n",
    "        n = self.size()\n",
    "        if n == 0:\n",
    "            return self.feats[:0], self.ids[:0]\n",
    "        k = min(int(max_items), n) if max_items is not None else n\n",
    "        if self.full:\n",
    "            idx = torch.arange(self.ptr - n, self.ptr, device=self.device) % self.capacity\n",
    "        else:\n",
    "            idx = torch.arange(0, n, device=self.device)\n",
    "        idx = idx[-k:]\n",
    "        return self.feats.index_select(0, idx), self.ids.index_select(0, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T19:45:14.842830Z",
     "iopub.status.busy": "2025-10-29T19:45:14.842125Z",
     "iopub.status.idle": "2025-10-29T19:45:14.849323Z",
     "shell.execute_reply": "2025-10-29T19:45:14.848509Z",
     "shell.execute_reply.started": "2025-10-29T19:45:14.842802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_model(\n",
    "    arch: str, din: int, dout: int, geom_init_data=None,\n",
    "    residual_ortho: str = \"project\",\n",
    "    ortho_eps: float = 1e-4,\n",
    "    ortho_lambda: float = 0.05,\n",
    "    unfreeze_geom_bias: bool = False,\n",
    "):\n",
    "    a = arch.lower()\n",
    "    if a == \"linear\":\n",
    "        return LinearProj(din, dout)\n",
    "    if a == \"mlp1\":\n",
    "        return MLP1(din, dout, hidden=512, pdrop=0.1)\n",
    "    if a == \"mlp2\":\n",
    "        return MLP2(din, dout, h1=1024, h2=512, pdrop=0.1)\n",
    "    if a == \"geom\":\n",
    "        assert geom_init_data is not None, \"geom_init_data required for arch='geom'.\"\n",
    "        Xtr_np, Ytr_np, eps = geom_init_data\n",
    "        A, b = procrustes_closed_form(Xtr_np, Ytr_np, eps=float(eps))\n",
    "        return GeomLinear(A, b)\n",
    "    if a in (\"geom_adapter\", \"geom+adapter\", \"bigjump\"):\n",
    "        assert geom_init_data is not None, \"geom_init_data required for geom+adapter/bigjump.\"\n",
    "        Xtr_np, Ytr_np, eps = geom_init_data\n",
    "        A, b = procrustes_closed_form(Xtr_np, Ytr_np, eps=float(eps))\n",
    "        return GeomWithAdapter(\n",
    "            A, b, din=din, dout=dout, hidden=1024, pdrop=0.1,\n",
    "            residual_ortho=residual_ortho, ortho_eps=ortho_eps,\n",
    "            ortho_lambda=ortho_lambda, unfreeze_bias=unfreeze_geom_bias\n",
    "        )\n",
    "    if a == \"auto\":\n",
    "        return MLP2(din, dout)\n",
    "    raise ValueError(f\"Unknown arch {arch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:46:37.964103Z",
     "iopub.status.busy": "2025-10-29T20:46:37.963390Z",
     "iopub.status.idle": "2025-10-29T20:46:37.975418Z",
     "shell.execute_reply": "2025-10-29T20:46:37.974633Z",
     "shell.execute_reply.started": "2025-10-29T20:46:37.964065Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------- Centroid helpers ----------\n",
    "def _centroids_from_img_ids(X_rows: np.ndarray, Y_rows: np.ndarray, img_names_rows: np.ndarray):\n",
    "    \"\"\"\n",
    "    Build per-image centroids from per-caption rows.\n",
    "    Returns:\n",
    "      Xc: (N_img, 1024)   mean text per image\n",
    "      Yc: (N_img, 1536)   mean image per image (usually equals the single image vector)\n",
    "      order_names: list[str] image names in same order\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "    buckets_x = defaultdict(list)\n",
    "    buckets_y = defaultdict(list)\n",
    "    for x, y, n in zip(X_rows, Y_rows, map(str, img_names_rows)):\n",
    "        buckets_x[n].append(x)\n",
    "        buckets_y[n].append(y)\n",
    "    order_names = sorted(buckets_x.keys())\n",
    "    Xc = np.stack([np.mean(buckets_x[n], axis=0) for n in order_names], axis=0).astype(np.float32)\n",
    "    Yc = np.stack([np.mean(buckets_y[n], axis=0) for n in order_names], axis=0).astype(np.float32)\n",
    "    return Xc, Yc, order_names\n",
    "\n",
    "# ---------- Geometry: closed-form (whiten→orthogonal align→recolor) on CENTROIDS ----------\n",
    "def procrustes_closed_form_centroids(X_rows: np.ndarray, Y_rows: np.ndarray, img_names_rows: np.ndarray, eps: float = 1e-5):\n",
    "    \"\"\"\n",
    "    Fit linear A,b using per-image centroids instead of per-caption rows.\n",
    "    \"\"\"\n",
    "    Xc, Yc, _ = _centroids_from_img_ids(X_rows, Y_rows, img_names_rows)\n",
    "\n",
    "    mu_x = Xc.mean(0, dtype=np.float64)\n",
    "    mu_y = Yc.mean(0, dtype=np.float64)\n",
    "    Xzc = Xc - mu_x\n",
    "    Yzc = Yc - mu_y\n",
    "\n",
    "    def _cov_eigh(zc, eps):\n",
    "        S, U = np.linalg.eigh((zc.T @ zc) / max(1, zc.shape[0]))\n",
    "        S = np.clip(S, 0.0, None)\n",
    "        inv_sqrt = 1.0 / np.sqrt(S + eps)\n",
    "        sqrt     = np.sqrt(S + eps)\n",
    "        C_mhalf = (U * inv_sqrt) @ U.T\n",
    "        C_phalf = (U * sqrt)     @ U.T\n",
    "        return C_mhalf.astype(np.float32), C_phalf.astype(np.float32)\n",
    "\n",
    "    Cx_mh, _     = _cov_eigh(Xzc, eps)      # 1024x1024\n",
    "    Cy_mh, Cy_ph = _cov_eigh(Yzc, eps)      # 1536x1536\n",
    "    Xw = Xzc @ Cx_mh.T                      # (Nimg,1024)\n",
    "    Yw = Yzc @ Cy_mh.T                      # (Nimg,1536)\n",
    "\n",
    "    # rectangular orthogonal alignment in whitened space\n",
    "    M = Xw.T @ Yw                           # (1024,1536)\n",
    "    U, _, Vt = np.linalg.svd(M, full_matrices=False)\n",
    "    R = (U @ Vt).T                          # (1536,1024)\n",
    "\n",
    "    # recolor to Y space\n",
    "    A = (Cy_ph @ R @ Cx_mh).astype(np.float32)     # (1536,1024)\n",
    "    b = (mu_y - (A @ mu_x)).astype(np.float32)     # (1536,)\n",
    "    return A, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:46:49.955603Z",
     "iopub.status.busy": "2025-10-29T20:46:49.955119Z",
     "iopub.status.idle": "2025-10-29T20:46:49.961620Z",
     "shell.execute_reply": "2025-10-29T20:46:49.960791Z",
     "shell.execute_reply.started": "2025-10-29T20:46:49.955579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ---------- Losses: fixed-τ InfoNCE + Triplet (cosine) ----------\n",
    "def info_nce_fixed(pred: torch.Tensor, tgt: torch.Tensor, tau: float):\n",
    "    # both should be L2-normalized already\n",
    "    logits = (pred @ tgt.t()) / float(tau)                 # (B,B)\n",
    "    labels = torch.arange(pred.size(0), device=pred.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "class TripletCosineLoss(nn.Module):\n",
    "    def __init__(self, margin: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.loss = nn.TripletMarginWithDistanceLoss(\n",
    "            distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n",
    "            margin=margin\n",
    "        )\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        return self.loss(anchor, positive, negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T20:58:27.513672Z",
     "iopub.status.busy": "2025-10-29T20:58:27.512758Z",
     "iopub.status.idle": "2025-10-29T20:58:27.541695Z",
     "shell.execute_reply": "2025-10-29T20:58:27.540861Z",
     "shell.execute_reply.started": "2025-10-29T20:58:27.513639Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    \"\"\"\n",
    "    Main (advanced, two-stage): \n",
    "      - Stage 1: Centroid-Procrustes + Frozen Base + Residual, trained with fixed-τ InfoNCE + Triplet (stable warm-up).\n",
    "      - Stage 2: Continue from Stage 1 and switch to BigJump trainer with a *small, late* queue + light mining + tiny agreement. \n",
    "                 Geometry stays frozen. No hard residual projection.\n",
    "      - Then: evaluate, log efficiency, generate submission.\n",
    "\n",
    "    Assumes these already exist in your file:\n",
    "      load_train, build_image_id_split, procrustes_closed_form_centroids\n",
    "      ResidualAdapter, train_simple, train_bigjump\n",
    "      validate_retrieval, count_params_mb, time_ms_per_query\n",
    "      apply_pooling, load_data, generate_submission, TEST_NPZ\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    # -----------------------\n",
    "    # Setup\n",
    "    # -----------------------\n",
    "    out_dir, seed = args.out_dir, args.seed\n",
    "    pooling, n_patches = args.pooling, None\n",
    "    OUT = Path(f\"/kaggle/working/outputs/{out_dir}\")\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    STAGE1 = OUT / \"stage1\"\n",
    "    STAGE2 = OUT / \"stage2\"\n",
    "    STAGE1.mkdir(parents=True, exist_ok=True)\n",
    "    STAGE2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Data\n",
    "    # -----------------------\n",
    "    X, Y, cap_ids, img_ids_row, full_img, all_img_ids = load_train(OUT)\n",
    "\n",
    "    # Split by image id (NO LEAKAGE)\n",
    "    cap_is_tr, cap_is_val, val_gallery, cap2gal_local, val_img_indices = build_image_id_split(\n",
    "        img_ids_row, all_img_ids, full_img, X, Y, args.val_ratio, seed, OUT\n",
    "    )\n",
    "    val_set = set(all_img_ids[val_img_indices])\n",
    "    train_set = set(all_img_ids) - val_set\n",
    "    assert val_set.isdisjoint(train_set), \"Leakage detected between TRAIN and VAL image sets!\"\n",
    "\n",
    "    # Masks to arrays\n",
    "    Xtr, Ytr = X[cap_is_tr], Y[cap_is_tr]\n",
    "    Xva      = X[cap_is_val]\n",
    "\n",
    "    din, dout = X.shape[1], Y.shape[1]\n",
    "    assert din == 1024 and dout == 1536, f\"Dimension mismatch: text={din}, image={dout} (expected 1024→1536)\"\n",
    "\n",
    "    # -----------------------\n",
    "    # Geometry init (CENTROIDS on TRAIN ONLY)\n",
    "    # -----------------------\n",
    "    A, b = procrustes_closed_form_centroids(\n",
    "        Xtr.astype(np.float32),\n",
    "        Ytr.astype(np.float32),\n",
    "        img_ids_row[cap_is_tr],\n",
    "        eps=float(args.geom_eps)\n",
    "    )\n",
    "\n",
    "    # -----------------------\n",
    "    # Model: Frozen base (A,b) + residual adapter\n",
    "    # -----------------------\n",
    "    class GeomFromWeights(nn.Module):\n",
    "        def __init__(self, A_np, b_np):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Linear(A_np.shape[1], A_np.shape[0], bias=True)\n",
    "            with torch.no_grad():\n",
    "                self.fc.weight.copy_(torch.from_numpy(A_np))\n",
    "                self.fc.bias.copy_(torch.from_numpy(b_np))\n",
    "        def forward(self, x): return self.fc(x)\n",
    "\n",
    "    geom = GeomFromWeights(A, b).to(device)\n",
    "    for p in geom.parameters(): p.requires_grad = False  # keep base frozen in both stages\n",
    "\n",
    "    # residual capacity like BigJump adapter\n",
    "    adapter = ResidualAdapter(din=din, hidden=1024, dout=dout, pdrop=0.1)\n",
    "\n",
    "    class GeomPlusAdapter(nn.Module):\n",
    "        def __init__(self, g, a): super().__init__(); self.geom=g; self.adapter=a\n",
    "        def forward(self, x): return self.geom(x) + self.adapter(x)\n",
    "\n",
    "    model = GeomPlusAdapter(geom, adapter).to(device)\n",
    "\n",
    "    # Probe\n",
    "    with torch.no_grad():\n",
    "        _probe = model(torch.zeros(2, din, device=device))\n",
    "    assert _probe.shape[-1] == 1536, f\"Translator output dim { _probe.shape[-1] } != 1536\"\n",
    "\n",
    "    # -----------------------\n",
    "    # Stage 1: simple warm-up (fixed τ InfoNCE + Triplet, in-batch negatives)\n",
    "    # -----------------------\n",
    "    if not args.eval_only:\n",
    "        print(f\"[stage1] τ_fixed={args.tau:.3f} | triplet_w={args.triplet_w} | margin={args.triplet_margin} | epochs={args.stage1_epochs}\")\n",
    "        best_stats_s1 = train_simple(\n",
    "            model,\n",
    "            Xtr, Ytr, img_ids_row[cap_is_tr],\n",
    "            Xva, val_gallery, cap2gal_local,\n",
    "            batch=args.batch, epochs=int(args.stage1_epochs), lr=args.lr, wd=args.wd,\n",
    "            tau_fixed=float(args.tau), triplet_margin=float(args.triplet_margin), triplet_w=float(args.triplet_w),\n",
    "            alpha_cos=float(args.alpha), moment_w=float(args.moment),\n",
    "            use_pk=bool(args.use_pk), P=int(args.P), K_pk=int(args.K_pk),\n",
    "            device=device, pooling=pooling, n_patches=n_patches,\n",
    "            out_dir=str(STAGE1), seed=seed,\n",
    "        )\n",
    "        # Load best from stage1 (ensure we continue from the best)\n",
    "        try:\n",
    "            ckpt1 = torch.load(STAGE1 / \"best.pt\", map_location=device)\n",
    "            model.load_state_dict(ckpt1[\"model\"])\n",
    "            print(f\"[stage1] resume best epoch={ckpt1.get('epoch','?')} MRR={ckpt1.get('val',{}).get('MRR','?')}\")\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "    else:\n",
    "        best_stats_s1 = None\n",
    "\n",
    "    # -----------------------\n",
    "    # Stage 2: BigJump+ (small queue, late; light mining; tiny agreement; geometry still frozen)\n",
    "    # -----------------------\n",
    "    if not args.eval_only and args.stage2_epochs > 0:\n",
    "        print(f\"[stage2] queue_warmup={args.queue_warmup_epochs} | recent_k={args.queue_recent_k} | mine_H={args.mine_H} | \"\n",
    "              f\"τ_sched={args.tau_start:.3f}→{args.tau_end:.3f} | agree={args.lambda_agree}\")\n",
    "        # Keep geometry frozen in BigJump\n",
    "        best_stats_s2 = train_bigjump(\n",
    "            model,\n",
    "            Xtr, Ytr, img_ids_row[cap_is_tr],\n",
    "            Xva, val_gallery, cap2gal_local,\n",
    "            batch=args.batch, epochs=int(args.stage2_epochs), base_lr=args.lr, wd=args.wd,\n",
    "            # legacy knobs (we’ll let InfoNCE dominate; keep cosine light, moment tiny/off)\n",
    "            tau=None,                                 # use τ curriculum in stage2\n",
    "            alpha_cos=float(args.alpha_stage2),       # e.g., 0.3\n",
    "            lambda_moment=float(args.moment_stage2),  # e.g., 0.01\n",
    "            queue_size=int(args.queue),\n",
    "            # schedules\n",
    "            tau_start=float(args.tau_start), tau_end=float(args.tau_end),\n",
    "            queue_warmup_epochs=int(args.queue_warmup_epochs),\n",
    "            queue_recent_schedule=tuple(args.queue_recent_k),\n",
    "            lambda_agree=float(args.lambda_agree),\n",
    "            # keep geometry frozen in stage2 as well\n",
    "            geom_unfreeze_epoch=0,\n",
    "            geom_lr_scale=float(args.geom_lr_scale),\n",
    "            # misc\n",
    "            device=device, pooling=pooling, n_patches=n_patches,\n",
    "            out_dir=str(STAGE2), seed=seed,\n",
    "            # NEW knobs\n",
    "            use_pk=bool(args.use_pk), P=int(args.P), K_pk=int(args.K_pk),\n",
    "            xbp_per_img=int(args.xbp_per_img), xbp_global=int(args.xbp_global),\n",
    "            use_dcl=bool(args.use_dcl), dcl_prior=float(args.dcl_prior),\n",
    "            mine_H=int(args.mine_H),\n",
    "            use_pred_queue=False, queue_pred_capacity=int(args.queue_pred_capacity),\n",
    "        )\n",
    "        # Load best from stage2 for final export\n",
    "        try:\n",
    "            ckpt2 = torch.load(STAGE2 / \"best.pt\", map_location=device)\n",
    "            model.load_state_dict(ckpt2[\"model\"])\n",
    "            best_stats_final = ckpt2.get(\"val\", None)\n",
    "            print(f\"[stage2] resume best epoch={ckpt2.get('epoch','?')} MRR={ckpt2.get('val',{}).get('MRR','?')}\")\n",
    "        except FileNotFoundError:\n",
    "            best_stats_final = best_stats_s1\n",
    "    else:\n",
    "        # eval-only: try to load stage2 first, else stage1\n",
    "        best_stats_final = None\n",
    "        if args.eval_only:\n",
    "            for cand in [STAGE2 / \"best.pt\", STAGE1 / \"best.pt\", OUT / \"best.pt\"]:\n",
    "                try:\n",
    "                    ckpt = torch.load(cand, map_location=device)\n",
    "                    model.load_state_dict(ckpt[\"model\"])\n",
    "                    best_stats_final = ckpt.get(\"val\", None)\n",
    "                    print(f\"[resume] loaded {cand} | epoch={ckpt.get('epoch','?')} MRR={ckpt.get('val',{}).get('MRR','?')}\")\n",
    "                    break\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "    # -----------------------\n",
    "    # Efficiency logging\n",
    "    # -----------------------\n",
    "    params, mb = count_params_mb(model)\n",
    "    ms_gpu, ms_cpu = time_ms_per_query(model, din, pooling, n_patches)\n",
    "    eff = {\"params\": params, \"mb_fp32\": mb, \"ms_per_query_gpu\": ms_gpu, \"ms_per_query_cpu\": ms_cpu}\n",
    "    (OUT / \"efficiency.json\").write_text(json.dumps(eff, indent=2))\n",
    "\n",
    "    # -----------------------\n",
    "    # Submission\n",
    "    # -----------------------\n",
    "    test_data = load_data(TEST_NPZ)\n",
    "    Q   = test_data[\"captions/embeddings\"].astype(np.float32)\n",
    "    ids = test_data.get(\"captions/ids\", np.arange(len(Q)).astype(str))\n",
    "    model.eval()\n",
    "    BS, outs = 1024, []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(Q), BS):\n",
    "            q = torch.from_numpy(Q[i:i+BS]).to(device)\n",
    "            q = apply_pooling(q, pooling, n_patches)\n",
    "            z = model(q)\n",
    "            z = F.normalize(z, dim=-1)\n",
    "            outs.append(z.detach().cpu().numpy())\n",
    "    pred_embds = np.concatenate(outs, axis=0)\n",
    "    sub = OUT / \"submission.csv\"\n",
    "    generate_submission(ids, pred_embds, str(sub))\n",
    "    print(f\"[ok] submission written → {sub}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Final sanity print\n",
    "    # -----------------------\n",
    "    if best_stats_final is None:\n",
    "        best_stats_final = validate_retrieval(model, Xva, val_gallery, cap2gal_local, pooling, n_patches)\n",
    "\n",
    "    sanity = {\n",
    "        \"dims\": {\"text\": int(din), \"image\": int(dout)},\n",
    "        \"split\": {\n",
    "            \"train_captions\": int(cap_is_tr.sum()),\n",
    "            \"val_captions\": int(cap_is_val.sum()),\n",
    "            \"val_unique_images\": int(val_gallery.shape[0]),\n",
    "            \"leakage\": False\n",
    "        },\n",
    "        \"val_metrics\": best_stats_final,\n",
    "        \"efficiency\": eff,\n",
    "        \"recipe\": {\n",
    "            \"stage1\": {\n",
    "                \"type\": \"simple_fixed_tau_infoNCE+triplet\",\n",
    "                \"tau_fixed\": float(args.tau),\n",
    "                \"triplet_margin\": float(args.triplet_margin),\n",
    "                \"triplet_w\": float(args.triplet_w),\n",
    "                \"alpha_cos\": float(args.alpha),\n",
    "                \"moment_w\": float(args.moment),\n",
    "                \"epochs\": int(args.stage1_epochs),\n",
    "                \"use_pk\": bool(args.use_pk), \"P\": int(args.P), \"K\": int(args.K_pk)\n",
    "            },\n",
    "            \"stage2\": {\n",
    "                \"type\": \"bigjump_plus_small_queue_light_mining\",\n",
    "                \"epochs\": int(args.stage2_epochs),\n",
    "                \"tau_sched\": [float(args.tau_start), float(args.tau_end)],\n",
    "                \"queue_warmup_epochs\": int(args.queue_warmup_epochs),\n",
    "                \"queue_recent_schedule\": list(map(int, args.queue_recent_k)),\n",
    "                \"mine_H\": int(args.mine_H),\n",
    "                \"lambda_agree\": float(args.lambda_agree),\n",
    "                \"geom_unfreeze_epoch\": 0\n",
    "            },\n",
    "            \"centroid_procrustes\": True,\n",
    "            \"geom_frozen\": True,\n",
    "            \"residual_projection\": \"none\"\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(sanity, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T21:00:25.060355Z",
     "iopub.status.busy": "2025-10-29T21:00:25.059799Z",
     "iopub.status.idle": "2025-10-29T21:04:40.953038Z",
     "shell.execute_reply": "2025-10-29T21:04:40.952121Z",
     "shell.execute_reply.started": "2025-10-29T21:00:25.060331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[meta] text_dim=1024 | image_dim=1536\n",
      "[stage1] τ_fixed=0.070 | triplet_w=0.3 | margin=0.2 | epochs=12\n",
      "[simple 01] loss=2.166024 | val_MRR=0.3573 | R@1=0.232 R@5=0.496 R@10=0.620 | median=6 p75=23\n",
      "[simple 02] loss=2.019182 | val_MRR=0.3671 | R@1=0.241 R@5=0.508 R@10=0.635 | median=5 p75=21\n",
      "[simple 03] loss=1.967485 | val_MRR=0.3740 | R@1=0.246 R@5=0.514 R@10=0.643 | median=5 p75=19\n",
      "[simple 04] loss=1.931230 | val_MRR=0.3808 | R@1=0.251 R@5=0.525 R@10=0.651 | median=5 p75=19\n",
      "[simple 05] loss=1.901381 | val_MRR=0.3852 | R@1=0.256 R@5=0.531 R@10=0.658 | median=5 p75=18\n",
      "[simple 06] loss=1.875517 | val_MRR=0.3887 | R@1=0.259 R@5=0.534 R@10=0.663 | median=5 p75=17\n",
      "[simple 07] loss=1.854193 | val_MRR=0.3924 | R@1=0.261 R@5=0.541 R@10=0.669 | median=5 p75=17\n",
      "[simple 08] loss=1.834112 | val_MRR=0.3955 | R@1=0.266 R@5=0.542 R@10=0.669 | median=4 p75=17\n",
      "[simple 09] loss=1.816755 | val_MRR=0.3977 | R@1=0.267 R@5=0.546 R@10=0.674 | median=5 p75=16\n",
      "[simple 10] loss=1.799566 | val_MRR=0.3992 | R@1=0.269 R@5=0.547 R@10=0.675 | median=4 p75=16\n",
      "[simple 11] loss=1.785649 | val_MRR=0.3987 | R@1=0.267 R@5=0.546 R@10=0.677 | median=4 p75=16\n",
      "[simple 12] loss=1.771600 | val_MRR=0.4017 | R@1=0.270 R@5=0.549 R@10=0.681 | median=4 p75=16\n",
      "[stage1] resume best epoch=12 MRR=0.40171702851521446\n",
      "[stage2] queue_warmup=3 | recent_k=[8000, 16000, 65536] | mine_H=32 | τ_sched=0.100→0.060 | agree=0.02\n",
      "[bigjump 01] loss=2.903003 | val_MRR=0.4041 | R@1=0.273 R@5=0.551 R@10=0.681 | median=4 p75=16 | queue=65536 | tau=0.100 | recentQ=0\n",
      "[bigjump 02] loss=2.679751 | val_MRR=0.4015 | R@1=0.271 R@5=0.548 R@10=0.678 | median=4 p75=16 | queue=65536 | tau=0.099 | recentQ=0\n",
      "[bigjump 03] loss=2.653387 | val_MRR=0.4027 | R@1=0.271 R@5=0.551 R@10=0.681 | median=4 p75=16 | queue=65536 | tau=0.097 | recentQ=0\n",
      "[bigjump 04] loss=4.032444 | val_MRR=0.3864 | R@1=0.255 R@5=0.534 R@10=0.668 | median=5 p75=17 | queue=65536 | tau=0.094 | recentQ=8000\n",
      "[bigjump 05] loss=3.894188 | val_MRR=0.3967 | R@1=0.263 R@5=0.549 R@10=0.680 | median=4 p75=16 | queue=65536 | tau=0.090 | recentQ=16000\n",
      "[bigjump 06] loss=3.850924 | val_MRR=0.4109 | R@1=0.277 R@5=0.564 R@10=0.695 | median=4 p75=15 | queue=65536 | tau=0.085 | recentQ=65536\n",
      "[bigjump 07] loss=3.767136 | val_MRR=0.4188 | R@1=0.286 R@5=0.571 R@10=0.701 | median=4 p75=14 | queue=65536 | tau=0.080 | recentQ=65536\n",
      "[bigjump 08] loss=3.692709 | val_MRR=0.4256 | R@1=0.289 R@5=0.581 R@10=0.710 | median=4 p75=13 | queue=65536 | tau=0.075 | recentQ=65536\n",
      "[bigjump 09] loss=3.629012 | val_MRR=0.4327 | R@1=0.297 R@5=0.591 R@10=0.714 | median=4 p75=13 | queue=65536 | tau=0.070 | recentQ=65536\n",
      "[bigjump 10] loss=3.565447 | val_MRR=0.4383 | R@1=0.302 R@5=0.598 R@10=0.722 | median=4 p75=12 | queue=65536 | tau=0.066 | recentQ=65536\n",
      "[bigjump 11] loss=3.544977 | val_MRR=0.4423 | R@1=0.307 R@5=0.601 R@10=0.723 | median=3 p75=12 | queue=65536 | tau=0.063 | recentQ=65536\n",
      "[bigjump 12] loss=3.535456 | val_MRR=0.4467 | R@1=0.312 R@5=0.601 R@10=0.725 | median=3 p75=12 | queue=65536 | tau=0.061 | recentQ=65536\n",
      "[bigjump 13] loss=3.555670 | val_MRR=0.4474 | R@1=0.313 R@5=0.606 R@10=0.728 | median=3 p75=12 | queue=65536 | tau=0.060 | recentQ=65536\n",
      "[stage2] resume best epoch=13 MRR=0.44740885495820404\n",
      "Generating submission file...\n",
      "✓ Saved submission to /kaggle/working/outputs/two_stage_push/submission.csv\n",
      "[ok] submission written → /kaggle/working/outputs/two_stage_push/submission.csv\n",
      "{\n",
      "  \"dims\": {\n",
      "    \"text\": 1024,\n",
      "    \"image\": 1536\n",
      "  },\n",
      "  \"split\": {\n",
      "    \"train_captions\": 112500,\n",
      "    \"val_captions\": 12500,\n",
      "    \"val_unique_images\": 2500,\n",
      "    \"leakage\": false\n",
      "  },\n",
      "  \"val_metrics\": {\n",
      "    \"MRR\": 0.44740885495820404,\n",
      "    \"R1\": 0.31264,\n",
      "    \"R5\": 0.6056,\n",
      "    \"R10\": 0.72752,\n",
      "    \"rank_median\": 3,\n",
      "    \"rank_p75\": 12\n",
      "  },\n",
      "  \"efficiency\": {\n",
      "    \"params\": 4198400,\n",
      "    \"mb_fp32\": 16.015625,\n",
      "    \"ms_per_query_gpu\": 0.0014920951798558235,\n",
      "    \"ms_per_query_cpu\": 0.03991997800767422\n",
      "  },\n",
      "  \"recipe\": {\n",
      "    \"stage1\": {\n",
      "      \"type\": \"simple_fixed_tau_infoNCE+triplet\",\n",
      "      \"tau_fixed\": 0.07,\n",
      "      \"triplet_margin\": 0.2,\n",
      "      \"triplet_w\": 0.3,\n",
      "      \"alpha_cos\": 0.0,\n",
      "      \"moment_w\": 0.0,\n",
      "      \"epochs\": 12,\n",
      "      \"use_pk\": false,\n",
      "      \"P\": 192,\n",
      "      \"K\": 3\n",
      "    },\n",
      "    \"stage2\": {\n",
      "      \"type\": \"bigjump_plus_small_queue_light_mining\",\n",
      "      \"epochs\": 13,\n",
      "      \"tau_sched\": [\n",
      "        0.1,\n",
      "        0.06\n",
      "      ],\n",
      "      \"queue_warmup_epochs\": 3,\n",
      "      \"queue_recent_schedule\": [\n",
      "        8000,\n",
      "        16000,\n",
      "        65536\n",
      "      ],\n",
      "      \"mine_H\": 32,\n",
      "      \"lambda_agree\": 0.02,\n",
      "      \"geom_unfreeze_epoch\": 0\n",
      "    },\n",
      "    \"centroid_procrustes\": true,\n",
      "    \"geom_frozen\": true,\n",
      "    \"residual_projection\": \"none\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "#        ARGPARSE\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    p = argparse.ArgumentParser(description=\"RoBERTa→DINOv2 | 2-Stage (Centroid-Procrustes Warmup → BigJump+)\")\n",
    "\n",
    "    # IO & run mode\n",
    "    p.add_argument(\"--out_dir\", type=str, default=\"two_stage_push\")\n",
    "    p.add_argument(\"--seed\", type=int, default=42)\n",
    "    p.add_argument(\"--eval_only\", action=\"store_true\")\n",
    "\n",
    "    # data & split\n",
    "    p.add_argument(\"--val_ratio\", type=float, default=0.10)\n",
    "\n",
    "    # architecture (we still build geom+adapter internally)\n",
    "    p.add_argument(\"--arch\", type=str, default=\"bigjump\",\n",
    "                   choices=[\"linear\",\"mlp1\",\"mlp2\",\"geom\",\"geom_adapter\",\"geom+adapter\",\"bigjump\"])\n",
    "\n",
    "    # optimization (shared)\n",
    "    p.add_argument(\"--epochs\", type=int, default=0, help=\"(unused; stages have their own epochs)\")\n",
    "    p.add_argument(\"--batch\", type=int, default=512)\n",
    "    p.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    p.add_argument(\"--wd\", type=float, default=1e-4)\n",
    "\n",
    "    # geometry init\n",
    "    p.add_argument(\"--geom_eps\", type=float, default=1e-5)\n",
    "\n",
    "    # pooling (no-op)\n",
    "    p.add_argument(\"--pooling\", type=str, default=\"CLS\", choices=[\"CLS\"])\n",
    "\n",
    "    # ---------- STAGE 1 (Simple: fixed-τ InfoNCE + Triplet, in-batch negatives) ----------\n",
    "    p.add_argument(\"--stage1_epochs\", type=int, default=12)\n",
    "    p.add_argument(\"--tau\", type=float, default=0.07, help=\"Fixed InfoNCE temperature for Stage 1.\")\n",
    "    p.add_argument(\"--triplet_margin\", type=float, default=0.2)\n",
    "    p.add_argument(\"--triplet_w\", type=float, default=0.3, help=\"Triplet weight; InfoNCE weight = 1 - triplet_w.\")\n",
    "    p.add_argument(\"--alpha\", type=float, default=0.0, help=\"Cosine auxiliary weight in Stage 1.\")\n",
    "    p.add_argument(\"--moment\", type=float, default=0.0, help=\"Moment alignment weight in Stage 1.\")\n",
    "\n",
    "    # Optional PK sampler (both stages)\n",
    "    p.add_argument(\"--use_pk\", action=\"store_true\")\n",
    "    p.add_argument(\"--P\", type=int, default=192)\n",
    "    p.add_argument(\"--K_pk\", type=int, default=3)\n",
    "\n",
    "    # ---------- STAGE 2 (BigJump+: small late queue, light mining, tiny agreement) ----------\n",
    "    p.add_argument(\"--stage2_epochs\", type=int, default=13)\n",
    "    p.add_argument(\"--tau_start\", type=float, default=0.10)\n",
    "    p.add_argument(\"--tau_end\",   type=float, default=0.06)\n",
    "\n",
    "    p.add_argument(\"--queue\", type=int, default=65536)\n",
    "    p.add_argument(\"--queue_warmup_epochs\", type=int, default=3)\n",
    "    p.add_argument(\"--queue_recent_k\", type=int, nargs=3, default=[8000, 16000, 65536])\n",
    "\n",
    "    p.add_argument(\"--mine_H\", type=int, default=32, help=\"Top-H hard negatives per anchor (recent queue slice).\")\n",
    "    p.add_argument(\"--lambda_agree\", type=float, default=0.02, help=\"Caption-agreement weight (tiny).\")\n",
    "    p.add_argument(\"--alpha_stage2\", type=float, default=0.3, help=\"Cosine auxiliary in Stage 2.\")\n",
    "    p.add_argument(\"--moment_stage2\", type=float, default=0.01, help=\"Moment alignment in Stage 2.\")\n",
    "    p.add_argument(\"--geom_lr_scale\", type=float, default=0.05)\n",
    "\n",
    "    # XBP / DCL / dual-queue toggles (kept for completeness; default off/safe)\n",
    "    p.add_argument(\"--xbp_per_img\", type=int, default=4)\n",
    "    p.add_argument(\"--xbp_global\", type=int, default=32000)\n",
    "    p.add_argument(\"--use_dcl\", action=\"store_true\")\n",
    "    p.add_argument(\"--dcl_prior\", type=float, default=0.01)\n",
    "    p.add_argument(\"--queue_pred_capacity\", type=int, default=65536)\n",
    "\n",
    "    args, _ = p.parse_known_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14220991,
     "sourceId": 117959,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

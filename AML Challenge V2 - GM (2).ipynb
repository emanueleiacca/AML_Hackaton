{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19240,"status":"ok","timestamp":1761861997614,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"},"user_tz":-60},"id":"DGa1mGbfmpH-","outputId":"8e5a66c7-70a3-40eb-f9ba-e133ba940d80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":527,"status":"ok","timestamp":1761862026591,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"},"user_tz":-60},"id":"0sD6oJABmys8","outputId":"542cd6d5-d795-482a-d7ae-ae009fa17979"},"outputs":[{"output_type":"stream","name":"stdout","text":["Current working directory: /content/drive/MyDrive/AML Challenge\n"]}],"source":["import os\n","\n","BASE_DIR = \"/content/drive/MyDrive/AML Challenge\"\n","os.chdir(BASE_DIR)\n","print(\"Current working directory:\", os.getcwd())\n"]},{"cell_type":"markdown","metadata":{"id":"INNIVdJ4phrW"},"source":["# Pre-Processing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24830,"status":"ok","timestamp":1761862055624,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"},"user_tz":-60},"id":"BNsu63vepjn7","outputId":"a6885d58-5013-4212-c6d1-9bcc255109e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024) (25000, 1536)\n","Test shape: (1500, 1024)\n"]}],"source":["import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np, pandas as pd\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]   # (125000, 1024)\n","im_train = train_data[\"images/embeddings\"]     # (25000, 1536)\n","tx_test  = test_data[\"captions/embeddings\"]    # (1500, 1024)\n","\n","print(\"Train shapes:\", tx_train.shape, im_train.shape)\n","print(\"Test shape:\", tx_test.shape)\n","\n","# ------------------------------------------------------\n","# 2. Match each caption to its corresponding image\n","# ------------------------------------------------------\n","repeat_factor = len(tx_train) // len(im_train)   # 5 captions per image\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)"]},{"cell_type":"markdown","metadata":{"id":"r4gxkwL7pM0F"},"source":["# Experiment 1: got score as 0.81780\n","# Residual-Orthogonal + Contrastive version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109633,"status":"ok","timestamp":1761772546022,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"},"user_tz":-60},"id":"E62bqTXwpRBn","outputId":"84ebf2ed-8865-45be-e234-5b1efe877fd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","\n","Training Residual-Orthogonal Translator...\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.5180\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.3856\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.3359\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.3061\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.2844\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.2677\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.2551\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.2456\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.2356\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.2279\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.2205\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.2146\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.2085\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.2038\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.1987\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.1950\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.1912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.1873\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.1838\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.1801\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 1.1774\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 1.1750\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 1.1724\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 1.1698\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 1.1676\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 1.1649\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 1.1626\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 1.1611\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 1.1584\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 1.1563\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: avg loss = 1.1551\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: avg loss = 1.1540\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: avg loss = 1.1522\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: avg loss = 1.1503\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: avg loss = 1.1491\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: avg loss = 1.1479\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: avg loss = 1.1457\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: avg loss = 1.1442\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: avg loss = 1.1431\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: avg loss = 1.1413\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 41: avg loss = 1.1420\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 42: avg loss = 1.1397\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 43: avg loss = 1.1392\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 44: avg loss = 1.1381\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 45: avg loss = 1.1366\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 46: avg loss = 1.1360\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 47: avg loss = 1.1351\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 48: avg loss = 1.1343\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 49: avg loss = 1.1329\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 50: avg loss = 1.1321\n","\n","✅ Training completed and model saved as residual_orthogonal.pth\n","✅ Saved submission_residual.csv\n","   id                                          embedding\n","0   1  [-0.010053236037492752, 0.014468204230070114, ...\n","1   2  [-0.03426738828420639, -0.030702663585543633, ...\n","2   3  [-0.0029787796083837748, -0.02091299369931221,...\n"]}],"source":["# ------------------------------------------------------\n","# 2. Match each caption to its corresponding image\n","# ------------------------------------------------------\n","repeat_factor = len(tx_train) // len(im_train)   # 5 captions per image\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","\n","# ------------------------------------------------------\n","# 3. Convert to tensors + center + normalize\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","# Center + normalize (same mean for test)\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t.mean(0, keepdim=True)\n","\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t = F.normalize(im_train_t - im_mean, p=2, dim=1)\n","tx_test_t  = F.normalize(tx_test_t  - tx_mean, p=2, dim=1)\n","\n","# Expand image embeddings for each caption\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 4. Compute Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh  # (1536 × 1024)\n","\n","print(\"Computed orthogonal base R:\", R.shape)\n","\n","# ------------------------------------------------------\n","# 5. Define Residual-Orthogonal Translator\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        out = F.normalize(base + res, p=2, dim=1)\n","        return out\n","\n","# ------------------------------------------------------\n","# 6. Define losses\n","# ------------------------------------------------------\n","def contrastive_loss(pred, target, tau=0.07):\n","    sims = pred @ target.T / tau\n","    labels = torch.arange(pred.size(0), device=device)\n","    return F.cross_entropy(sims, labels)\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 7. DataLoader\n","# ------------------------------------------------------\n","train_dataset = TensorDataset(tx_train_t, im_train_exp)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","\n","# ------------------------------------------------------\n","# 8. Initialize model + optimizer\n","# ------------------------------------------------------\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","\n","# ------------------------------------------------------\n","# 9. Training loop\n","# ------------------------------------------------------\n","EPOCHS = 50\n","print(\"\\nTraining Residual-Orthogonal Translator...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        optimizer.zero_grad()\n","\n","        y_pred = model(x_batch)\n","        idx = torch.randperm(x_batch.size(0), device=device)\n","        y_neg = y_batch[idx]\n","\n","        loss_cos = contrastive_loss(y_pred, y_batch)\n","        loss_tri = triplet_loss_fn(y_pred, y_batch, y_neg)\n","        loss = 0.6 * loss_cos + 0.4 * loss_tri\n","\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch:02d}: avg loss = {avg_loss:.4f}\")\n","\n","torch.save(model.state_dict(), \"residual_orthogonal.pth\")\n","print(\"\\n✅ Training completed and model saved as residual_orthogonal.pth\")\n","\n","# ------------------------------------------------------\n","# 10. Inference for submission\n","# ------------------------------------------------------\n","model.eval()\n","with torch.no_grad():\n","    tx_test_n = F.normalize(tx_test_t, p=2, dim=1)\n","    preds = model(tx_test_n).cpu().numpy()\n","\n","# ------------------------------------------------------\n","# 11. Save submission file\n","# ------------------------------------------------------\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_residual.csv\", index=False)\n","\n","print(\"✅ Saved submission_residual.csv\")\n","print(submission.head(3))"]},{"cell_type":"markdown","source":["# hard_negative approach"],"metadata":{"id":"K0tVvFJE8FJs"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import os\n","from google.colab import drive\n","\n","\n","# ------------------------------------------------------\n","# 3. Convert to tensors + center + normalize\n","#    (Using the exact preprocessing from your 0.81780 experiment)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device) # For R\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","# Center + normalize (same mean for test)\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True) # Mean of unique images\n","\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t  = F.normalize(tx_test_t  - tx_mean, p=2, dim=1)\n","\n","# Expand image embeddings for each caption (for training)\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 4. Compute Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh  # (1536 × 1024)\n","\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 5. Define Residual-Orthogonal Translator\n","#    (Same model as your 0.81780 experiment)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        out = F.normalize(base + res, p=2, dim=1)\n","        return out\n","\n","# ------------------------------------------------------\n","# 6. Define losses\n","# ------------------------------------------------------\n","def contrastive_loss(pred, target, tau=0.07):\n","    # pred: (B, D), target: (B, D)\n","    sims = pred @ target.T / tau\n","    labels = torch.arange(pred.size(0), device=device)\n","    return F.cross_entropy(sims, labels)\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 7. DataLoader\n","# ------------------------------------------------------\n","train_dataset = TensorDataset(tx_train_t, im_train_exp)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","\n","# ------------------------------------------------------\n","# 8. Initialize model + optimizer\n","# ------------------------------------------------------\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","scaler = torch.cuda.amp.GradScaler() # For mixed precision\n","\n","# ------------------------------------------------------\n","# 9. Training loop with HARD-NEGATIVE MINING\n","# ------------------------------------------------------\n","EPOCHS = 30\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_TRIPLET = 0.3 # Weight for the triplet loss term\n","LOSS_WEIGHT_CONTRASTIVE = 0.7 # Weight for the contrastive loss term\n","\n","print(\"\\nTraining Residual-Orthogonal Translator with Hard-Negative Mining...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            # --- Forward pass ---\n","            y_pred = model(x_batch) # (B, 1536)\n","\n","            # --- 1. Contrastive Loss (InfoNCE) ---\n","            # This is the main loss, computes (B, B) similarity matrix\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_cos = F.cross_entropy(sims_with_tau, labels)\n","\n","            # --- 2. Hard-Negative Mining for Triplet Loss ---\n","            with torch.no_grad():\n","                # We need similarities *without* temperature scaling\n","                sims_no_tau = y_pred @ y_batch.T # (B, B)\n","\n","                # Mask out the positive sample (the diagonal)\n","                sims_no_tau.masked_fill_(\n","                    torch.eye(y_batch.size(0), device=device, dtype=torch.bool),\n","                    -float('inf')\n","                )\n","\n","                # Find the highest-similarity (hardest) negative for each sample\n","                hard_neg_idx = sims_no_tau.argmax(dim=1) # (B)\n","\n","            y_hard_neg = y_batch[hard_neg_idx] # (B, 1536)\n","\n","            # --- 3. Triplet Loss (with hard negatives) ---\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            # --- 4. Combine losses ---\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_cos) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        # --- Backward pass ---\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch:02d}: avg loss = {avg_loss:.4f}\")\n","\n","torch.save(model.state_dict(), \"residual_hard_negative.pth\")\n","print(\"\\n✅ Training completed and model saved as residual_hard_negative.pth\")\n","\n","# ------------------------------------------------------\n","# 10. Inference for submission\n","# ------------------------------------------------------\n","model.eval()\n","with torch.no_grad():\n","    # We normalized tx_test_t during preprocessing\n","    preds = model(tx_test_t).cpu().numpy()\n","\n","# ------------------------------------------------------\n","# 11. Save submission file\n","# ------------------------------------------------------\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_hard_negative.csv\", index=False)\n","\n","print(\"✅ Saved submission_hard_negative.csv\")\n","print(submission.head(3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWQ2iNAI8Oll","executionInfo":{"status":"ok","timestamp":1761774564775,"user_tz":-60,"elapsed":67239,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"407b01d7-5300-493a-dd70-cef330070b6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-908034218.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # For mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","\n","Training Residual-Orthogonal Translator with Hard-Negative Mining...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/30:   0%|          | 0/245 [00:00<?, ?it/s]/tmp/ipython-input-908034218.py:109: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.8259\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.6697\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.6106\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.5756\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.5504\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.5309\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.5158\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.5029\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.4926\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.4824\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.4739\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.4665\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.4591\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.4535\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.4478\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.4431\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.4390\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.4329\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.4304\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.4263\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 1.4226\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 1.4196\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 1.4160\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 1.4138\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 1.4106\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 1.4079\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 1.4045\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 1.4034\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 1.4006\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 1.3981\n","\n","✅ Training completed and model saved as residual_hard_negative.pth\n","✅ Saved submission_hard_negative.csv\n","   id                                          embedding\n","0   1  [-0.010386133566498756, 0.015329176560044289, ...\n","1   2  [-0.03257265314459801, -0.027175934985280037, ...\n","2   3  [-0.00526655837893486, -0.014171533286571503, ...\n"]}]},{"cell_type":"markdown","source":["# Experiment 3: Residual + Hard Negative"],"metadata":{"id":"zVgRFGybAKoO"}},{"cell_type":"code","source":["# ======================================================\n","#  AML CHALLENGE — Residual-Orthogonal Translator v3\n","#  (Hard Negatives + Orthogonal Reg + CSLS + Ensemble)\n","# ======================================================\n","\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np, pandas as pd\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ------------------------------------------------------\n","# 1. Data preprocessing  (same as before)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 2. Orthogonal Procrustes base\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 3. Model\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res  = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# Orthogonality penalty\n","def orthogonal_penalty(W):\n","    WT_W = W.T @ W\n","    I = torch.eye(WT_W.shape[0], device=W.device)\n","    return ((WT_W - I)**2).mean()\n","\n","# ------------------------------------------------------\n","# 4. Losses\n","# ------------------------------------------------------\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 5. Dataloader\n","# ------------------------------------------------------\n","train_dataset = TensorDataset(tx_train_t, im_train_exp)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","\n","# ------------------------------------------------------\n","# 6. Train\n","# ------------------------------------------------------\n","def train_model(seed=42, save_path=\"residual_v3.pth\"):\n","    torch.manual_seed(seed)\n","    model = ResidualTranslator(R.detach().clone()).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","    scaler = torch.amp.GradScaler(\"cuda\")\n","\n","    EPOCHS = 30\n","    TAU = 0.07\n","    LOSS_W_CONTR = 0.7\n","    LOSS_W_TRIP = 0.3\n","    ORTHO_W = 1e-4\n","\n","    print(f\"\\nTraining Residual-Orthogonal Translator (seed={seed})...\\n\")\n","    for epoch in range(1, EPOCHS + 1):\n","        model.train()\n","        total_loss = 0.0\n","        for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","            optimizer.zero_grad()\n","            with torch.amp.autocast(\"cuda\"):\n","                y_pred = model(x_batch)\n","                sims = y_pred @ y_batch.T / TAU\n","                labels = torch.arange(y_pred.size(0), device=device)\n","                loss_con = F.cross_entropy(sims, labels)\n","\n","                with torch.no_grad():\n","                    sims_no_tau = y_pred @ y_batch.T\n","                    sims_no_tau.masked_fill_(torch.eye(y_batch.size(0), device=device, dtype=torch.bool), -float('inf'))\n","                    hard_idx = sims_no_tau.argmax(dim=1)\n","                y_hard = y_batch[hard_idx]\n","                loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard)\n","\n","                loss_ortho = orthogonal_penalty(model.R)\n","                loss = (LOSS_W_CONTR * loss_con) + (LOSS_W_TRIP * loss_tri) + (ORTHO_W * loss_ortho)\n","\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            total_loss += loss.item()\n","        print(f\"Epoch {epoch:02d}: avg loss = {total_loss/len(train_loader):.4f}\")\n","\n","    torch.save(model.state_dict(), save_path)\n","    print(f\"\\n✅ Model saved as {save_path}\\n\")\n","    return model\n","\n","# ------------------------------------------------------\n","# 7. Train + Ensemble (3 seeds)\n","# ------------------------------------------------------\n","models = []\n","for seed in [42, 77, 123]:\n","    model = train_model(seed, save_path=f\"residual_v3_seed{seed}.pth\")\n","    models.append(model)\n","\n","# ------------------------------------------------------\n","# 8. Inference (ensemble-averaged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def predict_ensemble(models, tx_test_t):\n","    preds_all = []\n","    for model in models:\n","        model.eval()\n","        preds_all.append(F.normalize(model(tx_test_t), p=2, dim=1).cpu().numpy())\n","    return np.mean(preds_all, axis=0)\n","\n","preds = predict_ensemble(models, tx_test_t)\n","\n","# ------------------------------------------------------\n","# 9. CSLS re-ranking (optional at test time)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def apply_csls(preds, im_base, k=10):\n","    preds = F.normalize(torch.tensor(preds, device=device), p=2, dim=1)\n","    im_base = F.normalize(im_base, p=2, dim=1)\n","    sim = preds @ im_base.T\n","    knn_q = torch.topk(sim, k=k, dim=1).values.mean(1, keepdim=True)\n","    knn_d = torch.topk(sim, k=k, dim=0).values.mean(0, keepdim=True)\n","    return (2 * sim - knn_q - knn_d).cpu().numpy()\n","\n","csls_sim = apply_csls(preds, im_train_t_unique)\n","\n","# ------------------------------------------------------\n","# 10. Save submission\n","# ------------------------------------------------------\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_residual_v3.csv\", index=False)\n","print(\"✅ submission_residual_v3.csv saved.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQQ_nBSxAYMW","executionInfo":{"status":"ok","timestamp":1761775748910,"user_tz":-60,"elapsed":194094,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"7b76e548-51cf-40a0-cd8e-bc61d3a2cc31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Data preprocessed and normalized.\n","Computed orthogonal base R: torch.Size([1536, 1024])\n","\n","Training Residual-Orthogonal Translator (seed=42)...\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.8249\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.6702\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.6105\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.5747\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.5506\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.5319\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.5163\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.5025\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.4919\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.4816\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.4734\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.4671\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.4598\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.4545\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.4482\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.4430\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.4380\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.4343\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.4303\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.4252\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 1.4222\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 1.4185\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 1.4157\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 1.4136\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 1.4098\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 1.4065\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 1.4053\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 1.4031\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 1.3996\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 1.3982\n","\n","✅ Model saved as residual_v3_seed42.pth\n","\n","\n","Training Residual-Orthogonal Translator (seed=77)...\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.8243\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.6696\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.6102\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.5757\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.5505\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.5311\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.5163\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.5034\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.4916\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.4823\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.4739\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.4669\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.4601\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.4538\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.4480\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.4431\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.4387\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.4351\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.4306\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.4263\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 1.4222\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 1.4196\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 1.4163\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 1.4131\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 1.4102\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 1.4079\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 1.4046\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 1.4033\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 1.3996\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 1.3976\n","\n","✅ Model saved as residual_v3_seed77.pth\n","\n","\n","Training Residual-Orthogonal Translator (seed=123)...\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.8248\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.6695\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.6100\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.5749\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.5509\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.5315\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.5155\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.5035\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.4925\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.4827\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.4734\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.4677\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.4602\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.4547\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.4492\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.4440\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.4395\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.4344\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.4301\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.4266\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 1.4228\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 1.4196\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 1.4164\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 1.4128\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 1.4103\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 1.4077\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 1.4061\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 1.4028\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 1.4008\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 1.3987\n","\n","✅ Model saved as residual_v3_seed123.pth\n","\n","✅ submission_residual_v3.csv saved.\n"]}]},{"cell_type":"markdown","source":["##"],"metadata":{"id":"ShRub6L-8Ccn"}},{"cell_type":"markdown","source":["#  Model: Residual-Orthogonal TRANSFORMER Translator\n","\n"],"metadata":{"id":"J0wpKofuJyvK"}},{"cell_type":"code","source":["\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. Model: Residual-Orthogonal (Simple MLP)\n","#    (Your proven 0.81780-scoring model)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Contrastive + CORRECT Triplet)\n","# ------------------------------------------------------\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (10%)\n","#    (We MUST pass the original image index)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\") # Use CPU for randperm\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","# Get the corresponding image index (0-24999) for each caption\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","# --- Create datasets ---\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","# --- Dataloaders now include the image index ---\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (for validation)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    \"\"\"\n","    Evaluates Text-to-Image Recall@K using CSLS re-ranking.\n","    - tx_queries: (N_queries, 1024) tensor of text embeddings\n","    - im_database: (N_images, 1536) tensor of ALL unique image embeddings\n","    - query_img_indices: (N_queries,) tensor of GT image indices (0-24999)\n","    \"\"\"\n","    model.eval()\n","\n","    # --- 1. Predict caption embeddings ---\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0) # (N_queries, 1536)\n","\n","    # --- 2. Compute Cosine Similarity Matrix ---\n","    sim = preds @ im_database.T # (N_queries, N_images)\n","\n","    # --- 3. Compute CSLS re-ranking scores ---\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True) # (N_queries, 1)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T # (1, N_images)\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d # (N_queries, N_images)\n","\n","    # --- 4. Get Ground Truth (already passed in) ---\n","    gt = query_img_indices # (N_queries,)\n","\n","    # --- 5. Calculate R@K ---\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop\n","# ------------------------------------------------------\n","EPOCHS = 40\n","LR = 1e-4\n","SAVE_PATH = \"correct_hard_negative_best.pth\"\n","\n","# Validation set for monitoring\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique # Use ALL unique images as the database\n","\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining with CORRECT Hard-Negative Mining...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            # --- Forward pass ---\n","            y_pred = model(x_batch) # (B, 1536)\n","\n","            # --- 1. Contrastive Loss (aligns pairs) ---\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            # --- 2. CORRECT Hard-Negative Mining ---\n","            with torch.no_grad():\n","                # (B, B) matrix of pairwise similarities\n","                sims_no_tau = y_pred @ y_batch.T\n","\n","                # (B, B) mask where True means it's a POSITIVE pair\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","\n","                # Mask out ALL positives (not just diagonal)\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","\n","                # The argmax is now guaranteed to be a TRUE negative\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","\n","            # --- 3. Triplet Loss (now correct) ---\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            # --- 4. Combine losses ---\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        # --- Backward pass ---\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","          f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    # tx_test_t was already preprocessed\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_correct_hard_negative.csv\", index=False)\n","print(\"\\n✅ submission_correct_hard_negative.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q7GmIywyFWxG","executionInfo":{"status":"ok","timestamp":1761778756463,"user_tz":-60,"elapsed":116556,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"32fc2b3a-b311-4695-9b5f-bd68cb1eb980"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2102333727.py:161: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training with CORRECT Hard-Negative Mining...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/40:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-2102333727.py:173: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=1.8393 | R@1=0.1730  R@5=0.3746  R@10=0.4700  R@50=0.7056\n","✅ Best model saved (epoch 1, R@10=0.4700)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=1.6837 | R@1=0.1834  R@5=0.3866  R@10=0.4880  R@50=0.7256\n","✅ Best model saved (epoch 2, R@10=0.4880)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=1.6235 | R@1=0.1890  R@5=0.3900  R@10=0.4998  R@50=0.7356\n","✅ Best model saved (epoch 3, R@10=0.4998)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.5871 | R@1=0.1950  R@5=0.3972  R@10=0.5024  R@50=0.7396\n","✅ Best model saved (epoch 4, R@10=0.5024)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.5612 | R@1=0.1966  R@5=0.3986  R@10=0.5090  R@50=0.7448\n","✅ Best model saved (epoch 5, R@10=0.5090)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.5412 | R@1=0.1952  R@5=0.4036  R@10=0.5090  R@50=0.7472\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.5255 | R@1=0.1992  R@5=0.4074  R@10=0.5096  R@50=0.7504\n","✅ Best model saved (epoch 7, R@10=0.5096)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.5129 | R@1=0.1964  R@5=0.4036  R@10=0.5148  R@50=0.7506\n","✅ Best model saved (epoch 8, R@10=0.5148)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.5015 | R@1=0.1964  R@5=0.4078  R@10=0.5132  R@50=0.7508\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.4920 | R@1=0.1972  R@5=0.4106  R@10=0.5126  R@50=0.7488\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.4832 | R@1=0.1984  R@5=0.4110  R@10=0.5154  R@50=0.7494\n","✅ Best model saved (epoch 11, R@10=0.5154)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.4768 | R@1=0.1998  R@5=0.4150  R@10=0.5162  R@50=0.7514\n","✅ Best model saved (epoch 12, R@10=0.5162)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.4683 | R@1=0.1988  R@5=0.4126  R@10=0.5206  R@50=0.7518\n","✅ Best model saved (epoch 13, R@10=0.5206)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.4626 | R@1=0.1976  R@5=0.4118  R@10=0.5162  R@50=0.7520\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.4580 | R@1=0.2010  R@5=0.4140  R@10=0.5204  R@50=0.7552\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.4530 | R@1=0.1982  R@5=0.4170  R@10=0.5202  R@50=0.7584\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=1.4488 | R@1=0.1970  R@5=0.4142  R@10=0.5226  R@50=0.7546\n","✅ Best model saved (epoch 17, R@10=0.5226)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=1.4451 | R@1=0.1988  R@5=0.4128  R@10=0.5226  R@50=0.7562\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=1.4400 | R@1=0.1996  R@5=0.4174  R@10=0.5222  R@50=0.7558\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=1.4368 | R@1=0.2004  R@5=0.4156  R@10=0.5212  R@50=0.7542\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=1.4342 | R@1=0.1994  R@5=0.4164  R@10=0.5210  R@50=0.7564\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=1.4316 | R@1=0.2006  R@5=0.4180  R@10=0.5208  R@50=0.7590\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=1.4289 | R@1=0.2030  R@5=0.4170  R@10=0.5212  R@50=0.7554\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=1.4255 | R@1=0.1998  R@5=0.4172  R@10=0.5208  R@50=0.7580\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=1.4240 | R@1=0.2020  R@5=0.4168  R@10=0.5198  R@50=0.7578\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=1.4217 | R@1=0.1990  R@5=0.4170  R@10=0.5236  R@50=0.7584\n","✅ Best model saved (epoch 26, R@10=0.5236)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=1.4195 | R@1=0.1996  R@5=0.4186  R@10=0.5224  R@50=0.7598\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=1.4175 | R@1=0.1994  R@5=0.4168  R@10=0.5226  R@50=0.7580\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=1.4159 | R@1=0.1996  R@5=0.4190  R@10=0.5242  R@50=0.7594\n","✅ Best model saved (epoch 29, R@10=0.5242)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=1.4148 | R@1=0.1982  R@5=0.4178  R@10=0.5232  R@50=0.7596\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=1.4136 | R@1=0.1998  R@5=0.4194  R@10=0.5222  R@50=0.7590\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=1.4140 | R@1=0.1996  R@5=0.4190  R@10=0.5220  R@50=0.7580\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=1.4115 | R@1=0.1976  R@5=0.4196  R@10=0.5224  R@50=0.7594\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=1.4116 | R@1=0.1992  R@5=0.4194  R@10=0.5224  R@50=0.7594\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=1.4105 | R@1=0.1994  R@5=0.4198  R@10=0.5224  R@50=0.7590\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=1.4100 | R@1=0.1984  R@5=0.4194  R@10=0.5226  R@50=0.7590\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=1.4085 | R@1=0.1994  R@5=0.4194  R@10=0.5222  R@50=0.7582\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=1.4093 | R@1=0.1986  R@5=0.4186  R@10=0.5224  R@50=0.7584\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=1.4099 | R@1=0.1986  R@5=0.4188  R@10=0.5224  R@50=0.7586\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=1.4086 | R@1=0.1986  R@5=0.4188  R@10=0.5224  R@50=0.7584\n","\n","🎯 Best R@10 on validation = 0.5242\n","Model saved as correct_hard_negative_best.pth\n","Loading best model from correct_hard_negative_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_correct_hard_negative.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["# Tuning to previous code and enhancement"],"metadata":{"id":"6YpfFGcdQZu0"}},{"cell_type":"code","source":["# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. NEW MODEL: EnhancedResidualTranslator\n","#    (Deeper, more powerful residual block)\n","# ------------------------------------------------------\n","class EnhancedResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=2048, output_dim=1536, dropout=0.1):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.LayerNorm(hidden_dim),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.LayerNorm(hidden_dim),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Unchanged)\n","# ------------------------------------------------------\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\") # Use CPU for randperm\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0) # (N_queries, 1536)\n","\n","    sim = preds @ im_database.T # (N_queries, N_images)\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True) # (N_queries, 1)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T # (1, N_images)\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d # (N_queries, N_images)\n","\n","    gt = query_img_indices # (N_queries,)\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (Unchanged, but with new model)\n","# ------------------------------------------------------\n","EPOCHS = 40\n","LR = 1e-4\n","SAVE_PATH = \"enhanced_hard_negative_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize NEW model ---\n","model = EnhancedResidualTranslator(R.detach().clone(), hidden_dim=2048).to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining with Enhanced Model + Correct Hard-Negative Mining...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","          f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = EnhancedResidualTranslator(R.detach().clone(), hidden_dim=2048).to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_enhanced_hard_negative.csv\", index=False)\n","print(\"\\n✅ submission_enhanced_hard_negative.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qppb5LLaNCxs","executionInfo":{"status":"ok","timestamp":1761779040050,"user_tz":-60,"elapsed":155337,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"743e8d3b-bada-4071-d618-ba5ae60f0481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2585142698.py:151: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training with Enhanced Model + Correct Hard-Negative Mining...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/40:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-2585142698.py:163: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=2.1814 | R@1=0.1082  R@5=0.2682  R@10=0.3684  R@50=0.6446\n","✅ Best model saved (epoch 1, R@10=0.3684)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=1.7374 | R@1=0.1358  R@5=0.3156  R@10=0.4212  R@50=0.6828\n","✅ Best model saved (epoch 2, R@10=0.4212)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=1.5760 | R@1=0.1534  R@5=0.3452  R@10=0.4548  R@50=0.7062\n","✅ Best model saved (epoch 3, R@10=0.4548)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.4609 | R@1=0.1690  R@5=0.3628  R@10=0.4750  R@50=0.7256\n","✅ Best model saved (epoch 4, R@10=0.4750)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.3669 | R@1=0.1742  R@5=0.3856  R@10=0.4906  R@50=0.7392\n","✅ Best model saved (epoch 5, R@10=0.4906)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.2881 | R@1=0.1876  R@5=0.3962  R@10=0.5082  R@50=0.7496\n","✅ Best model saved (epoch 6, R@10=0.5082)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.2187 | R@1=0.1978  R@5=0.4060  R@10=0.5104  R@50=0.7612\n","✅ Best model saved (epoch 7, R@10=0.5104)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.1550 | R@1=0.2050  R@5=0.4184  R@10=0.5274  R@50=0.7686\n","✅ Best model saved (epoch 8, R@10=0.5274)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.0961 | R@1=0.2146  R@5=0.4224  R@10=0.5278  R@50=0.7692\n","✅ Best model saved (epoch 9, R@10=0.5278)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.0405 | R@1=0.2170  R@5=0.4334  R@10=0.5330  R@50=0.7752\n","✅ Best model saved (epoch 10, R@10=0.5330)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=0.9926 | R@1=0.2292  R@5=0.4370  R@10=0.5428  R@50=0.7726\n","✅ Best model saved (epoch 11, R@10=0.5428)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=0.9457 | R@1=0.2266  R@5=0.4416  R@10=0.5458  R@50=0.7780\n","✅ Best model saved (epoch 12, R@10=0.5458)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=0.9024 | R@1=0.2278  R@5=0.4456  R@10=0.5496  R@50=0.7824\n","✅ Best model saved (epoch 13, R@10=0.5496)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=0.8637 | R@1=0.2254  R@5=0.4432  R@10=0.5490  R@50=0.7774\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=0.8275 | R@1=0.2300  R@5=0.4460  R@10=0.5508  R@50=0.7786\n","✅ Best model saved (epoch 15, R@10=0.5508)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=0.7962 | R@1=0.2338  R@5=0.4524  R@10=0.5592  R@50=0.7782\n","✅ Best model saved (epoch 16, R@10=0.5592)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=0.7651 | R@1=0.2356  R@5=0.4516  R@10=0.5568  R@50=0.7802\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=0.7386 | R@1=0.2404  R@5=0.4576  R@10=0.5574  R@50=0.7784\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=0.7150 | R@1=0.2406  R@5=0.4572  R@10=0.5538  R@50=0.7784\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=0.6920 | R@1=0.2426  R@5=0.4558  R@10=0.5580  R@50=0.7758\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=0.6726 | R@1=0.2426  R@5=0.4556  R@10=0.5616  R@50=0.7782\n","✅ Best model saved (epoch 21, R@10=0.5616)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=0.6544 | R@1=0.2384  R@5=0.4560  R@10=0.5572  R@50=0.7748\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=0.6378 | R@1=0.2422  R@5=0.4568  R@10=0.5578  R@50=0.7742\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=0.6224 | R@1=0.2418  R@5=0.4590  R@10=0.5604  R@50=0.7758\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=0.6093 | R@1=0.2434  R@5=0.4552  R@10=0.5592  R@50=0.7702\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=0.5981 | R@1=0.2448  R@5=0.4538  R@10=0.5568  R@50=0.7694\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=0.5870 | R@1=0.2416  R@5=0.4548  R@10=0.5564  R@50=0.7676\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=0.5769 | R@1=0.2430  R@5=0.4554  R@10=0.5586  R@50=0.7712\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=0.5673 | R@1=0.2444  R@5=0.4580  R@10=0.5554  R@50=0.7688\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=0.5601 | R@1=0.2428  R@5=0.4602  R@10=0.5582  R@50=0.7678\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=0.5531 | R@1=0.2428  R@5=0.4584  R@10=0.5582  R@50=0.7630\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=0.5468 | R@1=0.2452  R@5=0.4564  R@10=0.5594  R@50=0.7666\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=0.5441 | R@1=0.2426  R@5=0.4538  R@10=0.5596  R@50=0.7672\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=0.5391 | R@1=0.2446  R@5=0.4574  R@10=0.5596  R@50=0.7652\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=0.5346 | R@1=0.2434  R@5=0.4568  R@10=0.5578  R@50=0.7652\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=0.5315 | R@1=0.2444  R@5=0.4560  R@10=0.5586  R@50=0.7650\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=0.5302 | R@1=0.2436  R@5=0.4558  R@10=0.5578  R@50=0.7648\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=0.5287 | R@1=0.2440  R@5=0.4564  R@10=0.5584  R@50=0.7642\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=0.5279 | R@1=0.2440  R@5=0.4560  R@10=0.5572  R@50=0.7646\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=0.5281 | R@1=0.2444  R@5=0.4558  R@10=0.5572  R@50=0.7646\n","\n","🎯 Best R@10 on validation = 0.5616\n","Model saved as enhanced_hard_negative_best.pth\n","Loading best model from enhanced_hard_negative_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_enhanced_hard_negative.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["# Patching Transformer"],"metadata":{"id":"EUTUhmFjPoXl"}},{"cell_type":"code","source":["# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. NEW MODEL: ResidualPatchingTransformer\n","# ------------------------------------------------------\n","class ResidualPatchingTransformer(nn.Module):\n","    def __init__(self, R_init,\n","                 input_dim=1024,\n","                 output_dim=1536,\n","                 patch_size=64,\n","                 embed_dim=256,\n","                 depth=3,\n","                 nhead=8,\n","                 dropout=0.1):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","\n","        assert input_dim % patch_size == 0, \"Input dim must be divisible by patch size\"\n","        num_patches = input_dim // patch_size # 1024 // 64 = 16 patches\n","\n","        # --- 1. Patching and Embedding ---\n","        # We use a single Linear layer to do both patching and embedding\n","        self.patch_embed = nn.Linear(input_dim, embed_dim * num_patches)\n","        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim) * 0.02)\n","\n","        self.num_patches = num_patches\n","        self.embed_dim = embed_dim\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # --- 2. Transformer Encoder ---\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,\n","            nhead=nhead,\n","            dim_feedforward=embed_dim * 4,\n","            dropout=dropout,\n","            activation='gelu',\n","            batch_first=True,\n","            norm_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n","\n","        # --- 3. Output Projection ---\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.output_proj = nn.Linear(embed_dim * num_patches, output_dim)\n","\n","    def forward(self, x):\n","        # 1. Base (Orthogonal) Path\n","        base = x @ self.R.T # (B, 1536)\n","\n","        # 2. Residual (Transformer) Path\n","\n","        # Patch and embed\n","        x_res = self.patch_embed(x) # (B, 1024) -> (B, 16*256)\n","        x_res = x_res.view(-1, self.num_patches, self.embed_dim) # (B, 16, 256)\n","\n","        # Add positional embedding\n","        x_res = x_res + self.pos_embed # (B, 16, 256)\n","        x_res = self.dropout(x_res)\n","\n","        # Pass through Transformer\n","        x_res = self.transformer(x_res) # (B, 16, 256)\n","        x_res = self.norm(x_res)\n","\n","        # Flatten and project to output\n","        x_res = x_res.flatten(start_dim=1) # (B, 16*256)\n","        res = self.output_proj(x_res) # (B, 1536)\n","\n","        # 3. Combine and Normalize\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Unchanged)\n","# ------------------------------------------------------\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\") # Use CPU for randperm\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0) # (N_queries, 1536)\n","\n","    sim = preds @ im_database.T # (N_queries, N_images)\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True) # (N_queries, 1)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T # (1, N_images)\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d # (N_queries, N_images)\n","\n","    gt = query_img_indices # (N_queries,)\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (Unchanged, but with new model)\n","# ------------------------------------------------------\n","EPOCHS = 40\n","LR = 1e-4\n","SAVE_PATH = \"patching_transformer_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize NEW model ---\n","model = ResidualPatchingTransformer(\n","    R_init=R.detach().clone(),\n","    patch_size=64,    # 1024 -> 16 patches\n","    embed_dim=256,    # Each patch becomes 256-dim\n","    depth=3,          # 3 Transformer layers\n","    nhead=8           # 8 attention heads\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining with Patching Transformer + Correct Hard-Negative Mining...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","\n","\n"," # ----- Validation -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","          f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = ResidualPatchingTransformer(\n","    R_init=R.detach().clone(),\n","    patch_size=64, embed_dim=256, depth=3, nhead=8\n",").to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_patching_transformer.csv\", index=False)\n","print(\"\\n✅ submission_patching_transformer.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lau4gotqPqN0","executionInfo":{"status":"ok","timestamp":1761780086210,"user_tz":-60,"elapsed":364476,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"705c22f4-7256-46e3-c4e3-e99919190a0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-670452080.py:200: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training with Patching Transformer + Correct Hard-Negative Mining...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/40:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-670452080.py:212: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=2.2878 | R@1=0.0802  R@5=0.2088  R@10=0.2994  R@50=0.5694\n","✅ Best model saved (epoch 1, R@10=0.2994)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=1.8557 | R@1=0.1014  R@5=0.2558  R@10=0.3558  R@50=0.6164\n","✅ Best model saved (epoch 2, R@10=0.3558)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=1.7173 | R@1=0.1112  R@5=0.2776  R@10=0.3772  R@50=0.6456\n","✅ Best model saved (epoch 3, R@10=0.3772)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.6227 | R@1=0.1192  R@5=0.2940  R@10=0.3926  R@50=0.6594\n","✅ Best model saved (epoch 4, R@10=0.3926)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.5503 | R@1=0.1262  R@5=0.3068  R@10=0.4102  R@50=0.6770\n","✅ Best model saved (epoch 5, R@10=0.4102)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.4843 | R@1=0.1294  R@5=0.3174  R@10=0.4208  R@50=0.6836\n","✅ Best model saved (epoch 6, R@10=0.4208)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.4300 | R@1=0.1380  R@5=0.3248  R@10=0.4350  R@50=0.6926\n","✅ Best model saved (epoch 7, R@10=0.4350)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.3769 | R@1=0.1392  R@5=0.3294  R@10=0.4412  R@50=0.7040\n","✅ Best model saved (epoch 8, R@10=0.4412)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.3276 | R@1=0.1426  R@5=0.3294  R@10=0.4394  R@50=0.7098\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.2791 | R@1=0.1482  R@5=0.3384  R@10=0.4460  R@50=0.7118\n","✅ Best model saved (epoch 10, R@10=0.4460)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.2332 | R@1=0.1496  R@5=0.3402  R@10=0.4452  R@50=0.7156\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.1908 | R@1=0.1530  R@5=0.3384  R@10=0.4532  R@50=0.7136\n","✅ Best model saved (epoch 12, R@10=0.4532)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.1490 | R@1=0.1484  R@5=0.3486  R@10=0.4572  R@50=0.7144\n","✅ Best model saved (epoch 13, R@10=0.4572)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.1073 | R@1=0.1492  R@5=0.3444  R@10=0.4530  R@50=0.7164\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.0697 | R@1=0.1538  R@5=0.3488  R@10=0.4622  R@50=0.7100\n","✅ Best model saved (epoch 15, R@10=0.4622)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.0320 | R@1=0.1514  R@5=0.3466  R@10=0.4578  R@50=0.7132\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=0.9992 | R@1=0.1586  R@5=0.3470  R@10=0.4564  R@50=0.7076\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=0.9693 | R@1=0.1556  R@5=0.3430  R@10=0.4544  R@50=0.7010\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=0.9385 | R@1=0.1546  R@5=0.3460  R@10=0.4520  R@50=0.7008\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=0.9106 | R@1=0.1534  R@5=0.3414  R@10=0.4516  R@50=0.7038\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=0.8867 | R@1=0.1564  R@5=0.3398  R@10=0.4464  R@50=0.6992\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=0.8628 | R@1=0.1536  R@5=0.3418  R@10=0.4470  R@50=0.7036\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=0.8406 | R@1=0.1554  R@5=0.3424  R@10=0.4426  R@50=0.6978\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=0.8215 | R@1=0.1532  R@5=0.3408  R@10=0.4490  R@50=0.6944\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=0.8037 | R@1=0.1558  R@5=0.3384  R@10=0.4488  R@50=0.6940\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=0.7872 | R@1=0.1568  R@5=0.3418  R@10=0.4448  R@50=0.6942\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=0.7745 | R@1=0.1574  R@5=0.3364  R@10=0.4476  R@50=0.6956\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=0.7593 | R@1=0.1556  R@5=0.3420  R@10=0.4416  R@50=0.6894\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=0.7486 | R@1=0.1564  R@5=0.3424  R@10=0.4394  R@50=0.6906\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=0.7385 | R@1=0.1548  R@5=0.3400  R@10=0.4440  R@50=0.6908\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=0.7281 | R@1=0.1532  R@5=0.3428  R@10=0.4424  R@50=0.6896\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=0.7210 | R@1=0.1540  R@5=0.3364  R@10=0.4426  R@50=0.6882\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=0.7153 | R@1=0.1538  R@5=0.3416  R@10=0.4394  R@50=0.6858\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=0.7080 | R@1=0.1516  R@5=0.3388  R@10=0.4400  R@50=0.6876\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=0.7048 | R@1=0.1538  R@5=0.3396  R@10=0.4398  R@50=0.6870\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=0.7007 | R@1=0.1546  R@5=0.3392  R@10=0.4404  R@50=0.6860\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=0.6981 | R@1=0.1536  R@5=0.3386  R@10=0.4396  R@50=0.6854\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=0.6946 | R@1=0.1532  R@5=0.3388  R@10=0.4400  R@50=0.6860\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=0.6947 | R@1=0.1546  R@5=0.3392  R@10=0.4404  R@50=0.6864\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=0.6928 | R@1=0.1546  R@5=0.3392  R@10=0.4402  R@50=0.6862\n","\n","🎯 Best R@10 on validation = 0.4622\n","Model saved as patching_transformer_best.pth\n","Loading best model from patching_transformer_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_patching_transformer.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["# Relative Anchor Loss"],"metadata":{"id":"TbD7vR1fSRSg"}},{"cell_type":"code","source":["\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. MODEL: EnhancedResidualTranslator\n","#    (Our best-performing model from the 0.5616 run)\n","# ------------------------------------------------------\n","class EnhancedResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=2048, output_dim=1536, dropout=0.1):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.LayerNorm(hidden_dim),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.LayerNorm(hidden_dim),\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Hard-Negative + NEW Relative Loss)\n","# ------------------------------------------------------\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_PRIMARY = 1.0     # (Contrastive + Triplet)\n","LOSS_WEIGHT_RELATIVE = 0.5    # NEW loss term\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","def relative_anchor_loss(x_batch, y_pred, anchors_x, anchors_y):\n","    \"\"\"\n","    Forces the relative similarities to anchors to be the same.\n","    x_batch: (B, 1024), y_pred: (B, 1536)\n","    anchors_x: (K, 1024), anchors_y: (K, 1536)\n","    \"\"\"\n","    # (B, K)\n","    sim_x_rel = x_batch @ anchors_x.T\n","    sim_y_rel = y_pred @ anchors_y.T\n","\n","    # Force the two similarity vectors to be as close as possible\n","    return F.mse_loss(sim_x_rel, sim_y_rel)\n","\n","# ------------------------------------------------------\n","# 6. Validation split & Anchor Selection\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\") # Use CPU for randperm\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# --- Create Anchors for the Relative Loss ---\n","# We use a fixed set of 1024 anchors from the validation set\n","NUM_ANCHORS = 1024\n","anchor_indices = torch.randperm(len(val_idx), device=\"cpu\")[:NUM_ANCHORS]\n","anchors_x = tx_val_t[anchor_indices].detach() # (K, 1024)\n","anchors_y = im_val_t[anchor_indices].detach() # (K, 1536)\n","print(f\"Created {NUM_ANCHORS} anchors for relative loss.\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0) # (N_queries, 1536)\n","\n","    sim = preds @ im_database.T # (N_queries, N_images)\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True) # (N_queries, 1)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T # (1, N_images)\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d # (N_queries, N_images)\n","\n","    gt = query_img_indices # (N_queries,)\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (with NEW loss)\n","# ------------------------------------------------------\n","EPOCHS = 50 # Train for longer\n","LR = 1e-4\n","SAVE_PATH = \"relative_anchor_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize NEW model ---\n","model = EnhancedResidualTranslator(R.detach().clone(), hidden_dim=2048).to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n","# Scheduler now runs for 50 epochs\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining with Enhanced Model + Relative Anchor Loss...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            # --- 1. Primary Loss (Hard-Negative) ---\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            loss_primary = (0.7 * loss_con) + (0.3 * loss_tri)\n","\n","            # --- 2. NEW Relative Anchor Loss ---\n","            loss_rel = relative_anchor_loss(x_batch, y_pred, anchors_x, anchors_y)\n","\n","            # --- 3. Combine ---\n","            loss = (LOSS_WEIGHT_PRIMARY * loss_primary) + (LOSS_WEIGHT_RELATIVE * loss_rel)\n","\n","        scaler.scale(loss).backward()\n","        # Gradient clipping\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","          f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = EnhancedResidualTranslator(R.detach().clone(), hidden_dim=2048).to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_relative_anchor_loss.csv\", index=False)\n","print(\"\\n✅ submission_relative_anchor_loss.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ywa3P1lASSIX","executionInfo":{"status":"ok","timestamp":1761780470914,"user_tz":-60,"elapsed":208398,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"7b951869-30e2-4e17-9cc9-78d15f2bd5c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-661048062.py:173: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","Created 1024 anchors for relative loss.\n","\n","Training with Enhanced Model + Relative Anchor Loss...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-661048062.py:185: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=2.5054 | R@1=0.0822  R@5=0.2190  R@10=0.2984  R@50=0.5540\n","✅ Best model saved (epoch 1, R@10=0.2984)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=1.9838 | R@1=0.1040  R@5=0.2552  R@10=0.3498  R@50=0.6086\n","✅ Best model saved (epoch 2, R@10=0.3498)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=1.8360 | R@1=0.1202  R@5=0.2782  R@10=0.3796  R@50=0.6468\n","✅ Best model saved (epoch 3, R@10=0.3796)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.7429 | R@1=0.1300  R@5=0.2996  R@10=0.4024  R@50=0.6642\n","✅ Best model saved (epoch 4, R@10=0.4024)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.6715 | R@1=0.1378  R@5=0.3082  R@10=0.4122  R@50=0.6800\n","✅ Best model saved (epoch 5, R@10=0.4122)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.6152 | R@1=0.1468  R@5=0.3184  R@10=0.4238  R@50=0.6944\n","✅ Best model saved (epoch 6, R@10=0.4238)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.5667 | R@1=0.1490  R@5=0.3306  R@10=0.4348  R@50=0.7044\n","✅ Best model saved (epoch 7, R@10=0.4348)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.5256 | R@1=0.1552  R@5=0.3416  R@10=0.4460  R@50=0.7118\n","✅ Best model saved (epoch 8, R@10=0.4460)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.4877 | R@1=0.1552  R@5=0.3474  R@10=0.4550  R@50=0.7188\n","✅ Best model saved (epoch 9, R@10=0.4550)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.4524 | R@1=0.1590  R@5=0.3522  R@10=0.4626  R@50=0.7240\n","✅ Best model saved (epoch 10, R@10=0.4626)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.4316 | R@1=0.1624  R@5=0.3594  R@10=0.4628  R@50=0.7264\n","✅ Best model saved (epoch 11, R@10=0.4628)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.4151 | R@1=0.1638  R@5=0.3628  R@10=0.4716  R@50=0.7280\n","✅ Best model saved (epoch 12, R@10=0.4716)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.3997 | R@1=0.1658  R@5=0.3616  R@10=0.4738  R@50=0.7310\n","✅ Best model saved (epoch 13, R@10=0.4738)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.3855 | R@1=0.1682  R@5=0.3650  R@10=0.4760  R@50=0.7336\n","✅ Best model saved (epoch 14, R@10=0.4760)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.3715 | R@1=0.1726  R@5=0.3706  R@10=0.4788  R@50=0.7342\n","✅ Best model saved (epoch 15, R@10=0.4788)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.3569 | R@1=0.1722  R@5=0.3724  R@10=0.4852  R@50=0.7374\n","✅ Best model saved (epoch 16, R@10=0.4852)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=1.3429 | R@1=0.1764  R@5=0.3760  R@10=0.4870  R@50=0.7390\n","✅ Best model saved (epoch 17, R@10=0.4870)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=1.3315 | R@1=0.1784  R@5=0.3782  R@10=0.4868  R@50=0.7390\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=1.3178 | R@1=0.1796  R@5=0.3774  R@10=0.4880  R@50=0.7450\n","✅ Best model saved (epoch 19, R@10=0.4880)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=1.3098 | R@1=0.1808  R@5=0.3796  R@10=0.4904  R@50=0.7436\n","✅ Best model saved (epoch 20, R@10=0.4904)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=1.3035 | R@1=0.1794  R@5=0.3790  R@10=0.4918  R@50=0.7446\n","✅ Best model saved (epoch 21, R@10=0.4918)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=1.2975 | R@1=0.1806  R@5=0.3786  R@10=0.4926  R@50=0.7444\n","✅ Best model saved (epoch 22, R@10=0.4926)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=1.2918 | R@1=0.1810  R@5=0.3810  R@10=0.4930  R@50=0.7440\n","✅ Best model saved (epoch 23, R@10=0.4930)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=1.2883 | R@1=0.1836  R@5=0.3832  R@10=0.4932  R@50=0.7452\n","✅ Best model saved (epoch 24, R@10=0.4932)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=1.2834 | R@1=0.1828  R@5=0.3836  R@10=0.4944  R@50=0.7472\n","✅ Best model saved (epoch 25, R@10=0.4944)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=1.2791 | R@1=0.1848  R@5=0.3844  R@10=0.4944  R@50=0.7474\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=1.2742 | R@1=0.1840  R@5=0.3848  R@10=0.4964  R@50=0.7476\n","✅ Best model saved (epoch 27, R@10=0.4964)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=1.2690 | R@1=0.1852  R@5=0.3864  R@10=0.4958  R@50=0.7464\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=1.2662 | R@1=0.1864  R@5=0.3874  R@10=0.4964  R@50=0.7482\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=1.2641 | R@1=0.1864  R@5=0.3888  R@10=0.4968  R@50=0.7480\n","✅ Best model saved (epoch 30, R@10=0.4968)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=1.2628 | R@1=0.1858  R@5=0.3880  R@10=0.4968  R@50=0.7482\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=1.2605 | R@1=0.1862  R@5=0.3882  R@10=0.4964  R@50=0.7462\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=1.2586 | R@1=0.1866  R@5=0.3878  R@10=0.4976  R@50=0.7478\n","✅ Best model saved (epoch 33, R@10=0.4976)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=1.2591 | R@1=0.1868  R@5=0.3892  R@10=0.4990  R@50=0.7488\n","✅ Best model saved (epoch 34, R@10=0.4990)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=1.2564 | R@1=0.1882  R@5=0.3886  R@10=0.4982  R@50=0.7490\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=1.2549 | R@1=0.1870  R@5=0.3888  R@10=0.4984  R@50=0.7494\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=1.2538 | R@1=0.1870  R@5=0.3882  R@10=0.4978  R@50=0.7494\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=1.2540 | R@1=0.1878  R@5=0.3880  R@10=0.4984  R@50=0.7496\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=1.2525 | R@1=0.1874  R@5=0.3882  R@10=0.4990  R@50=0.7492\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=1.2535 | R@1=0.1882  R@5=0.3888  R@10=0.4986  R@50=0.7502\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 41: loss=1.2516 | R@1=0.1872  R@5=0.3886  R@10=0.4990  R@50=0.7500\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 42: loss=1.2519 | R@1=0.1884  R@5=0.3890  R@10=0.4986  R@50=0.7498\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 43: loss=1.2501 | R@1=0.1882  R@5=0.3892  R@10=0.4984  R@50=0.7500\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 44: loss=1.2504 | R@1=0.1882  R@5=0.3890  R@10=0.4996  R@50=0.7502\n","✅ Best model saved (epoch 44, R@10=0.4996)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 45: loss=1.2510 | R@1=0.1884  R@5=0.3896  R@10=0.4998  R@50=0.7502\n","✅ Best model saved (epoch 45, R@10=0.4998)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 46: loss=1.2500 | R@1=0.1880  R@5=0.3894  R@10=0.4996  R@50=0.7502\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 47: loss=1.2499 | R@1=0.1880  R@5=0.3892  R@10=0.4998  R@50=0.7502\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 48: loss=1.2504 | R@1=0.1880  R@5=0.3892  R@10=0.4996  R@50=0.7504\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 49: loss=1.2497 | R@1=0.1878  R@5=0.3892  R@10=0.4996  R@50=0.7504\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 50: loss=1.2493 | R@1=0.1878  R@5=0.3892  R@10=0.4998  R@50=0.7504\n","\n","🎯 Best R@10 on validation = 0.4998\n","Model saved as relative_anchor_best.pth\n","Loading best model from relative_anchor_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_relative_anchor_loss.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["#  Lightweight Transformer\n"],"metadata":{"id":"Hdhwi5YUVT_P"}},{"cell_type":"code","source":["\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. NEW MODEL: LightweightRegularizedTransformer\n","# ------------------------------------------------------\n","class LightweightRegularizedTransformer(nn.Module):\n","    def __init__(self, R_init,\n","                 input_dim=1024,\n","                 output_dim=1536,\n","                 patch_size=64,\n","                 embed_dim=256,\n","                 depth=2,  # SHALLOW: Only 2 layers\n","                 nhead=4,  # FEWER HEADS: 4 instead of 8\n","                 dropout=0.2): # HIGHER DROPOUT\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","\n","        assert input_dim % patch_size == 0, \"Input dim must be divisible by patch size\"\n","        num_patches = input_dim // patch_size # 1024 // 64 = 16 patches\n","\n","        # --- 1. Patching and Embedding ---\n","        self.patch_embed = nn.Linear(input_dim, embed_dim * num_patches)\n","        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, embed_dim) * 0.02)\n","\n","        self.num_patches = num_patches\n","        self.embed_dim = embed_dim\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # --- 2. Transformer Encoder ---\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=embed_dim,\n","            nhead=nhead,\n","            dim_feedforward=embed_dim * 2, # Smaller FFN\n","            dropout=dropout,\n","            activation='gelu',\n","            batch_first=True,\n","            norm_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n","\n","        # --- 3. Output Projection ---\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.output_proj = nn.Linear(embed_dim * num_patches, output_dim)\n","\n","    def forward(self, x):\n","        # 1. Base (Orthogonal) Path\n","        base = x @ self.R.T # (B, 1536)\n","\n","        # 2. Residual (Transformer) Path\n","        x_res = self.patch_embed(x)\n","        x_res = x_res.view(-1, self.num_patches, self.embed_dim)\n","        x_res = x_res + self.pos_embed\n","        x_res = self.dropout(x_res)\n","        x_res = self.transformer(x_res)\n","        x_res = self.norm(x_res)\n","        x_res = x_res.flatten(start_dim=1)\n","        res = self.output_proj(x_res)\n","\n","        # 3. Combine and Normalize\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Our proven Hard-Negative setup)\n","# ------------------------------------------------------\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\")\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0)\n","\n","    sim = preds @ im_database.T\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d\n","\n","    gt = query_img_indices\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (with NEW model and HIGHER weight_decay)\n","# ------------------------------------------------------\n","EPOCHS = 40\n","LR = 1e-4\n","WEIGHT_DECAY = 1e-4 # STRONGER REGULARIZATION\n","SAVE_PATH = \"lightweight_transformer_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize NEW model ---\n","model = LightweightRegularizedTransformer(\n","    R_init=R.detach().clone()\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining with Lightweight Transformer (Regularized)...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient Clipping\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","          f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = LightweightRegularizedTransformer(\n","    R_init=R.detach().clone()\n",").to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024]\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_lightweight_transformer.csv\", index=False)\n","print(\"\\n✅ submission_lightweight_transformer.csv saved successfully.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsQxwUBGVU7o","executionInfo":{"status":"ok","timestamp":1761781336507,"user_tz":-60,"elapsed":269302,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"f7e9b715-27ce-4baf-a5b2-b12b50e30269"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2089763401.py:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training with Lightweight Transformer (Regularized)...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/40:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-2089763401.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=2.8066 | R@1=0.0436  R@5=0.1308  R@10=0.1994  R@50=0.4430\n","✅ Best model saved (epoch 1, R@10=0.1994)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=2.2459 | R@1=0.0628  R@5=0.1722  R@10=0.2524  R@50=0.5132\n","✅ Best model saved (epoch 2, R@10=0.2524)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=2.0901 | R@1=0.0742  R@5=0.1990  R@10=0.2838  R@50=0.5454\n","✅ Best model saved (epoch 3, R@10=0.2838)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.9960 | R@1=0.0816  R@5=0.2178  R@10=0.3022  R@50=0.5664\n","✅ Best model saved (epoch 4, R@10=0.3022)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.9285 | R@1=0.0862  R@5=0.2332  R@10=0.3224  R@50=0.5892\n","✅ Best model saved (epoch 5, R@10=0.3224)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.8742 | R@1=0.0914  R@5=0.2388  R@10=0.3294  R@50=0.6040\n","✅ Best model saved (epoch 6, R@10=0.3294)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.8277 | R@1=0.0952  R@5=0.2460  R@10=0.3378  R@50=0.6148\n","✅ Best model saved (epoch 7, R@10=0.3378)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.7932 | R@1=0.0968  R@5=0.2532  R@10=0.3472  R@50=0.6276\n","✅ Best model saved (epoch 8, R@10=0.3472)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.7592 | R@1=0.0982  R@5=0.2590  R@10=0.3516  R@50=0.6340\n","✅ Best model saved (epoch 9, R@10=0.3516)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.7292 | R@1=0.0994  R@5=0.2614  R@10=0.3586  R@50=0.6396\n","✅ Best model saved (epoch 10, R@10=0.3586)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.7155 | R@1=0.1028  R@5=0.2672  R@10=0.3628  R@50=0.6434\n","✅ Best model saved (epoch 11, R@10=0.3628)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.7037 | R@1=0.1046  R@5=0.2688  R@10=0.3628  R@50=0.6490\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.6904 | R@1=0.1022  R@5=0.2690  R@10=0.3686  R@50=0.6510\n","✅ Best model saved (epoch 13, R@10=0.3686)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.6787 | R@1=0.1058  R@5=0.2718  R@10=0.3700  R@50=0.6504\n","✅ Best model saved (epoch 14, R@10=0.3700)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.6672 | R@1=0.1040  R@5=0.2778  R@10=0.3742  R@50=0.6548\n","✅ Best model saved (epoch 15, R@10=0.3742)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.6582 | R@1=0.1088  R@5=0.2762  R@10=0.3748  R@50=0.6564\n","✅ Best model saved (epoch 16, R@10=0.3748)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=1.6472 | R@1=0.1068  R@5=0.2782  R@10=0.3746  R@50=0.6578\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=1.6387 | R@1=0.1112  R@5=0.2780  R@10=0.3788  R@50=0.6604\n","✅ Best model saved (epoch 18, R@10=0.3788)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=1.6309 | R@1=0.1092  R@5=0.2806  R@10=0.3824  R@50=0.6626\n","✅ Best model saved (epoch 19, R@10=0.3824)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=1.6247 | R@1=0.1082  R@5=0.2814  R@10=0.3798  R@50=0.6656\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=1.6210 | R@1=0.1110  R@5=0.2782  R@10=0.3816  R@50=0.6648\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=1.6188 | R@1=0.1104  R@5=0.2800  R@10=0.3838  R@50=0.6660\n","✅ Best model saved (epoch 22, R@10=0.3838)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=1.6128 | R@1=0.1100  R@5=0.2820  R@10=0.3830  R@50=0.6636\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=1.6102 | R@1=0.1104  R@5=0.2814  R@10=0.3856  R@50=0.6668\n","✅ Best model saved (epoch 24, R@10=0.3856)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=1.6084 | R@1=0.1100  R@5=0.2818  R@10=0.3864  R@50=0.6670\n","✅ Best model saved (epoch 25, R@10=0.3864)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=1.6059 | R@1=0.1106  R@5=0.2822  R@10=0.3860  R@50=0.6678\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=1.6054 | R@1=0.1108  R@5=0.2852  R@10=0.3896  R@50=0.6666\n","✅ Best model saved (epoch 27, R@10=0.3896)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=1.6004 | R@1=0.1100  R@5=0.2828  R@10=0.3886  R@50=0.6668\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=1.5995 | R@1=0.1098  R@5=0.2830  R@10=0.3880  R@50=0.6686\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=1.5981 | R@1=0.1106  R@5=0.2840  R@10=0.3896  R@50=0.6680\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=1.5975 | R@1=0.1100  R@5=0.2840  R@10=0.3886  R@50=0.6682\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=1.5980 | R@1=0.1104  R@5=0.2848  R@10=0.3900  R@50=0.6676\n","✅ Best model saved (epoch 32, R@10=0.3900)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=1.5972 | R@1=0.1104  R@5=0.2848  R@10=0.3898  R@50=0.6680\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=1.5972 | R@1=0.1106  R@5=0.2836  R@10=0.3900  R@50=0.6680\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=1.5967 | R@1=0.1100  R@5=0.2842  R@10=0.3890  R@50=0.6682\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=1.5962 | R@1=0.1106  R@5=0.2846  R@10=0.3898  R@50=0.6680\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=1.5952 | R@1=0.1104  R@5=0.2846  R@10=0.3900  R@50=0.6676\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=1.5963 | R@1=0.1104  R@5=0.2852  R@10=0.3902  R@50=0.6678\n","✅ Best model saved (epoch 38, R@10=0.3902)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=1.5944 | R@1=0.1104  R@5=0.2854  R@10=0.3898  R@50=0.6678\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=1.5952 | R@1=0.1104  R@5=0.2852  R@10=0.3898  R@50=0.6678\n","\n","🎯 Best R@10 on validation = 0.3902\n","Model saved as lightweight_transformer_best.pth\n","Loading best model from lightweight_transformer_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_lightweight_transformer.csv saved successfully.\n"]}]},{"cell_type":"code","source":["Epoch 50: loss=1.2493 | R@1=0.1878  R@5=0.3892  R@10=0.4998  R@50=0.7504"],"metadata":{"id":"2bqu8CTLVs57"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pure Contrasitive Loss"],"metadata":{"id":"_hLRL_TPXVqE"}},{"cell_type":"markdown","source":["#"],"metadata":{"id":"QTzEx2z1XSME"}},{"cell_type":"code","source":["# ======================================================\n","#  AML Challenge — Residual-Orthogonal Translator v5\n","#  Fine-tuned version of pure-contrastive model\n","#  Adds: 0.1 Triplet term + tau=0.065 + CSLS inference\n","# ======================================================\n","\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np, pandas as pd\n","import os, math\n","from google.colab import drive\n","\n","# ------------------------------------------------------\n","# 0. Setup Google Drive\n","# ------------------------------------------------------\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive', force_remount=True)\n","\n","BASE_DIR = \"/content/drive/MyDrive/AML Challenge\"\n","os.makedirs(BASE_DIR, exist_ok=True)\n","os.chdir(BASE_DIR)\n","print(f\"Current working directory: {os.getcwd()}\")\n","\n","# ------------------------------------------------------\n","# 1. Device + Data Loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","\n","# ------------------------------------------------------\n","# 2. Preprocessing\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test, dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. Model\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res  = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss functions\n","# ------------------------------------------------------\n","TAU = 0.065\n","\n","def pure_contrastive_loss(y_pred, y_batch, img_indices):\n","    sims = y_pred @ y_batch.T / TAU\n","    positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0)).float()\n","    log_probs = F.log_softmax(sims, dim=1) * positive_mask\n","    num_pos = torch.clamp(positive_mask.sum(1), min=1.0)\n","    loss = -log_probs.sum(1) / num_pos\n","    return loss.mean()\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.15\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\")\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, ks=(1,5,10,50), k_csls=10):\n","    model.eval()\n","    preds = []\n","    for i in range(0, len(tx_queries), 1024):\n","        preds.append(model(tx_queries[i:i+1024].to(device)))\n","    preds = torch.cat(preds, dim=0)\n","    im_database = im_database.to(device)\n","    sim = preds @ im_database.T\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values.mean(1, keepdim=True)\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values.mean(1, keepdim=True).T\n","    csls_sim = 2 * sim - knn_q - knn_d\n","\n","    gt = query_img_indices.to(device)\n","    ranks = torch.argsort(csls_sim, dim=1, descending=True)\n","    recalls = {}\n","    for k in ks:\n","        match = (ranks[:, :k] == gt.unsqueeze(1)).any(dim=1)\n","        recalls[f\"R@{k}\"] = match.float().mean().item()\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training\n","# ------------------------------------------------------\n","EPOCHS = 45\n","LR = 1e-4\n","SAVE_PATH = \"residual_v5_best.pth\"\n","\n","val_subset_queries = tx_val_t[:5000]\n","val_subset_indices = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=5e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler()\n","\n","best_r10 = 0.0\n","print(\"\\nTraining Residual-Orthogonal Translator v5 (Contrastive + small Triplet)...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","    for x_batch, y_batch, img_idx in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","            loss_con = pure_contrastive_loss(y_pred, y_batch, img_idx)\n","            # create negatives by random permutation\n","            neg_idx = torch.randperm(y_batch.size(0), device=device)\n","            y_neg = y_batch[neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_neg)\n","            loss = 0.9 * loss_con + 0.1 * loss_tri\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","    scheduler.step()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    rec = recall_at_k(model, val_subset_queries, val_db_subset, val_subset_indices)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f} R@5={rec['R@5']:.4f} R@10={rec['R@10']:.4f} R@50={rec['R@50']:.4f}\")\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model updated (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best validation R@10 = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + CSLS Submission\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def apply_csls(preds, im_base, k=10):\n","    preds = F.normalize(torch.tensor(preds, device=device), p=2, dim=1)\n","    im_base = F.normalize(im_base, p=2, dim=1)\n","    sim = preds @ im_base.T\n","    knn_q = torch.topk(sim, k=k, dim=1).values.mean(1, keepdim=True)\n","    knn_d = torch.topk(sim, k=k, dim=0).values.mean(0, keepdim=True)\n","    return (2 * sim - knn_q - knn_d).cpu().numpy()\n","\n","print(f\"\\nLoading {SAVE_PATH} for inference...\")\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_all = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        preds_all.append(model(tx_test_t[i:i+1024].to(device)))\n","    preds = torch.cat(preds_all, dim=0).cpu().numpy()\n","\n","csls_sim = apply_csls(preds, im_train_t_unique)\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_residual_v5.csv\", index=False)\n","print(\"\\n✅ submission_residual_v5.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ETVcG7U9XTer","executionInfo":{"status":"ok","timestamp":1761782161943,"user_tz":-60,"elapsed":134633,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"d4b0dd0e-5ab8-4553-9b2c-a37559089e56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function NpzFile.__del__ at 0x7e43f5d17a60>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\", line 226, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\", line 221, in close\n","    self.fid.close()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Exception ignored in: <function NpzFile.__del__ at 0x7e43f5d17a60>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\", line 226, in __del__\n","    self.close()\n","  File \"/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\", line 221, in close\n","    self.fid.close()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]},{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Current working directory: /content/drive/MyDrive/AML Challenge\n","Using device: cuda\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-853231009.py:161: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training Residual-Orthogonal Translator v5 (Contrastive + small Triplet)...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/45:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-853231009.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=2.2875 | R@1=0.1460 R@5=0.3354 R@10=0.4256 R@50=0.6770\n","✅ Best model updated (epoch 1, R@10=0.4256)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=2.1124 | R@1=0.1636 R@5=0.3568 R@10=0.4550 R@50=0.7050\n","✅ Best model updated (epoch 2, R@10=0.4550)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=2.0351 | R@1=0.1726 R@5=0.3710 R@10=0.4720 R@50=0.7152\n","✅ Best model updated (epoch 3, R@10=0.4720)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.9873 | R@1=0.1790 R@5=0.3778 R@10=0.4794 R@50=0.7256\n","✅ Best model updated (epoch 4, R@10=0.4794)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.9501 | R@1=0.1804 R@5=0.3828 R@10=0.4866 R@50=0.7298\n","✅ Best model updated (epoch 5, R@10=0.4866)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.9216 | R@1=0.1864 R@5=0.3880 R@10=0.4902 R@50=0.7362\n","✅ Best model updated (epoch 6, R@10=0.4902)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.8984 | R@1=0.1880 R@5=0.3902 R@10=0.4982 R@50=0.7376\n","✅ Best model updated (epoch 7, R@10=0.4982)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.8804 | R@1=0.1886 R@5=0.3972 R@10=0.4990 R@50=0.7408\n","✅ Best model updated (epoch 8, R@10=0.4990)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.8635 | R@1=0.1874 R@5=0.3950 R@10=0.5020 R@50=0.7430\n","✅ Best model updated (epoch 9, R@10=0.5020)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.8486 | R@1=0.1894 R@5=0.3990 R@10=0.5056 R@50=0.7414\n","✅ Best model updated (epoch 10, R@10=0.5056)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.8417 | R@1=0.1894 R@5=0.4006 R@10=0.5054 R@50=0.7428\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.8351 | R@1=0.1910 R@5=0.4032 R@10=0.5076 R@50=0.7446\n","✅ Best model updated (epoch 12, R@10=0.5076)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.8295 | R@1=0.1928 R@5=0.4024 R@10=0.5082 R@50=0.7436\n","✅ Best model updated (epoch 13, R@10=0.5082)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.8239 | R@1=0.1916 R@5=0.4022 R@10=0.5076 R@50=0.7446\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.8189 | R@1=0.1922 R@5=0.4034 R@10=0.5084 R@50=0.7448\n","✅ Best model updated (epoch 15, R@10=0.5084)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.8134 | R@1=0.1932 R@5=0.4024 R@10=0.5094 R@50=0.7448\n","✅ Best model updated (epoch 16, R@10=0.5094)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=1.8083 | R@1=0.1926 R@5=0.4038 R@10=0.5072 R@50=0.7466\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=1.8045 | R@1=0.1922 R@5=0.4066 R@10=0.5102 R@50=0.7452\n","✅ Best model updated (epoch 18, R@10=0.5102)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=1.7993 | R@1=0.1924 R@5=0.4064 R@10=0.5098 R@50=0.7444\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=1.7968 | R@1=0.1936 R@5=0.4072 R@10=0.5104 R@50=0.7444\n","✅ Best model updated (epoch 20, R@10=0.5104)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=1.7953 | R@1=0.1950 R@5=0.4090 R@10=0.5104 R@50=0.7456\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=1.7937 | R@1=0.1942 R@5=0.4080 R@10=0.5096 R@50=0.7460\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=1.7919 | R@1=0.1952 R@5=0.4098 R@10=0.5116 R@50=0.7470\n","✅ Best model updated (epoch 23, R@10=0.5116)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=1.7898 | R@1=0.1952 R@5=0.4102 R@10=0.5120 R@50=0.7462\n","✅ Best model updated (epoch 24, R@10=0.5120)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=1.7887 | R@1=0.1952 R@5=0.4098 R@10=0.5126 R@50=0.7466\n","✅ Best model updated (epoch 25, R@10=0.5126)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=1.7869 | R@1=0.1966 R@5=0.4102 R@10=0.5132 R@50=0.7462\n","✅ Best model updated (epoch 26, R@10=0.5132)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=1.7845 | R@1=0.1952 R@5=0.4106 R@10=0.5132 R@50=0.7464\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=1.7844 | R@1=0.1954 R@5=0.4108 R@10=0.5126 R@50=0.7472\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=1.7832 | R@1=0.1958 R@5=0.4106 R@10=0.5122 R@50=0.7470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=1.7832 | R@1=0.1960 R@5=0.4112 R@10=0.5138 R@50=0.7466\n","✅ Best model updated (epoch 30, R@10=0.5138)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=1.7828 | R@1=0.1960 R@5=0.4112 R@10=0.5128 R@50=0.7470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=1.7817 | R@1=0.1966 R@5=0.4114 R@10=0.5128 R@50=0.7468\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=1.7812 | R@1=0.1958 R@5=0.4114 R@10=0.5138 R@50=0.7468\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=1.7810 | R@1=0.1966 R@5=0.4116 R@10=0.5132 R@50=0.7466\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=1.7807 | R@1=0.1966 R@5=0.4118 R@10=0.5134 R@50=0.7468\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=1.7799 | R@1=0.1966 R@5=0.4110 R@10=0.5134 R@50=0.7470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=1.7804 | R@1=0.1968 R@5=0.4116 R@10=0.5136 R@50=0.7466\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=1.7803 | R@1=0.1968 R@5=0.4118 R@10=0.5140 R@50=0.7470\n","✅ Best model updated (epoch 38, R@10=0.5140)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=1.7808 | R@1=0.1960 R@5=0.4112 R@10=0.5138 R@50=0.7472\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=1.7803 | R@1=0.1966 R@5=0.4116 R@10=0.5140 R@50=0.7474\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 41: loss=1.7804 | R@1=0.1968 R@5=0.4116 R@10=0.5144 R@50=0.7468\n","✅ Best model updated (epoch 41, R@10=0.5144)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 42: loss=1.7806 | R@1=0.1966 R@5=0.4116 R@10=0.5144 R@50=0.7470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 43: loss=1.7801 | R@1=0.1968 R@5=0.4116 R@10=0.5144 R@50=0.7470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 44: loss=1.7804 | R@1=0.1966 R@5=0.4116 R@10=0.5144 R@50=0.7470\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 45: loss=1.7789 | R@1=0.1966 R@5=0.4116 R@10=0.5144 R@50=0.7470\n","\n","🎯 Best validation R@10 = 0.5144\n","Model saved as residual_v5_best.pth\n","\n","Loading residual_v5_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_residual_v5.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["# 120 Epoches"],"metadata":{"id":"lVGaM498ZMj-"}},{"cell_type":"code","source":["\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. MODEL: ResidualTranslator (Slightly Enhanced)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    # Our 0.815 winner used hidden_dim=1024\n","    # Our 0.796 loser used hidden_dim=2048\n","    # Let's try the middle ground: 1536\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1536, output_dim=1536, dropout=0.1):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.LayerNorm(hidden_dim), # Add LayerNorm for stability\n","            nn.Dropout(dropout),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Our proven Hard-Negative setup)\n","# ------------------------------------------------------\n","TAU = 0.07 # Temperature for contrastive loss\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\")\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024].to(device) # Ensure chunk is on device\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0)\n","\n","    im_database = im_database.to(device) # Ensure database is on device\n","    query_img_indices = query_img_indices.to(device) # Ensure indices are on device\n","\n","    sim = preds @ im_database.T\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d\n","\n","    gt = query_img_indices\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (LONG HAUL)\n","# ------------------------------------------------------\n","EPOCHS = 120 # Train for much longer\n","LR = 1e-4\n","WEIGHT_DECAY = 5e-5 # Standard regularization\n","SAVE_PATH = \"final_push_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize model with modest capacity boost ---\n","model = ResidualTranslator(\n","    R_init=R.detach().clone(),\n","    hidden_dim=1536 # 1024 was good, 2048 was bad. Let's try 1536.\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","# Schedule the LR over the *entire* long run\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining Final Push Model (120 Epochs, hidden_dim=1536)...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            # --- Our proven winning loss combo ---\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient Clipping\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    # Only validate every 2 epochs to save time\n","    if epoch % 2 == 0 or epoch == EPOCHS:\n","        rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","        print(f\"Epoch {epoch:03d}: loss={avg_loss:.4f} | \"\n","              f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","              f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","        if rec['R@10'] > best_r10:\n","            best_r10 = rec['R@10']\n","            torch.save(model.state_dict(), SAVE_PATH)\n","            print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = ResidualTranslator(\n","    R_init=R.detach().clone(),\n","    hidden_dim=1536\n",").to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024].to(device) # Ensure chunk is on device\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_final_push.csv\", index=False)\n","print(\"\\n✅ submission_final_push.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4o6kIHj0ZPV9","executionInfo":{"status":"ok","timestamp":1761782546857,"user_tz":-60,"elapsed":343692,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"bef055b2-dee4-40f5-c96e-43e95fc58a47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1287973950.py:157: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training Final Push Model (120 Epochs, hidden_dim=1536)...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/120:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-1287973950.py:169: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 002: loss=2.0280 | R@1=0.1054  R@5=0.2632  R@10=0.3570  R@50=0.6082\n","✅ Best model saved (epoch 2, R@10=0.3570)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 004: loss=1.8083 | R@1=0.1284  R@5=0.3036  R@10=0.3938  R@50=0.6558\n","✅ Best model saved (epoch 4, R@10=0.3938)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 006: loss=1.7141 | R@1=0.1400  R@5=0.3240  R@10=0.4200  R@50=0.6780\n","✅ Best model saved (epoch 6, R@10=0.4200)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 008: loss=1.6553 | R@1=0.1430  R@5=0.3342  R@10=0.4358  R@50=0.6876\n","✅ Best model saved (epoch 8, R@10=0.4358)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 010: loss=1.6122 | R@1=0.1464  R@5=0.3410  R@10=0.4412  R@50=0.6964\n","✅ Best model saved (epoch 10, R@10=0.4412)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 012: loss=1.5923 | R@1=0.1502  R@5=0.3416  R@10=0.4466  R@50=0.7006\n","✅ Best model saved (epoch 12, R@10=0.4466)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 014: loss=1.5759 | R@1=0.1510  R@5=0.3516  R@10=0.4496  R@50=0.7026\n","✅ Best model saved (epoch 14, R@10=0.4496)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 016: loss=1.5610 | R@1=0.1522  R@5=0.3514  R@10=0.4542  R@50=0.7060\n","✅ Best model saved (epoch 16, R@10=0.4542)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 018: loss=1.5482 | R@1=0.1556  R@5=0.3532  R@10=0.4582  R@50=0.7090\n","✅ Best model saved (epoch 18, R@10=0.4582)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 020: loss=1.5349 | R@1=0.1558  R@5=0.3530  R@10=0.4594  R@50=0.7100\n","✅ Best model saved (epoch 20, R@10=0.4594)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 022: loss=1.5298 | R@1=0.1566  R@5=0.3544  R@10=0.4614  R@50=0.7114\n","✅ Best model saved (epoch 22, R@10=0.4614)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 024: loss=1.5233 | R@1=0.1576  R@5=0.3578  R@10=0.4632  R@50=0.7122\n","✅ Best model saved (epoch 24, R@10=0.4632)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 026: loss=1.5192 | R@1=0.1586  R@5=0.3550  R@10=0.4660  R@50=0.7122\n","✅ Best model saved (epoch 26, R@10=0.4660)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 028: loss=1.5122 | R@1=0.1578  R@5=0.3590  R@10=0.4672  R@50=0.7140\n","✅ Best model saved (epoch 28, R@10=0.4672)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 030: loss=1.5081 | R@1=0.1586  R@5=0.3590  R@10=0.4682  R@50=0.7152\n","✅ Best model saved (epoch 30, R@10=0.4682)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 032: loss=1.5060 | R@1=0.1590  R@5=0.3580  R@10=0.4682  R@50=0.7154\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 034: loss=1.5027 | R@1=0.1586  R@5=0.3598  R@10=0.4684  R@50=0.7154\n","✅ Best model saved (epoch 34, R@10=0.4684)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 036: loss=1.5005 | R@1=0.1604  R@5=0.3602  R@10=0.4670  R@50=0.7156\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 038: loss=1.4987 | R@1=0.1604  R@5=0.3640  R@10=0.4700  R@50=0.7150\n","✅ Best model saved (epoch 38, R@10=0.4700)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 040: loss=1.4968 | R@1=0.1602  R@5=0.3626  R@10=0.4682  R@50=0.7170\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 042: loss=1.4941 | R@1=0.1612  R@5=0.3632  R@10=0.4694  R@50=0.7174\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 044: loss=1.4915 | R@1=0.1610  R@5=0.3616  R@10=0.4702  R@50=0.7166\n","✅ Best model saved (epoch 44, R@10=0.4702)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 046: loss=1.4906 | R@1=0.1626  R@5=0.3624  R@10=0.4702  R@50=0.7174\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 048: loss=1.4876 | R@1=0.1630  R@5=0.3632  R@10=0.4700  R@50=0.7168\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 050: loss=1.4866 | R@1=0.1624  R@5=0.3646  R@10=0.4710  R@50=0.7162\n","✅ Best model saved (epoch 50, R@10=0.4710)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 052: loss=1.4846 | R@1=0.1624  R@5=0.3632  R@10=0.4706  R@50=0.7182\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 054: loss=1.4845 | R@1=0.1646  R@5=0.3628  R@10=0.4702  R@50=0.7194\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 056: loss=1.4823 | R@1=0.1640  R@5=0.3636  R@10=0.4724  R@50=0.7182\n","✅ Best model saved (epoch 56, R@10=0.4724)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 058: loss=1.4804 | R@1=0.1638  R@5=0.3638  R@10=0.4726  R@50=0.7188\n","✅ Best model saved (epoch 58, R@10=0.4726)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 060: loss=1.4787 | R@1=0.1638  R@5=0.3644  R@10=0.4718  R@50=0.7194\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 062: loss=1.4770 | R@1=0.1642  R@5=0.3658  R@10=0.4714  R@50=0.7210\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 064: loss=1.4758 | R@1=0.1648  R@5=0.3652  R@10=0.4732  R@50=0.7198\n","✅ Best model saved (epoch 64, R@10=0.4732)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 066: loss=1.4752 | R@1=0.1650  R@5=0.3658  R@10=0.4720  R@50=0.7198\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 068: loss=1.4744 | R@1=0.1652  R@5=0.3664  R@10=0.4732  R@50=0.7200\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 070: loss=1.4722 | R@1=0.1648  R@5=0.3664  R@10=0.4736  R@50=0.7196\n","✅ Best model saved (epoch 70, R@10=0.4736)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 072: loss=1.4720 | R@1=0.1644  R@5=0.3662  R@10=0.4740  R@50=0.7210\n","✅ Best model saved (epoch 72, R@10=0.4740)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 074: loss=1.4706 | R@1=0.1652  R@5=0.3656  R@10=0.4746  R@50=0.7208\n","✅ Best model saved (epoch 74, R@10=0.4746)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 076: loss=1.4698 | R@1=0.1646  R@5=0.3666  R@10=0.4738  R@50=0.7204\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 078: loss=1.4696 | R@1=0.1654  R@5=0.3680  R@10=0.4732  R@50=0.7206\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 080: loss=1.4685 | R@1=0.1670  R@5=0.3666  R@10=0.4744  R@50=0.7206\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 082: loss=1.4681 | R@1=0.1656  R@5=0.3678  R@10=0.4736  R@50=0.7218\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 084: loss=1.4677 | R@1=0.1660  R@5=0.3672  R@10=0.4736  R@50=0.7206\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 086: loss=1.4670 | R@1=0.1666  R@5=0.3682  R@10=0.4746  R@50=0.7212\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 088: loss=1.4664 | R@1=0.1672  R@5=0.3672  R@10=0.4748  R@50=0.7204\n","✅ Best model saved (epoch 88, R@10=0.4748)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 090: loss=1.4660 | R@1=0.1666  R@5=0.3672  R@10=0.4742  R@50=0.7204\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 092: loss=1.4663 | R@1=0.1660  R@5=0.3680  R@10=0.4740  R@50=0.7208\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 094: loss=1.4667 | R@1=0.1666  R@5=0.3688  R@10=0.4742  R@50=0.7214\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 096: loss=1.4656 | R@1=0.1668  R@5=0.3690  R@10=0.4750  R@50=0.7208\n","✅ Best model saved (epoch 96, R@10=0.4750)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 098: loss=1.4652 | R@1=0.1666  R@5=0.3690  R@10=0.4744  R@50=0.7210\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 100: loss=1.4647 | R@1=0.1666  R@5=0.3684  R@10=0.4746  R@50=0.7210\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 102: loss=1.4650 | R@1=0.1662  R@5=0.3684  R@10=0.4748  R@50=0.7212\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 104: loss=1.4639 | R@1=0.1662  R@5=0.3686  R@10=0.4754  R@50=0.7212\n","✅ Best model saved (epoch 104, R@10=0.4754)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 106: loss=1.4641 | R@1=0.1666  R@5=0.3686  R@10=0.4754  R@50=0.7206\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 108: loss=1.4646 | R@1=0.1662  R@5=0.3688  R@10=0.4744  R@50=0.7210\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 110: loss=1.4637 | R@1=0.1664  R@5=0.3694  R@10=0.4748  R@50=0.7210\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 112: loss=1.4644 | R@1=0.1664  R@5=0.3692  R@10=0.4746  R@50=0.7212\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 114: loss=1.4635 | R@1=0.1664  R@5=0.3692  R@10=0.4746  R@50=0.7214\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 116: loss=1.4647 | R@1=0.1664  R@5=0.3692  R@10=0.4748  R@50=0.7212\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 118: loss=1.4643 | R@1=0.1664  R@5=0.3692  R@10=0.4746  R@50=0.7212\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 120: loss=1.4635 | R@1=0.1664  R@5=0.3692  R@10=0.4748  R@50=0.7212\n","\n","🎯 Best R@10 on validation = 0.4754\n","Model saved as final_push_best.pth\n","Loading best model from final_push_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_final_push.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["# Fine TUning the original Residual model\n","# Highest score (0.82388) as per 10/30/2025"],"metadata":{"id":"vqYKt2VFbKk_"}},{"cell_type":"code","source":["# ----------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. MODEL: ResidualTranslator (Our 0.815 Kaggle Winner)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Proven Hard-Negative, Tuned Hyperparams)\n","# ------------------------------------------------------\n","# --- HYPERPARAMETER TUNE ---\n","TAU = 0.05       # Lower temperature (was 0.07) -> Stricter contrastive loss\n","MARGIN = 0.25    # Higher margin (was 0.2) -> Stricter triplet loss\n","# -------------------------\n","\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=MARGIN\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\")\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024].to(device) # Ensure chunk is on device\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0)\n","\n","    im_database = im_database.to(device) # Ensure database is on device\n","    query_img_indices = query_img_indices.to(device) # Ensure indices are on device\n","\n","    sim = preds @ im_database.T\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d\n","\n","    gt = query_img_indices\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (Original 40 epochs)\n","# ------------------------------------------------------\n","EPOCHS = 40 # Back to our 0.815 winner's epoch count\n","LR = 1e-4\n","WEIGHT_DECAY = 5e-5 # Standard regularization\n","SAVE_PATH = \"hyperparam_tune_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize WINNING model ---\n","model = ResidualTranslator(\n","    R_init=R.detach().clone(),\n","    input_dim=1024,\n","    hidden_dim=1024, # Our 0.815 winner's capacity\n","    output_dim=1536\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","print(f\"\\nTraining with Tuned Hyperparameters (Tau={TAU}, Margin={MARGIN})...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            # --- Our proven winning loss combo ---\n","            sims_with_tau = y_pred @ y_batch.T / TAU # Using new TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg) # Using new MARGIN\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient Clipping\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | \"\n","          f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","          f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","    if rec['R@10'] > best_r10:\n","        best_r10 = rec['R@10']\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = ResidualTranslator(\n","    R_init=R.detach().clone(),\n","    input_dim=1024,\n","    hidden_dim=1024,\n","    output_dim=1536\n",").to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024].to(device) # Ensure chunk is on device\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_hyperparam_tune.csv\", index=False)\n","print(\"\\n✅ submission_hyperparam_tune.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81O5BDSobOLm","executionInfo":{"status":"ok","timestamp":1761782828756,"user_tz":-60,"elapsed":118320,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"f131d54d-345c-481f-b106-fa8e41093c18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3346032053.py:157: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","\n","Training with Tuned Hyperparameters (Tau=0.05, Margin=0.25)...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/40:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-3346032053.py:169: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=1.5915 | R@1=0.1660  R@5=0.3586  R@10=0.4570  R@50=0.6866\n","✅ Best model saved (epoch 1, R@10=0.4570)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=1.4447 | R@1=0.1792  R@5=0.3820  R@10=0.4810  R@50=0.7132\n","✅ Best model saved (epoch 2, R@10=0.4810)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=1.3847 | R@1=0.1900  R@5=0.3940  R@10=0.4956  R@50=0.7306\n","✅ Best model saved (epoch 3, R@10=0.4956)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.3445 | R@1=0.1994  R@5=0.4014  R@10=0.5080  R@50=0.7402\n","✅ Best model saved (epoch 4, R@10=0.5080)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.3152 | R@1=0.2024  R@5=0.4076  R@10=0.5180  R@50=0.7454\n","✅ Best model saved (epoch 5, R@10=0.5180)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.2917 | R@1=0.2078  R@5=0.4162  R@10=0.5214  R@50=0.7552\n","✅ Best model saved (epoch 6, R@10=0.5214)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.2729 | R@1=0.2106  R@5=0.4208  R@10=0.5258  R@50=0.7566\n","✅ Best model saved (epoch 7, R@10=0.5258)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.2574 | R@1=0.2122  R@5=0.4256  R@10=0.5278  R@50=0.7616\n","✅ Best model saved (epoch 8, R@10=0.5278)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.2446 | R@1=0.2132  R@5=0.4276  R@10=0.5320  R@50=0.7660\n","✅ Best model saved (epoch 9, R@10=0.5320)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.2321 | R@1=0.2160  R@5=0.4294  R@10=0.5314  R@50=0.7668\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.2264 | R@1=0.2168  R@5=0.4306  R@10=0.5340  R@50=0.7670\n","✅ Best model saved (epoch 11, R@10=0.5340)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.2203 | R@1=0.2182  R@5=0.4304  R@10=0.5356  R@50=0.7674\n","✅ Best model saved (epoch 12, R@10=0.5356)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.2165 | R@1=0.2176  R@5=0.4326  R@10=0.5360  R@50=0.7706\n","✅ Best model saved (epoch 13, R@10=0.5360)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.2109 | R@1=0.2164  R@5=0.4322  R@10=0.5388  R@50=0.7692\n","✅ Best model saved (epoch 14, R@10=0.5388)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.2063 | R@1=0.2198  R@5=0.4356  R@10=0.5382  R@50=0.7690\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.2026 | R@1=0.2180  R@5=0.4364  R@10=0.5378  R@50=0.7710\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=1.1990 | R@1=0.2210  R@5=0.4362  R@10=0.5400  R@50=0.7702\n","✅ Best model saved (epoch 17, R@10=0.5400)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=1.1967 | R@1=0.2204  R@5=0.4358  R@10=0.5406  R@50=0.7696\n","✅ Best model saved (epoch 18, R@10=0.5406)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=1.1926 | R@1=0.2200  R@5=0.4392  R@10=0.5406  R@50=0.7714\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=1.1906 | R@1=0.2218  R@5=0.4384  R@10=0.5418  R@50=0.7718\n","✅ Best model saved (epoch 20, R@10=0.5418)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=1.1894 | R@1=0.2214  R@5=0.4390  R@10=0.5428  R@50=0.7710\n","✅ Best model saved (epoch 21, R@10=0.5428)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=1.1872 | R@1=0.2210  R@5=0.4410  R@10=0.5430  R@50=0.7722\n","✅ Best model saved (epoch 22, R@10=0.5430)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=1.1853 | R@1=0.2210  R@5=0.4410  R@10=0.5434  R@50=0.7728\n","✅ Best model saved (epoch 23, R@10=0.5434)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=1.1850 | R@1=0.2214  R@5=0.4404  R@10=0.5428  R@50=0.7734\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=1.1835 | R@1=0.2206  R@5=0.4412  R@10=0.5432  R@50=0.7742\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=1.1830 | R@1=0.2202  R@5=0.4406  R@10=0.5436  R@50=0.7728\n","✅ Best model saved (epoch 26, R@10=0.5436)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=1.1825 | R@1=0.2208  R@5=0.4424  R@10=0.5438  R@50=0.7736\n","✅ Best model saved (epoch 27, R@10=0.5438)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=1.1817 | R@1=0.2218  R@5=0.4410  R@10=0.5438  R@50=0.7742\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=1.1805 | R@1=0.2214  R@5=0.4412  R@10=0.5436  R@50=0.7730\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=1.1803 | R@1=0.2216  R@5=0.4416  R@10=0.5452  R@50=0.7734\n","✅ Best model saved (epoch 30, R@10=0.5452)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=1.1796 | R@1=0.2220  R@5=0.4418  R@10=0.5444  R@50=0.7734\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=1.1787 | R@1=0.2220  R@5=0.4412  R@10=0.5448  R@50=0.7738\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=1.1783 | R@1=0.2216  R@5=0.4416  R@10=0.5454  R@50=0.7740\n","✅ Best model saved (epoch 33, R@10=0.5454)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=1.1789 | R@1=0.2220  R@5=0.4412  R@10=0.5452  R@50=0.7738\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=1.1790 | R@1=0.2222  R@5=0.4416  R@10=0.5450  R@50=0.7736\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=1.1786 | R@1=0.2220  R@5=0.4416  R@10=0.5454  R@50=0.7732\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=1.1779 | R@1=0.2220  R@5=0.4418  R@10=0.5452  R@50=0.7738\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=1.1766 | R@1=0.2220  R@5=0.4416  R@10=0.5450  R@50=0.7736\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=1.1772 | R@1=0.2218  R@5=0.4416  R@10=0.5452  R@50=0.7734\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=1.1783 | R@1=0.2218  R@5=0.4416  R@10=0.5452  R@50=0.7734\n","\n","🎯 Best R@10 on validation = 0.5454\n","Model saved as hyperparam_tune_best.pth\n","Loading best model from hyperparam_tune_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_hyperparam_tune.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["## Fine tunning again"],"metadata":{"id":"3Qe2n2qPdete"}},{"cell_type":"code","source":["\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t_unique.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(f\"Computed orthogonal base R: {R.shape}\")\n","\n","# ------------------------------------------------------\n","# 4. MODEL: ResidualTranslator (Our 0.82388 Kaggle Winner)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Our 0.82388 Winning Hyperparams)\n","# ------------------------------------------------------\n","TAU = 0.05       # Our winning stricter temperature\n","MARGIN = 0.25    # Our winning stricter margin\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=MARGIN\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\")\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024].to(device) # Ensure chunk is on device\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0)\n","\n","    im_database = im_database.to(device) # Ensure database is on device\n","    query_img_indices = query_img_indices.to(device) # Ensure indices are on device\n","\n","    sim = preds @ im_database.T\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d\n","\n","    gt = query_img_indices\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (THE \"MARATHON\" RUN)\n","# ------------------------------------------------------\n","EPOCHS = 150 # Let's give it a truly long run\n","LR = 1e-4\n","WEIGHT_DECAY = 5e-5\n","SAVE_PATH = \"marathon_run_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize WINNING model ---\n","model = ResidualTranslator(\n","    R_init=R.detach().clone(),\n","    input_dim=1024,\n","    hidden_dim=1024, # Our 0.82388 winner's capacity\n","    output_dim=1536\n",").to(device)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","# Schedule the LR over the *entire* marathon\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","best_r10 = 0.0\n","# Load the weights from our 0.82388 winner to resume training\n","try:\n","    model.load_state_dict(torch.load(\"hyperparam_tune_best.pth\"))\n","    best_r10 = 0.5454 # Start from our previous best\n","    print(\"✅ Loaded weights from 'hyperparam_tune_best.pth'. Resuming training.\")\n","except Exception as e:\n","    print(f\"Could not load previous weights, starting from scratch. Error: {e}\")\n","\n","print(f\"\\nTraining Final Marathon (Epochs: {EPOCHS}, Tau={TAU}, Margin={MARGIN})...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            # --- Our proven winning loss combo ---\n","            sims_with_tau = y_pred @ y_batch.T / TAU # Using new TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg) # Using new MARGIN\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient Clipping\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation -----\n","    # Only validate every 2 epochs to save time\n","    if epoch % 2 == 0 or epoch == EPOCHS:\n","        rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","        print(f\"Epoch {epoch:03d}: loss={avg_loss:.4f} | \"\n","              f\"R@1={rec['R@1']:.4f}  R@5={rec['R@5']:.4f}  \"\n","              f\"R@10={rec['R@10']:.4f}  R@50={rec['R@50']:.4f}\")\n","\n","        if rec['R@10'] > best_r10:\n","            best_r10 = rec['R@10']\n","            torch.save(model.state_dict(), SAVE_PATH)\n","            print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = ResidualTranslator(\n","    R_init=R.detach().clone(),\n","    input_dim=1024,\n","    hidden_dim=1024,\n","    output_dim=1536\n",").to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_text_t), 1024):\n","        chunk = tx_test_t[i:i+1024].to(device) # Ensure chunk is on device\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_marathon_run.csv\", index=False)\n","print(\"\\n✅ submission_marathon_run.csv saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PwRcVds4dgwI","executionInfo":{"status":"error","timestamp":1761783579586,"user_tz":-60,"elapsed":389634,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"46a78563-0528-40da-affc-6e94276ea478"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n","Data preprocessed and normalized.\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2490564187.py:155: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","Train pairs: 112500, Validation pairs: 12500\n","✅ Loaded weights from 'hyperparam_tune_best.pth'. Resuming training.\n","\n","Training Final Marathon (Epochs: 150, Tau=0.05, Margin=0.25)...\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1/150:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-2490564187.py:175: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 002: loss=1.1780 | R@1=0.2324  R@5=0.4612  R@10=0.5588  R@50=0.7840\n","✅ Best model saved (epoch 2, R@10=0.5588)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 004: loss=1.1646 | R@1=0.2340  R@5=0.4624  R@10=0.5608  R@50=0.7856\n","✅ Best model saved (epoch 4, R@10=0.5608)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 006: loss=1.1537 | R@1=0.2326  R@5=0.4614  R@10=0.5608  R@50=0.7838\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 008: loss=1.1442 | R@1=0.2358  R@5=0.4622  R@10=0.5582  R@50=0.7826\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 010: loss=1.1332 | R@1=0.2346  R@5=0.4616  R@10=0.5614  R@50=0.7864\n","✅ Best model saved (epoch 10, R@10=0.5614)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 012: loss=1.1250 | R@1=0.2356  R@5=0.4608  R@10=0.5628  R@50=0.7870\n","✅ Best model saved (epoch 12, R@10=0.5628)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 014: loss=1.1225 | R@1=0.2360  R@5=0.4634  R@10=0.5614  R@50=0.7872\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 016: loss=1.1200 | R@1=0.2374  R@5=0.4628  R@10=0.5632  R@50=0.7866\n","✅ Best model saved (epoch 16, R@10=0.5632)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 018: loss=1.1141 | R@1=0.2356  R@5=0.4626  R@10=0.5624  R@50=0.7876\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 020: loss=1.1094 | R@1=0.2368  R@5=0.4636  R@10=0.5634  R@50=0.7864\n","✅ Best model saved (epoch 20, R@10=0.5634)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 022: loss=1.1074 | R@1=0.2364  R@5=0.4658  R@10=0.5626  R@50=0.7882\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 024: loss=1.1048 | R@1=0.2366  R@5=0.4640  R@10=0.5638  R@50=0.7880\n","✅ Best model saved (epoch 24, R@10=0.5638)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 026: loss=1.1031 | R@1=0.2374  R@5=0.4648  R@10=0.5640  R@50=0.7886\n","✅ Best model saved (epoch 26, R@10=0.5640)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 028: loss=1.1000 | R@1=0.2374  R@5=0.4654  R@10=0.5648  R@50=0.7876\n","✅ Best model saved (epoch 28, R@10=0.5648)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 030: loss=1.0999 | R@1=0.2382  R@5=0.4646  R@10=0.5662  R@50=0.7876\n","✅ Best model saved (epoch 30, R@10=0.5662)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 032: loss=1.0971 | R@1=0.2382  R@5=0.4642  R@10=0.5654  R@50=0.7884\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 034: loss=1.0954 | R@1=0.2370  R@5=0.4648  R@10=0.5656  R@50=0.7874\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 036: loss=1.0944 | R@1=0.2368  R@5=0.4648  R@10=0.5646  R@50=0.7882\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 038: loss=1.0900 | R@1=0.2372  R@5=0.4642  R@10=0.5654  R@50=0.7884\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 040: loss=1.0893 | R@1=0.2380  R@5=0.4642  R@10=0.5664  R@50=0.7882\n","✅ Best model saved (epoch 40, R@10=0.5664)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 042: loss=1.0897 | R@1=0.2380  R@5=0.4634  R@10=0.5658  R@50=0.7884\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 044: loss=1.0880 | R@1=0.2384  R@5=0.4654  R@10=0.5636  R@50=0.7880\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 046: loss=1.0878 | R@1=0.2374  R@5=0.4638  R@10=0.5646  R@50=0.7882\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 048: loss=1.0874 | R@1=0.2388  R@5=0.4622  R@10=0.5634  R@50=0.7872\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 050: loss=1.0846 | R@1=0.2372  R@5=0.4640  R@10=0.5650  R@50=0.7892\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 052: loss=1.0836 | R@1=0.2384  R@5=0.4634  R@10=0.5652  R@50=0.7892\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 054: loss=1.0821 | R@1=0.2372  R@5=0.4644  R@10=0.5634  R@50=0.7888\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 056: loss=1.0805 | R@1=0.2376  R@5=0.4634  R@10=0.5648  R@50=0.7894\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 058: loss=1.0788 | R@1=0.2392  R@5=0.4634  R@10=0.5654  R@50=0.7890\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 060: loss=1.0787 | R@1=0.2392  R@5=0.4642  R@10=0.5666  R@50=0.7892\n","✅ Best model saved (epoch 60, R@10=0.5666)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 062: loss=1.0771 | R@1=0.2378  R@5=0.4618  R@10=0.5654  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 064: loss=1.0745 | R@1=0.2378  R@5=0.4638  R@10=0.5650  R@50=0.7902\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 066: loss=1.0745 | R@1=0.2382  R@5=0.4640  R@10=0.5652  R@50=0.7892\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 068: loss=1.0737 | R@1=0.2380  R@5=0.4634  R@10=0.5664  R@50=0.7906\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 070: loss=1.0730 | R@1=0.2392  R@5=0.4640  R@10=0.5648  R@50=0.7902\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 072: loss=1.0725 | R@1=0.2372  R@5=0.4632  R@10=0.5648  R@50=0.7900\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 074: loss=1.0705 | R@1=0.2384  R@5=0.4622  R@10=0.5660  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 076: loss=1.0688 | R@1=0.2388  R@5=0.4634  R@10=0.5654  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 078: loss=1.0675 | R@1=0.2376  R@5=0.4622  R@10=0.5654  R@50=0.7906\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 080: loss=1.0670 | R@1=0.2390  R@5=0.4640  R@10=0.5656  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 082: loss=1.0670 | R@1=0.2390  R@5=0.4628  R@10=0.5656  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 084: loss=1.0666 | R@1=0.2388  R@5=0.4640  R@10=0.5658  R@50=0.7906\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 086: loss=1.0671 | R@1=0.2376  R@5=0.4638  R@10=0.5660  R@50=0.7922\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 088: loss=1.0660 | R@1=0.2392  R@5=0.4618  R@10=0.5660  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 090: loss=1.0661 | R@1=0.2388  R@5=0.4640  R@10=0.5654  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 092: loss=1.0662 | R@1=0.2384  R@5=0.4642  R@10=0.5658  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 094: loss=1.0649 | R@1=0.2380  R@5=0.4642  R@10=0.5664  R@50=0.7918\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 096: loss=1.0637 | R@1=0.2396  R@5=0.4644  R@10=0.5656  R@50=0.7908\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 098: loss=1.0639 | R@1=0.2392  R@5=0.4656  R@10=0.5660  R@50=0.7902\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 100: loss=1.0625 | R@1=0.2388  R@5=0.4638  R@10=0.5660  R@50=0.7908\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 102: loss=1.0625 | R@1=0.2390  R@5=0.4634  R@10=0.5660  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 104: loss=1.0619 | R@1=0.2384  R@5=0.4644  R@10=0.5666  R@50=0.7908\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 106: loss=1.0621 | R@1=0.2372  R@5=0.4648  R@10=0.5654  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 108: loss=1.0625 | R@1=0.2374  R@5=0.4640  R@10=0.5660  R@50=0.7916\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 110: loss=1.0622 | R@1=0.2376  R@5=0.4648  R@10=0.5656  R@50=0.7920\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 112: loss=1.0606 | R@1=0.2398  R@5=0.4636  R@10=0.5664  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 114: loss=1.0608 | R@1=0.2402  R@5=0.4642  R@10=0.5670  R@50=0.7908\n","✅ Best model saved (epoch 114, R@10=0.5670)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 116: loss=1.0609 | R@1=0.2400  R@5=0.4644  R@10=0.5662  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 118: loss=1.0606 | R@1=0.2394  R@5=0.4640  R@10=0.5672  R@50=0.7914\n","✅ Best model saved (epoch 118, R@10=0.5672)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 120: loss=1.0606 | R@1=0.2400  R@5=0.4652  R@10=0.5664  R@50=0.7922\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 122: loss=1.0600 | R@1=0.2390  R@5=0.4642  R@10=0.5664  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 124: loss=1.0605 | R@1=0.2390  R@5=0.4644  R@10=0.5660  R@50=0.7910\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 126: loss=1.0596 | R@1=0.2394  R@5=0.4644  R@10=0.5666  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 128: loss=1.0602 | R@1=0.2390  R@5=0.4654  R@10=0.5670  R@50=0.7916\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 130: loss=1.0598 | R@1=0.2396  R@5=0.4646  R@10=0.5670  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 132: loss=1.0590 | R@1=0.2390  R@5=0.4644  R@10=0.5666  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 134: loss=1.0582 | R@1=0.2392  R@5=0.4650  R@10=0.5672  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 136: loss=1.0594 | R@1=0.2394  R@5=0.4646  R@10=0.5670  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 138: loss=1.0593 | R@1=0.2394  R@5=0.4640  R@10=0.5670  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 140: loss=1.0590 | R@1=0.2390  R@5=0.4648  R@10=0.5672  R@50=0.7912\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 142: loss=1.0599 | R@1=0.2392  R@5=0.4648  R@10=0.5670  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 144: loss=1.0582 | R@1=0.2390  R@5=0.4644  R@10=0.5672  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 146: loss=1.0589 | R@1=0.2392  R@5=0.4648  R@10=0.5668  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 148: loss=1.0584 | R@1=0.2392  R@5=0.4648  R@10=0.5668  R@50=0.7914\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 150: loss=1.0585 | R@1=0.2392  R@5=0.4648  R@10=0.5668  R@50=0.7914\n","\n","🎯 Best R@10 on validation = 0.5672\n","Model saved as marathon_run_best.pth\n","Loading best model from marathon_run_best.pth for inference...\n","Best model loaded.\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'tx_text_t' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2490564187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mpreds_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_text_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtx_test_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ensure chunk is on device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mpreds_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tx_text_t' is not defined"]}]},{"cell_type":"code","source":["\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","test_data  = np.load(\"test.clean.npz\")\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","# --- We only need the *mean* from the training set for normalization ---\n","# Load a small chunk to save RAM\n","train_data_captions = np.load(\"train.npz\")[\"captions/embeddings\"]\n","tx_mean = torch.as_tensor(train_data_captions.mean(0, keepdims=True), dtype=torch.float32, device=device)\n","\n","# ------------------------------------------------------\n","# 2. Define the Model (MUST be identical to the one trained)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init_shape, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        # We don't need the R_init, just a buffer of the right shape\n","        self.register_buffer(\"R\", torch.zeros(R_init_shape))\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 3. Load Data and Model\n","# ------------------------------------------------------\n","CHECKPOINT_PATH = \"marathon_run_best.pth\"\n","SUBMISSION_PATH = \"submission_marathon_run.csv\"\n","\n","# --- Prepare test data ---\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","print(\"Test data loaded and normalized.\")\n","\n","# --- Load the saved model ---\n","print(f\"Loading best model from {CHECKPOINT_PATH} for inference...\")\n","# We must pass a dummy R_init_shape. We know it's (1536, 1024)\n","model = ResidualTranslator(\n","    R_init_shape=(1536, 1024),\n","    input_dim=1024,\n","    hidden_dim=1024,\n","    output_dim=1536\n",").to(device)\n","\n","model.load_state_dict(torch.load(CHECKPOINT_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","# ------------------------------------------------------\n","# 4. Inference + Submission (Corrected)\n","# ------------------------------------------------------\n","with torch.no_grad():\n","    preds_list = []\n","    # --- THIS IS THE FIX: tx_test_t (not tx_text_t) ---\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024].to(device)\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","print(\"Inference complete.\")\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(SUBMISSION_PATH, index=False)\n","print(f\"\\n✅ {SUBMISSION_PATH} saved successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7I_eTfShfiqm","executionInfo":{"status":"ok","timestamp":1761783734176,"user_tz":-60,"elapsed":7405,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"3c73fe15-7402-460b-d53d-9f01da023195"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Test data loaded and normalized.\n","Loading best model from marathon_run_best.pth for inference...\n","Best model loaded.\n","Inference complete.\n","\n","✅ submission_marathon_run.csv saved successfully.\n"]}]},{"cell_type":"markdown","source":["# Previous 2 blocks belongs to one fine-tunned experiment"],"metadata":{"id":"8SqFCmqJg6po"}},{"cell_type":"markdown","source":["# New Fine-Tunned experiment\n","# got score as 0.83413"],"metadata":{"id":"MXs32KaBhAzp"}},{"cell_type":"code","source":["# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","print(f\"Train shapes: {tx_train.shape}, {im_train.shape}, {im_train_expanded.shape}\")\n","print(f\"Test shape: {tx_test.shape}\")\n","\n","# ------------------------------------------------------\n","# 2. Data preprocessing (Centering + Normalization)\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t_unique = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t_unique.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t_unique = F.normalize(im_train_t_unique - im_mean, p=2, dim=1)\n","tx_test_t = F.normalize(tx_test_t - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","print(\"Data preprocessed and normalized.\")\n","\n","# ------------------------------------------------------\n","# 3. Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","# We only need the shape to initialize the model class\n","R_SHAPE = (1536, 1024)\n","print(f\"Base R shape: {R_SHAPE}\")\n","\n","# ------------------------------------------------------\n","# 4. MODEL: ResidualTranslator (Our 0.83359 Kaggle Winner)\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init_shape, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", torch.zeros(R_init_shape))\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 5. Loss Functions (Our Winning Hyperparams)\n","# ------------------------------------------------------\n","TAU = 0.05       # Our winning stricter temperature\n","MARGIN = 0.25    # Our winning stricter margin\n","LOSS_WEIGHT_CONTRASTIVE = 0.7\n","LOSS_WEIGHT_TRIPLET = 0.3\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=MARGIN\n",")\n","\n","# ------------------------------------------------------\n","# 6. Validation split (Unchanged)\n","# ------------------------------------------------------\n","N = len(tx_train_t)\n","val_size = int(0.1 * N)\n","idx_cpu = torch.randperm(N, device=\"cpu\")\n","val_idx, train_idx = idx_cpu[:val_size], idx_cpu[val_size:]\n","\n","img_indices_train = (train_idx // 5).to(device)\n","img_indices_val = (val_idx // 5).to(device)\n","\n","tx_val_t, im_val_t = tx_train_t[val_idx], im_train_exp[val_idx]\n","tx_train_t_sub, im_train_exp_sub = tx_train_t[train_idx], im_train_exp[train_idx]\n","\n","train_dataset = TensorDataset(tx_train_t_sub, im_train_exp_sub, img_indices_train)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","print(f\"Train pairs: {len(train_dataset)}, Validation pairs: {len(val_idx)}\")\n","\n","# ------------------------------------------------------\n","# 7. Recall@K utility (Unchanged)\n","# ------------------------------------------------------\n","@torch.no_grad()\n","def recall_at_k(model, tx_queries, im_database, query_img_indices, repeat_factor=5, ks=(1, 5, 10, 50), k_csls=10):\n","    model.eval()\n","    preds_list = []\n","    # Process in chunks to avoid OOM on validation\n","    for i in range(0, len(tx_queries), 1024):\n","        chunk = tx_queries[i:i+1024].to(device)\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0)\n","\n","    im_database = im_database.to(device)\n","    query_img_indices = query_img_indices.to(device)\n","\n","    # CSLS calculation\n","    sim = preds @ im_database.T\n","\n","    knn_q = torch.topk(sim, k=k_csls, dim=1).values\n","    mean_knn_q = knn_q.mean(1, keepdim=True)\n","\n","    knn_d = torch.topk(sim.T, k=k_csls, dim=1).values\n","    mean_knn_d = knn_d.mean(1, keepdim=True).T\n","\n","    csls_sim = 2 * sim - mean_knn_q - mean_knn_d\n","\n","    gt = query_img_indices\n","\n","    top_indices = torch.argsort(csls_sim, dim=1, descending=True)\n","\n","    recalls = {}\n","    for k in ks:\n","        top_k_preds = top_indices[:, :k]\n","        correct_in_top_k = (top_k_preds == gt.unsqueeze(1)).any(dim=1)\n","        recall_at_k = correct_in_top_k.float().mean().item()\n","        recalls[f\"R@{k}\"] = recall_at_k\n","\n","    return recalls\n","\n","# ------------------------------------------------------\n","# 8. Training Loop (THE \"FINAL FINE-TUNE\" RUN)\n","# ------------------------------------------------------\n","EPOCHS = 50 # Max epochs for fine-tuning\n","START_LR = 1e-5 # Start with a low learning rate\n","WEIGHT_DECAY = 5e-5\n","LOAD_PATH = \"marathon_run_best.pth\"\n","SAVE_PATH = \"final_finetune_best.pth\" # New save path\n","\n","val_query_subset = tx_val_t[:5000]\n","val_indices_subset = img_indices_val[:5000]\n","val_db_subset = im_train_t_unique\n","\n","# --- Initialize WINNING model ---\n","model = ResidualTranslator(\n","    R_init_shape=R_SHAPE,\n","    input_dim=1024,\n","    hidden_dim=1024, # Our 0.83359 winner's capacity\n","    output_dim=1536\n",").to(device)\n","\n","# --- Load our best-ever model ---\n","try:\n","    model.load_state_dict(torch.load(LOAD_PATH))\n","    print(f\"✅ Loaded weights from '{LOAD_PATH}'. Starting fine-tune.\")\n","except Exception as e:\n","    print(f\"FATAL: Could not load '{LOAD_PATH}'. Stopping. Error: {e}\")\n","    # Stop the script if we can't load the model\n","    raise e\n","\n","best_r10 = 0.5672 # Start from our marathon-run's best score\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=START_LR, weight_decay=WEIGHT_DECAY)\n","\n","# --- This is the new, smart scheduler ---\n","# It will monitor the R@10 score ('max' mode)\n","# 'patience=5': Wait 5 epochs for an improvement\n","# 'factor=0.5': If no improvement, cut LR in half\n","# 'min_lr=1e-7': Don't go lower than this\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, 'max',\n","    patience=5,\n","    factor=0.5,\n","    min_lr=1e-7\n","    # verbose=True was removed, it is deprecated.\n",")\n","scaler = torch.cuda.amp.GradScaler() # Mixed precision\n","\n","print(f\"\\nTraining Final Fine-Tune (Epochs: {EPOCHS}, Start LR={START_LR})...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch, img_indices in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        optimizer.zero_grad()\n","\n","        with torch.cuda.amp.autocast():\n","            y_pred = model(x_batch)\n","\n","            # --- Our proven winning loss combo ---\n","            sims_with_tau = y_pred @ y_batch.T / TAU\n","            labels = torch.arange(y_pred.size(0), device=device)\n","            loss_con = F.cross_entropy(sims_with_tau, labels)\n","\n","            with torch.no_grad():\n","                sims_no_tau = y_pred @ y_batch.T\n","                positive_mask = (img_indices.unsqueeze(1) == img_indices.unsqueeze(0))\n","                sims_no_tau.masked_fill_(positive_mask, -float('inf'))\n","                hard_neg_idx = sims_no_tau.argmax(dim=1)\n","\n","            y_hard_neg = y_batch[hard_neg_idx]\n","            loss_tri = triplet_loss_fn(y_pred, y_batch, y_hard_neg)\n","\n","            loss = (LOSS_WEIGHT_CONTRASTIVE * loss_con) + (LOSS_WEIGHT_TRIPLET * loss_tri)\n","\n","        scaler.scale(loss).backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","\n","    # ----- Validation (MUST run every epoch for the scheduler) -----\n","    rec = recall_at_k(model, val_query_subset, val_db_subset, val_indices_subset)\n","    current_r10 = rec['R@10']\n","\n","    # --- THIS IS THE CORRECTED PRINT STATEMENT ---\n","    print(f\"Epoch {epoch:02d}: loss={avg_loss:.4f} | R@1(Val)={rec['R@1']:.4f}  R@5(Val)={rec['R@5']:.4f}  R@10(Val)={current_r10:.4f}  R@50(Val)={rec['R@50']:.4f}\")\n","\n","    if current_r10 > best_r10:\n","        best_r10 = current_r10\n","        torch.save(model.state_dict(), SAVE_PATH)\n","        print(f\"✅ Best model saved (epoch {epoch}, R@10={best_r10:.4f})\")\n","\n","    # Step the scheduler based on the R@10 score\n","    scheduler.step(current_r10)\n","\n","    # Manually print the LR to replace 'verbose'\n","    current_lr = optimizer.param_groups[0]['lr']\n","    if 'last_lr' not in locals(): locals()['last_lr'] = START_LR\n","    if current_lr != locals()['last_lr']:\n","        print(f\"--- Learning rate reduced to {current_lr:.1e} ---\")\n","        locals()['last_lr'] = current_lr\n","\n","    # Check if LR is at minimum (early stopping)\n","    if current_lr <= (1e-7 + 1e-9):\n","        print(f\"Learning rate at minimum. Stopping training early at epoch {epoch}.\")\n","        break\n","\n","# --- This block is now DE-DENTED ---\n","print(f\"\\n🎯 Best R@10 on validation = {best_r10:.4f}\\nModel saved as {SAVE_PATH}\")\n","\n","# ------------------------------------------------------\n","# 9. Inference + Submission\n","# ------------------------------------------------------\n","print(f\"Loading best model from {SAVE_PATH} for inference...\")\n","model = ResidualTranslator(\n","    R_init_shape=R_SHAPE,\n","    input_dim=1024,\n","    hidden_dim=1024,\n","    output_dim=1536\n",").to(device)\n","model.load_state_dict(torch.load(SAVE_PATH))\n","model.eval()\n","print(\"Best model loaded.\")\n","\n","with torch.no_grad():\n","    preds_list = []\n","    for i in range(0, len(tx_test_t), 1024):\n","        chunk = tx_test_t[i:i+1024].to(device)\n","        preds_list.append(model(chunk))\n","    preds = torch.cat(preds_list, dim=0).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_final_finetune.csv\", index=False)\n","print(\"\\n✅ submission_final_finetune.csv saved successfully.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxKH3VNRhJDf","executionInfo":{"status":"ok","timestamp":1761784912891,"user_tz":-60,"elapsed":123639,"user":{"displayName":"Ghulam Mujtaba","userId":"12658049345253595626"}},"outputId":"bb27551d-b33a-41f8-b6ba-dc0e5dc14af9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024), (25000, 1536), (125000, 1536)\n","Test shape: (1500, 1024)\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2478858331.py:178: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler() # Mixed precision\n"]},{"output_type":"stream","name":"stdout","text":["Data preprocessed and normalized.\n","Base R shape: (1536, 1024)\n","Train pairs: 112500, Validation pairs: 12500\n","✅ Loaded weights from 'marathon_run_best.pth'. Starting fine-tune.\n","\n","Training Final Fine-Tune (Epochs: 50, Start LR=1e-05)...\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50:   0%|          | 0/220 [00:00<?, ?it/s]/tmp/ipython-input-2478858331.py:189: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 01: loss=1.0723 | R@1(Val)=0.2684  R@5(Val)=0.5124  R@10(Val)=0.6172  R@50(Val)=0.8304\n","✅ Best model saved (epoch 1, R@10=0.6172)\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: loss=1.0722 | R@1(Val)=0.2684  R@5(Val)=0.5110  R@10(Val)=0.6166  R@50(Val)=0.8308\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: loss=1.0714 | R@1(Val)=0.2678  R@5(Val)=0.5120  R@10(Val)=0.6162  R@50(Val)=0.8306\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: loss=1.0712 | R@1(Val)=0.2680  R@5(Val)=0.5110  R@10(Val)=0.6146  R@50(Val)=0.8298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: loss=1.0706 | R@1(Val)=0.2682  R@5(Val)=0.5106  R@10(Val)=0.6142  R@50(Val)=0.8304\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: loss=1.0707 | R@1(Val)=0.2680  R@5(Val)=0.5100  R@10(Val)=0.6134  R@50(Val)=0.8304\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: loss=1.0692 | R@1(Val)=0.2678  R@5(Val)=0.5108  R@10(Val)=0.6130  R@50(Val)=0.8298\n","--- Learning rate reduced to 5.0e-06 ---\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: loss=1.0685 | R@1(Val)=0.2674  R@5(Val)=0.5098  R@10(Val)=0.6112  R@50(Val)=0.8298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: loss=1.0690 | R@1(Val)=0.2664  R@5(Val)=0.5096  R@10(Val)=0.6124  R@50(Val)=0.8302\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: loss=1.0682 | R@1(Val)=0.2662  R@5(Val)=0.5092  R@10(Val)=0.6120  R@50(Val)=0.8298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: loss=1.0689 | R@1(Val)=0.2662  R@5(Val)=0.5094  R@10(Val)=0.6120  R@50(Val)=0.8300\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: loss=1.0690 | R@1(Val)=0.2660  R@5(Val)=0.5088  R@10(Val)=0.6120  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: loss=1.0673 | R@1(Val)=0.2654  R@5(Val)=0.5090  R@10(Val)=0.6120  R@50(Val)=0.8298\n","--- Learning rate reduced to 2.5e-06 ---\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: loss=1.0681 | R@1(Val)=0.2656  R@5(Val)=0.5090  R@10(Val)=0.6124  R@50(Val)=0.8298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: loss=1.0683 | R@1(Val)=0.2652  R@5(Val)=0.5082  R@10(Val)=0.6124  R@50(Val)=0.8298\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: loss=1.0683 | R@1(Val)=0.2652  R@5(Val)=0.5084  R@10(Val)=0.6124  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: loss=1.0676 | R@1(Val)=0.2654  R@5(Val)=0.5086  R@10(Val)=0.6130  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: loss=1.0682 | R@1(Val)=0.2654  R@5(Val)=0.5086  R@10(Val)=0.6122  R@50(Val)=0.8290\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: loss=1.0675 | R@1(Val)=0.2658  R@5(Val)=0.5080  R@10(Val)=0.6122  R@50(Val)=0.8296\n","--- Learning rate reduced to 1.3e-06 ---\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: loss=1.0676 | R@1(Val)=0.2656  R@5(Val)=0.5086  R@10(Val)=0.6124  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: loss=1.0679 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6124  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: loss=1.0683 | R@1(Val)=0.2656  R@5(Val)=0.5086  R@10(Val)=0.6124  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: loss=1.0674 | R@1(Val)=0.2654  R@5(Val)=0.5084  R@10(Val)=0.6120  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: loss=1.0681 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6120  R@50(Val)=0.8296\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: loss=1.0670 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6126  R@50(Val)=0.8292\n","--- Learning rate reduced to 6.3e-07 ---\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: loss=1.0670 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6122  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: loss=1.0670 | R@1(Val)=0.2658  R@5(Val)=0.5084  R@10(Val)=0.6118  R@50(Val)=0.8292\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: loss=1.0660 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6120  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: loss=1.0676 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6120  R@50(Val)=0.8292\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: loss=1.0675 | R@1(Val)=0.2658  R@5(Val)=0.5084  R@10(Val)=0.6120  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: loss=1.0665 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6118  R@50(Val)=0.8294\n","--- Learning rate reduced to 3.1e-07 ---\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: loss=1.0688 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6122  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: loss=1.0681 | R@1(Val)=0.2654  R@5(Val)=0.5084  R@10(Val)=0.6120  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: loss=1.0671 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6120  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: loss=1.0675 | R@1(Val)=0.2656  R@5(Val)=0.5084  R@10(Val)=0.6122  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: loss=1.0670 | R@1(Val)=0.2658  R@5(Val)=0.5084  R@10(Val)=0.6122  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: loss=1.0676 | R@1(Val)=0.2658  R@5(Val)=0.5080  R@10(Val)=0.6126  R@50(Val)=0.8292\n","--- Learning rate reduced to 1.6e-07 ---\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: loss=1.0670 | R@1(Val)=0.2658  R@5(Val)=0.5080  R@10(Val)=0.6124  R@50(Val)=0.8292\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: loss=1.0665 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6124  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: loss=1.0682 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6124  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 41: loss=1.0672 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6122  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 42: loss=1.0669 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6124  R@50(Val)=0.8294\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 43: loss=1.0669 | R@1(Val)=0.2656  R@5(Val)=0.5082  R@10(Val)=0.6124  R@50(Val)=0.8294\n","--- Learning rate reduced to 1.0e-07 ---\n","Learning rate at minimum. Stopping training early at epoch 43.\n","\n","🎯 Best R@10 on validation = 0.6172\n","Model saved as final_finetune_best.pth\n","Loading best model from final_finetune_best.pth for inference...\n","Best model loaded.\n","\n","✅ submission_final_finetune.csv saved successfully.\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyMhZ5RH1nB9aC87WOwYkSCn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGa1mGbfmpH-","executionInfo":{"status":"ok","timestamp":1761685737534,"user_tz":-60,"elapsed":17834,"user":{"displayName":"ghulam mujtaba","userId":"00646622876627477130"}},"outputId":"319135a4-0f53-4590-c930-067f692cb87e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["import os\n","\n","BASE_DIR = \"/content/drive/MyDrive/AML Challenge\"\n","os.chdir(BASE_DIR)\n","print(\"Current working directory:\", os.getcwd())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sD6oJABmys8","executionInfo":{"status":"ok","timestamp":1761685743015,"user_tz":-60,"elapsed":14,"user":{"displayName":"ghulam mujtaba","userId":"00646622876627477130"}},"outputId":"362398de-7aec-4c37-9773-4b65f2d1a1c6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Current working directory: /content/drive/MyDrive/AML Challenge\n"]}]},{"cell_type":"markdown","source":["# Pre-Processing"],"metadata":{"id":"INNIVdJ4phrW"}},{"cell_type":"code","source":["import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np, pandas as pd\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]   # (125000, 1024)\n","im_train = train_data[\"images/embeddings\"]     # (25000, 1536)\n","tx_test  = test_data[\"captions/embeddings\"]    # (1500, 1024)\n","\n","print(\"Train shapes:\", tx_train.shape, im_train.shape)\n","print(\"Test shape:\", tx_test.shape)\n","\n","# ------------------------------------------------------\n","# 2. Match each caption to its corresponding image\n","# ------------------------------------------------------\n","repeat_factor = len(tx_train) // len(im_train)   # 5 captions per image\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNsu63vepjn7","executionInfo":{"status":"ok","timestamp":1761685759975,"user_tz":-60,"elapsed":15164,"user":{"displayName":"ghulam mujtaba","userId":"00646622876627477130"}},"outputId":"5003c8da-298d-412b-8e3e-d3abacdd76c2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024) (25000, 1536)\n","Test shape: (1500, 1024)\n"]}]},{"cell_type":"markdown","source":["# Experiment 1: got score as 0.81780\n","# Residual-Orthogonal + Contrastive version"],"metadata":{"id":"r4gxkwL7pM0F"}},{"cell_type":"code","source":["# ------------------------------------------------------\n","# 2. Match each caption to its corresponding image\n","# ------------------------------------------------------\n","repeat_factor = len(tx_train) // len(im_train)   # 5 captions per image\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","\n","# ------------------------------------------------------\n","# 3. Convert to tensors + center + normalize\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","# Center + normalize (same mean for test)\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t.mean(0, keepdim=True)\n","\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t = F.normalize(im_train_t - im_mean, p=2, dim=1)\n","tx_test_t  = F.normalize(tx_test_t  - tx_mean, p=2, dim=1)\n","\n","# Expand image embeddings for each caption\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 4. Compute Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh  # (1536 × 1024)\n","\n","print(\"Computed orthogonal base R:\", R.shape)\n","\n","# ------------------------------------------------------\n","# 5. Define Residual-Orthogonal Translator\n","# ------------------------------------------------------\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        out = F.normalize(base + res, p=2, dim=1)\n","        return out\n","\n","# ------------------------------------------------------\n","# 6. Define losses\n","# ------------------------------------------------------\n","def contrastive_loss(pred, target, tau=0.07):\n","    sims = pred @ target.T / tau\n","    labels = torch.arange(pred.size(0), device=device)\n","    return F.cross_entropy(sims, labels)\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.2\n",")\n","\n","# ------------------------------------------------------\n","# 7. DataLoader\n","# ------------------------------------------------------\n","train_dataset = TensorDataset(tx_train_t, im_train_exp)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","\n","# ------------------------------------------------------\n","# 8. Initialize model + optimizer\n","# ------------------------------------------------------\n","model = ResidualTranslator(R.detach().clone()).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","\n","# ------------------------------------------------------\n","# 9. Training loop\n","# ------------------------------------------------------\n","EPOCHS = 30\n","print(\"\\nTraining Residual-Orthogonal Translator...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","\n","    for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        optimizer.zero_grad()\n","\n","        y_pred = model(x_batch)\n","        idx = torch.randperm(x_batch.size(0), device=device)\n","        y_neg = y_batch[idx]\n","\n","        loss_cos = contrastive_loss(y_pred, y_batch)\n","        loss_tri = triplet_loss_fn(y_pred, y_batch, y_neg)\n","        loss = 0.7 * loss_cos + 0.3 * loss_tri\n","\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch:02d}: avg loss = {avg_loss:.4f}\")\n","\n","torch.save(model.state_dict(), \"residual_orthogonal.pth\")\n","print(\"\\n✅ Training completed and model saved as residual_orthogonal.pth\")\n","\n","# ------------------------------------------------------\n","# 10. Inference for submission\n","# ------------------------------------------------------\n","model.eval()\n","with torch.no_grad():\n","    tx_test_n = F.normalize(tx_test_t, p=2, dim=1)\n","    preds = model(tx_test_n).cpu().numpy()\n","\n","# ------------------------------------------------------\n","# 11. Save submission file\n","# ------------------------------------------------------\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_residual.csv\", index=False)\n","\n","print(\"✅ Saved submission_residual.csv\")\n","print(submission.head(3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E62bqTXwpRBn","executionInfo":{"status":"ok","timestamp":1761685908844,"user_tz":-60,"elapsed":71174,"user":{"displayName":"ghulam mujtaba","userId":"00646622876627477130"}},"outputId":"4c69e492-928c-4985-8491-eb9d4248960c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Computed orthogonal base R: torch.Size([1536, 1024])\n","\n","Training Residual-Orthogonal Translator...\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.7665\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.6145\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.5555\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.5218\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.4972\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.4781\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.4637\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.4508\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.4397\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.4303\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.4221\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.4162\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.4093\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.4035\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.3994\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.3924\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.3882\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.3842\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.3798\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.3767\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 1.3730\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 1.3689\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 1.3669\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 1.3636\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 1.3608\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 1.3575\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 1.3553\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 1.3527\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 1.3505\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 1.3500\n","\n","✅ Training completed and model saved as residual_orthogonal.pth\n","✅ Saved submission_residual.csv\n","   id                                          embedding\n","0   1  [-0.002457899274304509, 0.01370843406766653, 0...\n","1   2  [-0.03886483237147331, -0.031252775341272354, ...\n","2   3  [-0.006630855146795511, -0.019226159900426865,...\n"]}]},{"cell_type":"markdown","source":["# Experiment 2: got Score as: 0.73571\n","\n","# Residual-Orthogonal + Contrastive version\n","# Deep Residual Translator (ResMLP-style)"],"metadata":{"id":"RFqCKpfBwZqN"}},{"cell_type":"code","source":["# ======================================================\n","#  EXPERIMENT 7 — Deep Residual Translator (ResMLP-style)\n","# ======================================================\n","\n","import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","from tqdm import tqdm\n","import numpy as np, pandas as pd\n","\n","# ------------------------------------------------------\n","# 1. Device and data loading\n","# ------------------------------------------------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","\n","print(\"Train shapes:\", tx_train.shape, im_train.shape, \"Test:\", tx_test.shape)\n","\n","# ------------------------------------------------------\n","# 2. Convert to tensors + center + normalize\n","# ------------------------------------------------------\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t.mean(0, keepdim=True)\n","\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t = F.normalize(im_train_t - im_mean, p=2, dim=1)\n","tx_test_t  = F.normalize(tx_test_t  - tx_mean, p=2, dim=1)\n","\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 3. Compute Orthogonal Procrustes base (R)\n","# ------------------------------------------------------\n","tx_centroids = tx_train_t.view(-1, 5, tx_train_t.shape[1]).mean(dim=1)\n","M = im_train_t.T @ tx_centroids\n","U, S, Vh = torch.linalg.svd(M, full_matrices=False)\n","R = U @ Vh\n","print(\"Computed orthogonal base R:\", R.shape)\n","\n","# ------------------------------------------------------\n","# 4. Residual building blocks (ResMLP-style)\n","# ------------------------------------------------------\n","class ResidualBlock(nn.Module):\n","    def __init__(self, dim, dropout=0.1):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(dim)\n","        self.fc1 = nn.Linear(dim, dim)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(dim, dim)\n","        self.drop = nn.Dropout(dropout)\n","    def forward(self, x):\n","        out = self.fc2(self.act(self.fc1(self.norm(x))))\n","        return x + self.drop(out)\n","\n","# ------------------------------------------------------\n","# 5. Deep Residual Translator\n","# ------------------------------------------------------\n","class DeepResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536, depth=3):\n","        super().__init__()\n","        self.R = nn.Parameter(R_init.clone(), requires_grad=False)   # frozen at start\n","        blocks = []\n","        for _ in range(depth):\n","            blocks.append(ResidualBlock(hidden_dim))\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            *blocks,\n","            nn.LayerNorm(hidden_dim),\n","            nn.Linear(hidden_dim, output_dim)\n","        )\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        return F.normalize(base + res, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 6. Loss functions\n","# ------------------------------------------------------\n","def contrastive_loss(pred, target, tau=0.07):\n","    sims = pred @ target.T / tau\n","    labels = torch.arange(pred.size(0), device=device)\n","    return F.cross_entropy(sims, labels)\n","\n","triplet_loss_fn = nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y),\n","    margin=0.25\n",")\n","\n","# ------------------------------------------------------\n","# 7. Dataloader\n","# ------------------------------------------------------\n","train_dataset = TensorDataset(tx_train_t, im_train_exp)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","\n","# ------------------------------------------------------\n","# 8. Model + optimizer\n","# ------------------------------------------------------\n","model = DeepResidualTranslator(R, depth=3).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)\n","\n","# ------------------------------------------------------\n","# 9. Training loop (2-phase: frozen R then fine-tune)\n","# ------------------------------------------------------\n","EPOCHS = 40\n","print(\"\\nTraining Deep Residual Translator...\\n\")\n","\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","    for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        optimizer.zero_grad()\n","\n","        y_pred = model(x_batch)\n","        idx = torch.randperm(x_batch.size(0), device=device)\n","        y_neg = y_batch[idx]\n","\n","        loss_cos = contrastive_loss(y_pred, y_batch, tau=0.05)\n","        loss_tri = triplet_loss_fn(y_pred, y_batch, y_neg)\n","        loss = 0.7 * loss_cos + 0.3 * loss_tri\n","\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    scheduler.step()\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch:02d}: avg loss = {avg_loss:.4f}\")\n","\n","    # unfreeze R after 20 epochs\n","    if epoch == 20:\n","        model.R.requires_grad = True\n","        print(\"→ Unfroze R for fine-tuning.\")\n","\n","torch.save(model.state_dict(), \"deep_resmlp_translator.pth\")\n","print(\"\\n✅ Training completed and model saved as deep_resmlp_translator.pth\")\n","\n","# ------------------------------------------------------\n","# 10. Inference and submission\n","# ------------------------------------------------------\n","model.eval()\n","with torch.no_grad():\n","    tx_test_n = F.normalize(tx_test_t, p=2, dim=1)\n","    preds = model(tx_test_n).cpu().numpy()\n","\n","test_ids = test_data[\"captions/ids\"].astype(int)\n","submission = pd.DataFrame({\n","    \"id\": test_ids,\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_ResMLP.csv\", index=False)\n","print(\"✅ Saved submission_ResMLP.csv\")\n","print(submission.head(3))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVvkM0umweVc","executionInfo":{"status":"ok","timestamp":1761687885820,"user_tz":-60,"elapsed":180324,"user":{"displayName":"ghulam mujtaba","userId":"00646622876627477130"}},"outputId":"6ff0271c-9ae5-4654-a819-26d473405442"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Train shapes: (125000, 1024) (25000, 1536) Test: (1500, 1024)\n","Computed orthogonal base R: torch.Size([1536, 1024])\n","\n","Training Deep Residual Translator...\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.8413\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.3359\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.1679\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.0494\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 0.9493\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 0.8607\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 0.7788\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 0.7037\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 0.6339\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 0.5697\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 0.5124\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 0.4607\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 0.4200\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 0.3812\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 0.3486\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 0.3203\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 0.2970\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 0.2745\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 0.2578\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 0.2416\n","→ Unfroze R for fine-tuning.\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21: avg loss = 0.2270\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22: avg loss = 0.2155\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23: avg loss = 0.2036\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24: avg loss = 0.1944\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25: avg loss = 0.1862\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26: avg loss = 0.1787\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27: avg loss = 0.1713\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28: avg loss = 0.1654\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29: avg loss = 0.1596\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30: avg loss = 0.1551\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 31: avg loss = 0.1512\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 32: avg loss = 0.1468\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 33: avg loss = 0.1447\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 34: avg loss = 0.1426\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 35: avg loss = 0.1392\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 36: avg loss = 0.1386\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 37: avg loss = 0.1378\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 38: avg loss = 0.1359\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 39: avg loss = 0.1354\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 40: avg loss = 0.1360\n","\n","✅ Training completed and model saved as deep_resmlp_translator.pth\n","✅ Saved submission_ResMLP.csv\n","   id                                          embedding\n","0   1  [0.01420063991099596, -0.005876442883163691, 0...\n","1   2  [0.00700219115242362, 7.112888852134347e-05, 0...\n","2   3  [0.006470625754445791, -0.02336675114929676, 0...\n"]}]},{"cell_type":"markdown","source":["# Experiment 3: got score as 0\n","# Structure-Aware Contrastive Fine-Tune"],"metadata":{"id":"2ckup2MZ3ey1"}},{"cell_type":"code","source":["# ======================================================\n","#  EXPERIMENT 9 — Structure-Aware Residual-Orthogonal Translator\n","# ======================================================\n","\n","import torch, torch.nn.functional as F\n","from tqdm import tqdm\n","import numpy as np, pandas as pd\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# ------------------------------------------------------\n","# 1. Load data\n","# ------------------------------------------------------\n","train_data = np.load(\"train.npz\")\n","test_data  = np.load(\"test.clean.npz\")\n","tx_train = train_data[\"captions/embeddings\"]\n","im_train = train_data[\"images/embeddings\"]\n","tx_test  = test_data[\"captions/embeddings\"]\n","\n","repeat_factor = len(tx_train) // len(im_train)\n","im_train_expanded = np.repeat(im_train, repeat_factor, axis=0)\n","\n","tx_train_t = torch.as_tensor(tx_train, dtype=torch.float32, device=device)\n","im_train_t = torch.as_tensor(im_train, dtype=torch.float32, device=device)\n","tx_test_t  = torch.as_tensor(tx_test,  dtype=torch.float32, device=device)\n","im_train_exp = torch.as_tensor(im_train_expanded, dtype=torch.float32, device=device)\n","\n","# normalize\n","tx_mean = tx_train_t.mean(0, keepdim=True)\n","im_mean = im_train_t.mean(0, keepdim=True)\n","tx_train_t = F.normalize(tx_train_t - tx_mean, p=2, dim=1)\n","im_train_t = F.normalize(im_train_t - im_mean, p=2, dim=1)\n","tx_test_t  = F.normalize(tx_test_t  - tx_mean, p=2, dim=1)\n","im_train_exp = F.normalize(im_train_exp - im_mean, p=2, dim=1)\n","\n","# ------------------------------------------------------\n","# 2. Reload your previous trained model\n","# ------------------------------------------------------\n","model = ResidualTranslator(torch.zeros(1536,1024)).to(device)\n","model.load_state_dict(torch.load(\"residual_orthogonal.pth\"))\n","print(\"Loaded previous 0.8180 checkpoint.\")\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n","\n","\n","class ResidualTranslator(nn.Module):\n","    def __init__(self, R_init, input_dim=1024, hidden_dim=1024, output_dim=1536):\n","        super().__init__()\n","        self.register_buffer(\"R\", R_init)\n","        self.residual = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        base = x @ self.R.T\n","        res = self.residual(x)\n","        out = F.normalize(base + res, p=2, dim=1)\n","        return out\n","\n","\n","# ------------------------------------------------------\n","# 3. New structure-aware losses\n","# ------------------------------------------------------\n","def contrastive_loss(pred, target, tau=0.07):\n","    sims = pred @ target.T / tau\n","    labels = torch.arange(pred.size(0), device=device)\n","    return F.cross_entropy(sims, labels)\n","\n","triplet_loss_fn = torch.nn.TripletMarginWithDistanceLoss(\n","    distance_function=lambda x, y: 1 - F.cosine_similarity(x, y), margin=0.2\n",")\n","\n","def knn_preservation_loss(src, mapped, k=10):\n","    # preserve relative neighbors\n","    with torch.no_grad():\n","        sims_src = src @ src.T\n","        _, nn_src = sims_src.topk(k, dim=1)\n","    sims_map = mapped @ mapped.T\n","    gathered = sims_map.gather(1, nn_src)\n","    return 1 - gathered.mean()  # smaller is better\n","\n","def caption_consistency_loss(pred, group=5):\n","    # Ensure divisible by 5\n","    bsz = pred.shape[0] - (pred.shape[0] % group)\n","    if bsz == 0:\n","        return torch.tensor(0.0, device=pred.device)\n","    pred = pred[:bsz]\n","    pred_g = pred.view(-1, group, pred.shape[1])\n","    centroid = pred_g.mean(1, keepdim=True)\n","    return (1 - F.cosine_similarity(pred_g, centroid, dim=2)).mean()\n","\n","# ------------------------------------------------------\n","# 4. DataLoader\n","# ------------------------------------------------------\n","from torch.utils.data import DataLoader, TensorDataset\n","train_dataset = TensorDataset(tx_train_t, im_train_exp)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n","\n","# ------------------------------------------------------\n","# 5. Fine-tune 10–15 epochs\n","# ------------------------------------------------------\n","EPOCHS = 20\n","for epoch in range(1, EPOCHS + 1):\n","    model.train()\n","    total_loss = 0.0\n","    for x_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False):\n","        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n","        optimizer.zero_grad()\n","\n","        y_pred = model(x_batch)\n","        idx = torch.randperm(x_batch.size(0), device=device)\n","        y_neg = y_batch[idx]\n","\n","        loss_cos = contrastive_loss(y_pred, y_batch)\n","        loss_tri = triplet_loss_fn(y_pred, y_batch, y_neg)\n","        loss_knn = knn_preservation_loss(x_batch, y_pred)\n","        loss_cap = caption_consistency_loss(y_pred)\n","\n","        loss = 0.6*loss_cos + 0.2*loss_tri + 0.1*loss_knn + 0.1*loss_cap\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch:02d}: avg loss = {total_loss/len(train_loader):.4f}\")\n","\n","torch.save(model.state_dict(), \"residual_structureAware.pth\")\n","print(\"✅ Saved fine-tuned structure-aware model.\")\n","\n","# ------------------------------------------------------\n","# 6. Inference with CSLS re-scoring\n","# ------------------------------------------------------\n","def csls_similarity(x, y, k=10):\n","    x_norm = F.normalize(x, p=2, dim=1)\n","    y_norm = F.normalize(y, p=2, dim=1)\n","    sim = x_norm @ y_norm.T\n","    r_x = sim.topk(k, dim=1).values.mean(1, keepdim=True)\n","    r_y = sim.topk(k, dim=0).values.mean(0, keepdim=True)\n","    csls = 2 * sim - r_x - r_y\n","    return csls\n","\n","model.eval()\n","with torch.no_grad():\n","    tx_test_n = F.normalize(tx_test_t, p=2, dim=1)\n","    pred_t = model(tx_test_n)\n","    im_train_n = F.normalize(im_train_t, p=2, dim=1)\n","    csls_sims = csls_similarity(pred_t, im_train_n)\n","\n","# (optional local recall check)\n","top1 = csls_sims.topk(1, dim=1).indices\n","print(\"Top-1 indices sample:\", top1[:10].T)\n","\n","# save final embeddings for Kaggle submission\n","preds = pred_t.cpu().numpy()\n","submission = pd.DataFrame({\n","    \"id\": test_data[\"captions/ids\"].astype(int),\n","    \"embedding\": [list(map(float, row)) for row in preds]\n","})\n","submission.to_csv(\"submission_structureAware.csv\", index=False)\n","print(\"✅ Saved submission_structureAware.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z3at0tS68uox","executionInfo":{"status":"ok","timestamp":1761691523883,"user_tz":-60,"elapsed":64344,"user":{"displayName":"ghulam mujtaba","userId":"00646622876627477130"}},"outputId":"f7cb1504-0d11-4bf3-a8d0-c3d34776a9b2"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loaded previous 0.8180 checkpoint.\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 01: avg loss = 1.2627\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 02: avg loss = 1.2615\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 03: avg loss = 1.2604\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 04: avg loss = 1.2595\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 05: avg loss = 1.2589\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 06: avg loss = 1.2584\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 07: avg loss = 1.2577\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 08: avg loss = 1.2570\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 09: avg loss = 1.2555\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10: avg loss = 1.2549\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11: avg loss = 1.2540\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12: avg loss = 1.2543\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13: avg loss = 1.2530\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14: avg loss = 1.2523\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15: avg loss = 1.2515\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16: avg loss = 1.2516\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17: avg loss = 1.2507\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18: avg loss = 1.2490\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19: avg loss = 1.2499\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20: avg loss = 1.2488\n","✅ Saved fine-tuned structure-aware model.\n","Top-1 indices sample: tensor([[ 7337,  2192,  1924, 17138, 19291, 13162,  8291,  9711, 16323, 19278]],\n","       device='cuda:0')\n","✅ Saved submission_structureAware.csv\n"]}]}]}